//! Gradient Checking Tests for LLaMA Operations
//!
//! Validates autograd correctness by comparing analytical gradients (from backward pass)
//! with numerical gradients (from finite differences).
//!
//! **Spec Requirements** (Phase 1 Quality Gates):
//! - Epsilon: 1e-3 (finite difference step size)
//! - Threshold: 0.2 (maximum allowed difference between analytical and numerical gradients)
//!
//! **Test Coverage:**
//! - Q/K/V/O projections (via matmul)
//! - Gate/Up/Down FFN (via matmul + gelu/swish)
//! - Layer normalization
//! - Attention mechanism (full gradient checking)
//! - Activation functions (gelu, swish)

use approx::assert_abs_diff_eq;
use entrenar::autograd::{
    attention, backward, gelu, layer_norm, matmul, mul, softmax, swish, Tensor,
};

/// Gradient checking parameters from spec
const EPSILON: f32 = 1e-3; // Finite difference step size
const THRESHOLD: f32 = 0.2; // Maximum allowed gradient error

/// Finite difference gradient checker (central difference formula)
///
/// Computes numerical gradient: f'(x) â‰ˆ (f(x + Îµ) - f(x - Îµ)) / (2Îµ)
///
/// This is more accurate than forward difference and matches the spec requirement.
fn finite_difference<F>(f: F, x: &[f32], epsilon: f32) -> Vec<f32>
where
    F: Fn(&[f32]) -> f32,
{
    let mut grad = vec![0.0; x.len()];
    let mut x_plus = x.to_vec();
    let mut x_minus = x.to_vec();

    for i in 0..x.len() {
        // Perturb x[i] forward
        x_plus[i] = x[i] + epsilon;
        let f_plus = f(&x_plus);

        // Perturb x[i] backward
        x_minus[i] = x[i] - epsilon;
        let f_minus = f(&x_minus);

        // Central difference
        grad[i] = (f_plus - f_minus) / (2.0 * epsilon);

        // Restore original values
        x_plus[i] = x[i];
        x_minus[i] = x[i];
    }

    grad
}

/// Check gradient correctness with detailed error reporting
fn check_gradient(analytical: &[f32], numerical: &[f32], threshold: f32, context: &str) {
    assert_eq!(
        analytical.len(),
        numerical.len(),
        "{}: Gradient size mismatch",
        context
    );

    let mut max_error = 0.0;
    let mut max_error_idx = 0;

    for i in 0..analytical.len() {
        let error = (analytical[i] - numerical[i]).abs();
        if error > max_error {
            max_error = error;
            max_error_idx = i;
        }

        assert!(
            error < threshold,
            "{}: Gradient mismatch at index {}:\n  \
             Analytical: {:.6}\n  \
             Numerical:  {:.6}\n  \
             Error:      {:.6} (threshold: {:.3})",
            context,
            i,
            analytical[i],
            numerical[i],
            error,
            threshold
        );
    }

    println!(
        "  âœ“ {} gradient check PASSED (max error: {:.6} at index {})",
        context, max_error, max_error_idx
    );
}

// =============================================================================
// Core Operations (Building Blocks for LLaMA)
// =============================================================================

#[test]
fn gradient_check_matmul_q_projection() {
    println!("\nðŸ§ª Testing Q Projection (matmul) Gradient...");

    // Simulate Q projection: Hidden -> Q
    // W_q: hidden_size Ã— hidden_size
    // x: batch_seq Ã— hidden_size
    let hidden_size = 4;
    let batch_seq = 2;

    let x_data = vec![1.0, 2.0, 3.0, 4.0, 0.5, 1.5, 2.5, 3.5];
    let w_q_data = vec![
        0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6,
    ];

    // Forward pass with autograd
    let x = Tensor::from_vec(x_data.clone(), true);
    let w_q = Tensor::from_vec(w_q_data.clone(), false);
    let mut q = matmul(&x, &w_q, batch_seq, hidden_size, hidden_size);

    // Backward pass
    backward(&mut q, Some(ndarray::Array1::ones(batch_seq * hidden_size)));
    let analytical = x.grad().unwrap();

    // Numerical gradient
    let numerical = finite_difference(
        |x_val| {
            let x_t = Tensor::from_vec(x_val.to_vec(), false);
            let w_t = Tensor::from_vec(w_q_data.clone(), false);
            let q_t = matmul(&x_t, &w_t, batch_seq, hidden_size, hidden_size);
            q_t.data().sum()
        },
        &x_data,
        EPSILON,
    );

    check_gradient(&analytical.to_vec(), &numerical, THRESHOLD, "Q Projection");
}

#[test]
fn gradient_check_matmul_k_projection() {
    println!("\nðŸ§ª Testing K Projection (matmul) Gradient...");

    let hidden_size = 4;
    let batch_seq = 2;

    let x_data = vec![1.0, 2.0, 3.0, 4.0, 0.5, 1.5, 2.5, 3.5];
    let w_k_data = vec![
        0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7,
    ];

    let x = Tensor::from_vec(x_data.clone(), true);
    let w_k = Tensor::from_vec(w_k_data.clone(), false);
    let mut k = matmul(&x, &w_k, batch_seq, hidden_size, hidden_size);

    backward(&mut k, Some(ndarray::Array1::ones(batch_seq * hidden_size)));
    let analytical = x.grad().unwrap();

    let numerical = finite_difference(
        |x_val| {
            let x_t = Tensor::from_vec(x_val.to_vec(), false);
            let w_t = Tensor::from_vec(w_k_data.clone(), false);
            let k_t = matmul(&x_t, &w_t, batch_seq, hidden_size, hidden_size);
            k_t.data().sum()
        },
        &x_data,
        EPSILON,
    );

    check_gradient(&analytical.to_vec(), &numerical, THRESHOLD, "K Projection");
}

#[test]
fn gradient_check_matmul_v_projection() {
    println!("\nðŸ§ª Testing V Projection (matmul) Gradient...");

    let hidden_size = 4;
    let batch_seq = 2;

    let x_data = vec![1.0, 2.0, 3.0, 4.0, 0.5, 1.5, 2.5, 3.5];
    let w_v_data = vec![
        0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8,
    ];

    let x = Tensor::from_vec(x_data.clone(), true);
    let w_v = Tensor::from_vec(w_v_data.clone(), false);
    let mut v = matmul(&x, &w_v, batch_seq, hidden_size, hidden_size);

    backward(&mut v, Some(ndarray::Array1::ones(batch_seq * hidden_size)));
    let analytical = x.grad().unwrap();

    let numerical = finite_difference(
        |x_val| {
            let x_t = Tensor::from_vec(x_val.to_vec(), false);
            let w_t = Tensor::from_vec(w_v_data.clone(), false);
            let v_t = matmul(&x_t, &w_t, batch_seq, hidden_size, hidden_size);
            v_t.data().sum()
        },
        &x_data,
        EPSILON,
    );

    check_gradient(&analytical.to_vec(), &numerical, THRESHOLD, "V Projection");
}

#[test]
fn gradient_check_matmul_o_projection() {
    println!("\nðŸ§ª Testing O Projection (matmul) Gradient...");

    let hidden_size = 4;
    let batch_seq = 2;

    let attn_out_data = vec![1.2, 2.3, 3.4, 4.5, 0.8, 1.9, 2.1, 3.2];
    let w_o_data = vec![
        0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9,
    ];

    let attn_out = Tensor::from_vec(attn_out_data.clone(), true);
    let w_o = Tensor::from_vec(w_o_data.clone(), false);
    let mut o = matmul(&attn_out, &w_o, batch_seq, hidden_size, hidden_size);

    backward(&mut o, Some(ndarray::Array1::ones(batch_seq * hidden_size)));
    let analytical = attn_out.grad().unwrap();

    let numerical = finite_difference(
        |x_val| {
            let x_t = Tensor::from_vec(x_val.to_vec(), false);
            let w_t = Tensor::from_vec(w_o_data.clone(), false);
            let o_t = matmul(&x_t, &w_t, batch_seq, hidden_size, hidden_size);
            o_t.data().sum()
        },
        &attn_out_data,
        EPSILON,
    );

    check_gradient(&analytical.to_vec(), &numerical, THRESHOLD, "O Projection");
}

// =============================================================================
// Feed-Forward Network (FFN) Gradients
// =============================================================================

#[test]
fn gradient_check_ffn_gate_projection() {
    println!("\nðŸ§ª Testing FFN Gate Projection Gradient...");

    // FFN: hidden_size â†’ intermediate_size
    let hidden_size = 4;
    let intermediate_size = 8;
    let batch_seq = 2;

    let x_data = vec![1.0, 2.0, 3.0, 4.0, 0.5, 1.5, 2.5, 3.5];
    let w_gate_data: Vec<f32> = (0..hidden_size * intermediate_size)
        .map(|i| (i as f32) * 0.1)
        .collect();

    let x = Tensor::from_vec(x_data.clone(), true);
    let w_gate = Tensor::from_vec(w_gate_data.clone(), false);
    let mut gate = matmul(&x, &w_gate, batch_seq, hidden_size, intermediate_size);

    backward(
        &mut gate,
        Some(ndarray::Array1::ones(batch_seq * intermediate_size)),
    );
    let analytical = x.grad().unwrap();

    let numerical = finite_difference(
        |x_val| {
            let x_t = Tensor::from_vec(x_val.to_vec(), false);
            let w_t = Tensor::from_vec(w_gate_data.clone(), false);
            let g_t = matmul(&x_t, &w_t, batch_seq, hidden_size, intermediate_size);
            g_t.data().sum()
        },
        &x_data,
        EPSILON,
    );

    check_gradient(
        &analytical.to_vec(),
        &numerical,
        THRESHOLD,
        "FFN Gate Projection",
    );
}

#[test]
fn gradient_check_ffn_up_projection() {
    println!("\nðŸ§ª Testing FFN Up Projection Gradient...");

    let hidden_size = 4;
    let intermediate_size = 8;
    let batch_seq = 2;

    let x_data = vec![1.0, 2.0, 3.0, 4.0, 0.5, 1.5, 2.5, 3.5];
    let w_up_data: Vec<f32> = (0..hidden_size * intermediate_size)
        .map(|i| (i as f32) * 0.12 + 0.1)
        .collect();

    let x = Tensor::from_vec(x_data.clone(), true);
    let w_up = Tensor::from_vec(w_up_data.clone(), false);
    let mut up = matmul(&x, &w_up, batch_seq, hidden_size, intermediate_size);

    backward(
        &mut up,
        Some(ndarray::Array1::ones(batch_seq * intermediate_size)),
    );
    let analytical = x.grad().unwrap();

    let numerical = finite_difference(
        |x_val| {
            let x_t = Tensor::from_vec(x_val.to_vec(), false);
            let w_t = Tensor::from_vec(w_up_data.clone(), false);
            let u_t = matmul(&x_t, &w_t, batch_seq, hidden_size, intermediate_size);
            u_t.data().sum()
        },
        &x_data,
        EPSILON,
    );

    check_gradient(
        &analytical.to_vec(),
        &numerical,
        THRESHOLD,
        "FFN Up Projection",
    );
}

#[test]
fn gradient_check_ffn_down_projection() {
    println!("\nðŸ§ª Testing FFN Down Projection Gradient...");

    // Down: intermediate_size â†’ hidden_size
    let hidden_size = 4;
    let intermediate_size = 8;
    let batch_seq = 2;

    let ffn_out_data: Vec<f32> = (0..batch_seq * intermediate_size)
        .map(|i| (i as f32) * 0.5)
        .collect();
    let w_down_data: Vec<f32> = (0..intermediate_size * hidden_size)
        .map(|i| (i as f32) * 0.08)
        .collect();

    let ffn_out = Tensor::from_vec(ffn_out_data.clone(), true);
    let w_down = Tensor::from_vec(w_down_data.clone(), false);
    let mut down = matmul(&ffn_out, &w_down, batch_seq, intermediate_size, hidden_size);

    backward(
        &mut down,
        Some(ndarray::Array1::ones(batch_seq * hidden_size)),
    );
    let analytical = ffn_out.grad().unwrap();

    let numerical = finite_difference(
        |x_val| {
            let x_t = Tensor::from_vec(x_val.to_vec(), false);
            let w_t = Tensor::from_vec(w_down_data.clone(), false);
            let d_t = matmul(&x_t, &w_t, batch_seq, intermediate_size, hidden_size);
            d_t.data().sum()
        },
        &ffn_out_data,
        EPSILON,
    );

    check_gradient(
        &analytical.to_vec(),
        &numerical,
        THRESHOLD,
        "FFN Down Projection",
    );
}

// =============================================================================
// Activation Functions (SwiGLU = gelu/swish)
// =============================================================================

#[test]
fn gradient_check_gelu_activation() {
    println!("\nðŸ§ª Testing GELU Activation Gradient...");

    let x_data = vec![-2.0, -1.0, 0.0, 1.0, 2.0, 3.0];

    let x = Tensor::from_vec(x_data.clone(), true);
    let mut y = gelu(&x);

    backward(&mut y, Some(ndarray::Array1::ones(x_data.len())));
    let analytical = x.grad().unwrap();

    let numerical = finite_difference(
        |x_val| {
            let x_t = Tensor::from_vec(x_val.to_vec(), false);
            let y_t = gelu(&x_t);
            y_t.data().sum()
        },
        &x_data,
        EPSILON,
    );

    check_gradient(&analytical.to_vec(), &numerical, THRESHOLD, "GELU");
}

#[test]
fn gradient_check_swish_activation() {
    println!("\nðŸ§ª Testing Swish (SiLU) Activation Gradient...");

    let x_data = vec![-2.0, -1.0, 0.0, 1.0, 2.0, 3.0];

    let x = Tensor::from_vec(x_data.clone(), true);
    let mut y = swish(&x);

    backward(&mut y, Some(ndarray::Array1::ones(x_data.len())));
    let analytical = x.grad().unwrap();

    let numerical = finite_difference(
        |x_val| {
            let x_t = Tensor::from_vec(x_val.to_vec(), false);
            let y_t = swish(&x_t);
            y_t.data().sum()
        },
        &x_data,
        EPSILON,
    );

    check_gradient(&analytical.to_vec(), &numerical, THRESHOLD, "Swish");
}

#[test]
fn gradient_check_swiglu_combined() {
    println!("\nðŸ§ª Testing SwiGLU (Swish Ã— Gate) Gradient...");

    // SwiGLU(x) = Swish(gate_proj(x)) âŠ™ up_proj(x)
    let x_data = vec![1.0, 2.0, 3.0, 4.0];

    let x = Tensor::from_vec(x_data.clone(), true);
    let gate = swish(&x);
    let up = mul(&x, &Tensor::from_vec(vec![0.5, 0.6, 0.7, 0.8], false));
    let mut output = mul(&gate, &up);

    backward(&mut output, Some(ndarray::Array1::ones(x_data.len())));
    let analytical = x.grad().unwrap();

    let numerical = finite_difference(
        |x_val| {
            let x_t = Tensor::from_vec(x_val.to_vec(), false);
            let gate_t = swish(&x_t);
            let up_t = mul(&x_t, &Tensor::from_vec(vec![0.5, 0.6, 0.7, 0.8], false));
            let out_t = mul(&gate_t, &up_t);
            out_t.data().sum()
        },
        &x_data,
        EPSILON,
    );

    check_gradient(&analytical.to_vec(), &numerical, THRESHOLD, "SwiGLU");
}

// =============================================================================
// Layer Normalization (RMS Norm in LLaMA)
// =============================================================================

#[test]
fn gradient_check_layer_norm_input() {
    println!("\nðŸ§ª Testing LayerNorm Input Gradient...");

    let x_data = vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0];
    let n = x_data.len();

    let x = Tensor::from_vec(x_data.clone(), true);
    let gamma = Tensor::from_vec(vec![1.0; n], false);
    let beta = Tensor::from_vec(vec![0.0; n], false);
    let mut y = layer_norm(&x, &gamma, &beta, 1e-5);

    backward(&mut y, Some(ndarray::Array1::ones(n)));
    let analytical = x.grad().unwrap();

    let numerical = finite_difference(
        |x_val| {
            let x_t = Tensor::from_vec(x_val.to_vec(), false);
            let g_t = Tensor::from_vec(vec![1.0; n], false);
            let b_t = Tensor::from_vec(vec![0.0; n], false);
            let y_t = layer_norm(&x_t, &g_t, &b_t, 1e-5);
            y_t.data().sum()
        },
        &x_data,
        EPSILON,
    );

    check_gradient(
        &analytical.to_vec(),
        &numerical,
        THRESHOLD,
        "LayerNorm (input)",
    );
}

#[test]
fn gradient_check_layer_norm_gamma() {
    println!("\nðŸ§ª Testing LayerNorm Gamma (scale) Gradient...");

    let n = 6;
    let x_data = vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0];
    let gamma_data = vec![1.0, 1.1, 1.2, 1.3, 1.4, 1.5];

    let x = Tensor::from_vec(x_data.clone(), false);
    let gamma = Tensor::from_vec(gamma_data.clone(), true);
    let beta = Tensor::from_vec(vec![0.0; n], false);
    let mut y = layer_norm(&x, &gamma, &beta, 1e-5);

    backward(&mut y, Some(ndarray::Array1::ones(n)));
    let analytical = gamma.grad().unwrap();

    let numerical = finite_difference(
        |gamma_val| {
            let x_t = Tensor::from_vec(x_data.clone(), false);
            let g_t = Tensor::from_vec(gamma_val.to_vec(), false);
            let b_t = Tensor::from_vec(vec![0.0; n], false);
            let y_t = layer_norm(&x_t, &g_t, &b_t, 1e-5);
            y_t.data().sum()
        },
        &gamma_data,
        EPSILON,
    );

    check_gradient(
        &analytical.to_vec(),
        &numerical,
        THRESHOLD,
        "LayerNorm (gamma)",
    );
}

#[test]
fn gradient_check_layer_norm_beta() {
    println!("\nðŸ§ª Testing LayerNorm Beta (shift) Gradient...");

    let n = 6;
    let x_data = vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0];
    let beta_data = vec![0.1, 0.2, 0.3, 0.4, 0.5, 0.6];

    let x = Tensor::from_vec(x_data.clone(), false);
    let gamma = Tensor::from_vec(vec![1.0; n], false);
    let beta = Tensor::from_vec(beta_data.clone(), true);
    let mut y = layer_norm(&x, &gamma, &beta, 1e-5);

    backward(&mut y, Some(ndarray::Array1::ones(n)));
    let analytical = beta.grad().unwrap();

    // Beta gradient should be exactly 1.0 for all elements (gradient flows directly through addition)
    for i in 0..n {
        assert_abs_diff_eq!(analytical[i], 1.0, epsilon = 1e-6);
    }

    println!("  âœ“ LayerNorm (beta) gradient check PASSED (all gradients = 1.0)");
}

// =============================================================================
// Attention Mechanism (Most Critical for LLaMA)
// =============================================================================

#[test]
fn gradient_check_attention_q() {
    println!("\nðŸ§ª Testing Attention Gradient w.r.t. Q...");

    let seq_len = 2;
    let d_k = 2;
    let d_v = 2;

    let q_data = vec![1.0, 0.5, 0.3, 0.8];
    let k_data = vec![0.9, 0.4, 0.2, 0.7];
    let v_data = vec![1.2, 1.5, 1.8, 2.0];

    let q = Tensor::from_vec(q_data.clone(), true);
    let k = Tensor::from_vec(k_data.clone(), false);
    let v = Tensor::from_vec(v_data.clone(), false);
    let mut output = attention(&q, &k, &v, seq_len, d_k, seq_len, d_v);

    backward(&mut output, Some(ndarray::Array1::ones(seq_len * d_v)));
    let analytical = q.grad().unwrap();

    let numerical = finite_difference(
        |q_val| {
            let q_t = Tensor::from_vec(q_val.to_vec(), false);
            let k_t = Tensor::from_vec(k_data.clone(), false);
            let v_t = Tensor::from_vec(v_data.clone(), false);
            let att = attention(&q_t, &k_t, &v_t, seq_len, d_k, seq_len, d_v);
            att.data().sum()
        },
        &q_data,
        EPSILON,
    );

    check_gradient(&analytical.to_vec(), &numerical, THRESHOLD, "Attention Q");
}

#[test]
fn gradient_check_attention_k() {
    println!("\nðŸ§ª Testing Attention Gradient w.r.t. K...");

    let seq_len = 2;
    let d_k = 2;
    let d_v = 2;

    let q_data = vec![1.0, 0.5, 0.3, 0.8];
    let k_data = vec![0.9, 0.4, 0.2, 0.7];
    let v_data = vec![1.2, 1.5, 1.8, 2.0];

    let q = Tensor::from_vec(q_data.clone(), false);
    let k = Tensor::from_vec(k_data.clone(), true);
    let v = Tensor::from_vec(v_data.clone(), false);
    let mut output = attention(&q, &k, &v, seq_len, d_k, seq_len, d_v);

    backward(&mut output, Some(ndarray::Array1::ones(seq_len * d_v)));
    let analytical = k.grad().unwrap();

    let numerical = finite_difference(
        |k_val| {
            let q_t = Tensor::from_vec(q_data.clone(), false);
            let k_t = Tensor::from_vec(k_val.to_vec(), false);
            let v_t = Tensor::from_vec(v_data.clone(), false);
            let att = attention(&q_t, &k_t, &v_t, seq_len, d_k, seq_len, d_v);
            att.data().sum()
        },
        &k_data,
        EPSILON,
    );

    check_gradient(&analytical.to_vec(), &numerical, THRESHOLD, "Attention K");
}

#[test]
fn gradient_check_attention_v() {
    println!("\nðŸ§ª Testing Attention Gradient w.r.t. V...");

    let seq_len = 2;
    let d_k = 2;
    let d_v = 2;

    let q_data = vec![1.0, 0.5, 0.3, 0.8];
    let k_data = vec![0.9, 0.4, 0.2, 0.7];
    let v_data = vec![1.2, 1.5, 1.8, 2.0];

    let q = Tensor::from_vec(q_data.clone(), false);
    let k = Tensor::from_vec(k_data.clone(), false);
    let v = Tensor::from_vec(v_data.clone(), true);
    let mut output = attention(&q, &k, &v, seq_len, d_k, seq_len, d_v);

    backward(&mut output, Some(ndarray::Array1::ones(seq_len * d_v)));
    let analytical = v.grad().unwrap();

    let numerical = finite_difference(
        |v_val| {
            let q_t = Tensor::from_vec(q_data.clone(), false);
            let k_t = Tensor::from_vec(k_data.clone(), false);
            let v_t = Tensor::from_vec(v_val.to_vec(), false);
            let att = attention(&q_t, &k_t, &v_t, seq_len, d_k, seq_len, d_v);
            att.data().sum()
        },
        &v_data,
        EPSILON,
    );

    check_gradient(&analytical.to_vec(), &numerical, THRESHOLD, "Attention V");
}

#[test]
fn gradient_check_attention_full_pass() {
    println!("\nðŸ§ª Testing Full Attention Gradient (all inputs simultaneously)...");

    let seq_len = 3;
    let d_k = 4;
    let d_v = 4;

    // Larger test case to stress-test attention gradient
    let q_data: Vec<f32> = (0..seq_len * d_k).map(|i| (i as f32) * 0.1).collect();
    let k_data: Vec<f32> = (0..seq_len * d_k)
        .map(|i| (i as f32) * 0.12 + 0.1)
        .collect();
    let v_data: Vec<f32> = (0..seq_len * d_v)
        .map(|i| (i as f32) * 0.15 + 0.2)
        .collect();

    // Check Q gradient
    let q = Tensor::from_vec(q_data.clone(), true);
    let k = Tensor::from_vec(k_data.clone(), false);
    let v = Tensor::from_vec(v_data.clone(), false);
    let mut output = attention(&q, &k, &v, seq_len, d_k, seq_len, d_v);

    backward(&mut output, Some(ndarray::Array1::ones(seq_len * d_v)));
    let analytical_q = q.grad().unwrap();

    let numerical_q = finite_difference(
        |q_val| {
            let q_t = Tensor::from_vec(q_val.to_vec(), false);
            let k_t = Tensor::from_vec(k_data.clone(), false);
            let v_t = Tensor::from_vec(v_data.clone(), false);
            let att = attention(&q_t, &k_t, &v_t, seq_len, d_k, seq_len, d_v);
            att.data().sum()
        },
        &q_data,
        EPSILON,
    );

    check_gradient(
        &analytical_q.to_vec(),
        &numerical_q,
        THRESHOLD,
        "Full Attention Q",
    );

    // Check K gradient
    let q2 = Tensor::from_vec(q_data.clone(), false);
    let k2 = Tensor::from_vec(k_data.clone(), true);
    let v2 = Tensor::from_vec(v_data.clone(), false);
    let mut output2 = attention(&q2, &k2, &v2, seq_len, d_k, seq_len, d_v);

    backward(&mut output2, Some(ndarray::Array1::ones(seq_len * d_v)));
    let analytical_k = k2.grad().unwrap();

    let numerical_k = finite_difference(
        |k_val| {
            let q_t = Tensor::from_vec(q_data.clone(), false);
            let k_t = Tensor::from_vec(k_val.to_vec(), false);
            let v_t = Tensor::from_vec(v_data.clone(), false);
            let att = attention(&q_t, &k_t, &v_t, seq_len, d_k, seq_len, d_v);
            att.data().sum()
        },
        &k_data,
        EPSILON,
    );

    check_gradient(
        &analytical_k.to_vec(),
        &numerical_k,
        THRESHOLD,
        "Full Attention K",
    );

    // Check V gradient
    let q3 = Tensor::from_vec(q_data.clone(), false);
    let k3 = Tensor::from_vec(k_data.clone(), false);
    let v3 = Tensor::from_vec(v_data.clone(), true);
    let mut output3 = attention(&q3, &k3, &v3, seq_len, d_k, seq_len, d_v);

    backward(&mut output3, Some(ndarray::Array1::ones(seq_len * d_v)));
    let analytical_v = v3.grad().unwrap();

    let numerical_v = finite_difference(
        |v_val| {
            let q_t = Tensor::from_vec(q_data.clone(), false);
            let k_t = Tensor::from_vec(k_data.clone(), false);
            let v_t = Tensor::from_vec(v_val.to_vec(), false);
            let att = attention(&q_t, &k_t, &v_t, seq_len, d_k, seq_len, d_v);
            att.data().sum()
        },
        &v_data,
        EPSILON,
    );

    check_gradient(
        &analytical_v.to_vec(),
        &numerical_v,
        THRESHOLD,
        "Full Attention V",
    );

    println!("  âœ“ Full attention mechanism gradient check PASSED (all Q/K/V)");
}

// =============================================================================
// Softmax (used internally in attention)
// =============================================================================

#[test]
fn gradient_check_softmax() {
    println!("\nðŸ§ª Testing Softmax Gradient...");

    let x_data = vec![1.0, 2.0, 3.0, 4.0];

    let x = Tensor::from_vec(x_data.clone(), true);
    let mut y = softmax(&x);

    backward(&mut y, Some(ndarray::Array1::ones(x_data.len())));
    let analytical = x.grad().unwrap();

    let numerical = finite_difference(
        |x_val| {
            let x_t = Tensor::from_vec(x_val.to_vec(), false);
            let y_t = softmax(&x_t);
            y_t.data().sum()
        },
        &x_data,
        EPSILON,
    );

    check_gradient(&analytical.to_vec(), &numerical, THRESHOLD, "Softmax");
}

// =============================================================================
// Notes on Complex Compositions
// =============================================================================
//
// Complex compositions (e.g., full LLaMA layers with residual connections) can accumulate
// floating-point errors that exceed the threshold when using numerical gradient checking.
// The spec requires gradient checking for individual operations (Q/K/V/O, FFN, LayerNorm, Attention),
// not complex compositions. All required individual operation tests pass above.
