<!DOCTYPE HTML>
<html lang="en" class="rust sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Tape-Based Computation Graphs - Entrenar - Training &amp; Optimization Library</title>


        <!-- Custom HTML head -->

        <meta name="description" content="A comprehensive guide to building neural network training systems with autograd, optimizers, LoRA/QLoRA, and quantization using EXTREME TDD methodology">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="../favicon.svg">
        <link rel="shortcut icon" href="../favicon.png">
        <link rel="stylesheet" href="../css/variables.css">
        <link rel="stylesheet" href="../css/general.css">
        <link rel="stylesheet" href="../css/chrome.css">
        <link rel="stylesheet" href="../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="../highlight.css">
        <link rel="stylesheet" href="../tomorrow-night.css">
        <link rel="stylesheet" href="../ayu-highlight.css">

        <!-- Custom theme stylesheets -->


        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "../";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "rust";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="../toc.js"></script>
    </head>
    <body>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('rust')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var sidebar = null;
            var sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="../toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Entrenar - Training &amp; Optimization Library</h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/paiml/entrenar" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>
                        <a href="https://github.com/paiml/entrenar/edit/main/book/src/autograd/tape-based-graphs.md" title="Suggest an edit" aria-label="Suggest an edit">
                            <i id="git-edit-button" class="fa fa-edit"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="tape-based-computation-graphs"><a class="header" href="#tape-based-computation-graphs">Tape-Based Computation Graphs</a></h1>
<p>Entrenar uses a <strong>tape-based</strong> approach to record computational graphs during the forward pass and replay them in reverse during backpropagation. This chapter explains how the tape works and why it's efficient.</p>
<h2 id="the-tape-metaphor"><a class="header" href="#the-tape-metaphor">The Tape Metaphor</a></h2>
<p>Think of the tape like a cassette recorder:</p>
<ul>
<li><strong>Forward pass</strong>: Record each operation onto the tape</li>
<li><strong>Backward pass</strong>: Rewind and play back in reverse</li>
<li><strong>Gradient computation</strong>: Each operation knows how to propagate gradients</li>
</ul>
<pre><code>Forward (Recording):
  x → [Op1] → a → [Op2] → b → [Op3] → output
  Tape: [Op1, Op2, Op3]

Backward (Playback):
  dx ← [Op1*] ← da ← [Op2*] ← db ← [Op3*] ← dout=1.0
  Process tape in reverse: Op3* → Op2* → Op1*
</code></pre>
<h2 id="tape-structure"><a class="header" href="#tape-structure">Tape Structure</a></h2>
<p>Entrenar's tape stores operation metadata, not full tensors:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>struct TapeEntry {
    operation: OpType,      // What operation (Add, Mul, MatMul, etc.)
    inputs: Vec&lt;TensorId&gt;,  // Input tensor IDs
    output: TensorId,       // Output tensor ID
    metadata: OpMetadata,   // Operation-specific data
}

enum OpType {
    Add,
    Mul,
    MatMul { rows, cols, batch },
    ReLU,
    LayerNorm,
    // ... etc
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Key insight</strong>: We don't store actual tensor data on the tape, only <strong>references</strong> (IDs) and operation metadata.</p>
<h2 id="example-recording-operations"><a class="header" href="#example-recording-operations">Example: Recording Operations</a></h2>
<p>Let's trace a simple computation:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use entrenar::{Tensor, backward};

let x = Tensor::from_vec(vec![2.0], true);  // ID: 0
let y = Tensor::from_vec(vec![3.0], true);  // ID: 1

let a = &amp;x + &amp;y;  // ID: 2, records Add(0, 1) -&gt; 2
let b = &amp;a * &amp;x;  // ID: 3, records Mul(2, 0) -&gt; 3

backward(&amp;b);
<span class="boring">}</span></code></pre></pre>
<p><strong>Tape after forward pass:</strong></p>
<pre><code>Tape = [
  Entry {
    operation: Add,
    inputs: [tensor_0_id, tensor_1_id],  // x, y
    output: tensor_2_id,                  // a
    metadata: {},
  },
  Entry {
    operation: Mul,
    inputs: [tensor_2_id, tensor_0_id],  // a, x
    output: tensor_3_id,                  // b
    metadata: {},
  },
]
</code></pre>
<h2 id="backward-pass-replaying-the-tape"><a class="header" href="#backward-pass-replaying-the-tape">Backward Pass: Replaying the Tape</a></h2>
<p>During <code>backward(&amp;b)</code>, Entrenar processes the tape in <strong>reverse order</strong>:</p>
<h3 id="step-1-initialize-output-gradient"><a class="header" href="#step-1-initialize-output-gradient">Step 1: Initialize Output Gradient</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// db/db = 1.0 (seed gradient)
b.set_grad(vec![1.0]);
<span class="boring">}</span></code></pre></pre>
<h3 id="step-2-process-tape-entry-1-mul"><a class="header" href="#step-2-process-tape-entry-1-mul">Step 2: Process Tape Entry 1 (Mul)</a></h3>
<pre><code>Entry: Mul(a, x) -&gt; b
Current: db = 1.0

Backward rule for Mul:
  da = db * x = 1.0 * 2.0 = 2.0
  dx += db * a = 1.0 * 5.0 = 5.0  (accumulate)

Update gradients:
  a.grad = [2.0]
  x.grad = [5.0]
</code></pre>
<h3 id="step-3-process-tape-entry-0-add"><a class="header" href="#step-3-process-tape-entry-0-add">Step 3: Process Tape Entry 0 (Add)</a></h3>
<pre><code>Entry: Add(x, y) -&gt; a
Current: da = 2.0

Backward rule for Add:
  dx += da * 1 = 2.0
  dy = da * 1 = 2.0

Update gradients:
  x.grad = [5.0 + 2.0] = [7.0]  (accumulated!)
  y.grad = [2.0]
</code></pre>
<h3 id="final-gradients"><a class="header" href="#final-gradients">Final Gradients</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>println!("db/dx = {}", x.grad()[0]);  // 7.0 ✅
println!("db/dy = {}", y.grad()[0]);  // 2.0 ✅
<span class="boring">}</span></code></pre></pre>
<p><strong>Verification</strong> (manual chain rule):</p>
<pre><code>b = a * x = (x + y) * x = x² + xy
db/dx = 2x + y = 2(2) + 3 = 7 ✅
db/dy = x = 2 ✅
</code></pre>
<h2 id="gradient-accumulation"><a class="header" href="#gradient-accumulation">Gradient Accumulation</a></h2>
<p>Notice that <code>x</code> appears twice in the computation graph:</p>
<pre><code>    y
    │
    ▼
x ─┬─&gt; Add -&gt; a ─┐
   │              │
   └──────────────┴─&gt; Mul -&gt; b
</code></pre>
<p><strong>Gradients must accumulate</strong> when a tensor has multiple consumers:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// First use: x in Add
dx_from_add = da * 1 = 2.0

// Second use: x in Mul
dx_from_mul = db * a = 5.0

// Total gradient (sum of paths)
dx_total = dx_from_add + dx_from_mul = 7.0
<span class="boring">}</span></code></pre></pre>
<p>Entrenar handles this automatically via <code>+=</code> in gradient updates:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>x.grad_mut()[i] += gradient_contribution;  // Accumulation
<span class="boring">}</span></code></pre></pre>
<h2 id="operation-metadata"><a class="header" href="#operation-metadata">Operation Metadata</a></h2>
<p>Some operations need extra context for backward passes:</p>
<h3 id="matrix-multiplication"><a class="header" href="#matrix-multiplication">Matrix Multiplication</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>Entry {
    operation: MatMul,
    inputs: [a_id, b_id],
    output: c_id,
    metadata: MatMulMeta {
        rows: 128,
        cols: 64,
        batch: 32,
    },
}
<span class="boring">}</span></code></pre></pre>
<p>During backward:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Need dimensions to compute dA = dC @ B^T
let dA = matmul(dC, B_transpose, rows, cols, batch);
<span class="boring">}</span></code></pre></pre>
<h3 id="layer-normalization"><a class="header" href="#layer-normalization">Layer Normalization</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>Entry {
    operation: LayerNorm,
    inputs: [x_id],
    output: y_id,
    metadata: LayerNormMeta {
        mean: 0.5,      // Saved from forward pass
        variance: 0.25,
    },
}
<span class="boring">}</span></code></pre></pre>
<p>During backward:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Need mean/variance from forward pass to compute gradients
let dx = layernorm_backward(dy, x, saved_mean, saved_variance);
<span class="boring">}</span></code></pre></pre>
<h2 id="memory-efficiency"><a class="header" href="#memory-efficiency">Memory Efficiency</a></h2>
<p>Tape-based autograd is memory efficient because:</p>
<h3 id="1-store-operations-not-tensors"><a class="header" href="#1-store-operations-not-tensors">1. Store Operations, Not Tensors</a></h3>
<p><strong>Bad</strong> (store full tensors):</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Memory: O(n_ops * tensor_size)
struct TapeEntry {
    input_data: Vec&lt;f32&gt;,  // ❌ Wasteful
    output_data: Vec&lt;f32&gt;, // ❌ Wasteful
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Good</strong> (store IDs):</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Memory: O(n_ops)
struct TapeEntry {
    input_ids: Vec&lt;TensorId&gt;,  // ✅ Just integers
    output_id: TensorId,        // ✅ Just one integer
}
<span class="boring">}</span></code></pre></pre>
<h3 id="2-tensors-managed-separately"><a class="header" href="#2-tensors-managed-separately">2. Tensors Managed Separately</a></h3>
<p>Tensors are reference-counted (<code>Rc&lt;RefCell&lt;TensorData&gt;&gt;</code>):</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let x = Tensor::from_vec(vec![1.0, 2.0], true);
let y = &amp;x * &amp;x;  // y shares data with x via Rc

// When y is computed, x's data is still available
// Tape only stores IDs, not copies of data
<span class="boring">}</span></code></pre></pre>
<h3 id="3-tape-is-cleared-after-backward"><a class="header" href="#3-tape-is-cleared-after-backward">3. Tape is Cleared After Backward</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>backward(&amp;loss);  // Processes tape

// Tape is consumed and cleared
// Memory freed for next forward pass
<span class="boring">}</span></code></pre></pre>
<h2 id="dynamic-graphs"><a class="header" href="#dynamic-graphs">Dynamic Graphs</a></h2>
<p>Entrenar's tape enables <strong>dynamic computational graphs</strong> - the graph can change every forward pass:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>for epoch in 0..100 {
    let output = if epoch &lt; 50 {
        // First 50 epochs: simple model
        &amp;w1 * &amp;x + &amp;b1
    } else {
        // Last 50 epochs: complex model
        let h = relu(&amp;(&amp;w1 * &amp;x + &amp;b1));
        &amp;w2 * &amp;h + &amp;b2
    };

    backward(&amp;output);  // Different tape each epoch!
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Contrast with static graphs</strong> (TensorFlow 1.x):</p>
<ul>
<li>Static: Define graph once, compile, reuse</li>
<li>Dynamic (Entrenar): Build new graph every forward pass</li>
</ul>
<p><strong>Trade-offs</strong>:</p>
<ul>
<li>✅ Dynamic: Flexible (control flow, debugging)</li>
<li>✅ Static: Faster (compiled optimizations)</li>
<li>Entrenar chooses flexibility (similar to PyTorch)</li>
</ul>
<h2 id="tape-implementation-details"><a class="header" href="#tape-implementation-details">Tape Implementation Details</a></h2>
<h3 id="tape-creation"><a class="header" href="#tape-creation">Tape Creation</a></h3>
<p>When you create a tensor with <code>requires_grad=true</code>:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let x = Tensor::from_vec(vec![1.0], true);
<span class="boring">}</span></code></pre></pre>
<p>Entrenar initializes:</p>
<ol>
<li>Tensor data storage</li>
<li>Gradient storage (same size as data)</li>
<li>Registration for tape recording</li>
</ol>
<h3 id="operation-recording"><a class="header" href="#operation-recording">Operation Recording</a></h3>
<p>Every operation checks if recording is needed:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn add(lhs: &amp;Tensor, rhs: &amp;Tensor) -&gt; Tensor {
    // Forward computation
    let result_data = lhs.data() + rhs.data();

    // Check if we need to record
    if lhs.requires_grad() || rhs.requires_grad() {
        let result = Tensor::new(result_data, true);

        // Record on tape
        TAPE.with(|tape| {
            tape.borrow_mut().push(TapeEntry {
                operation: OpType::Add,
                inputs: vec![lhs.id(), rhs.id()],
                output: result.id(),
                metadata: {},
            });
        });

        result
    } else {
        // No gradients needed, skip tape
        Tensor::new(result_data, false)
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="backward-execution"><a class="header" href="#backward-execution">Backward Execution</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub fn backward(loss: &amp;Tensor) {
    // Seed gradient: dloss/dloss = 1.0
    loss.set_grad(vec![1.0]);

    // Get tape entries
    TAPE.with(|tape| {
        let entries = tape.borrow_mut().drain(..).collect::&lt;Vec&lt;_&gt;&gt;();

        // Process in reverse
        for entry in entries.into_iter().rev() {
            match entry.operation {
                OpType::Add =&gt; {
                    // Get output gradient
                    let grad_out = get_tensor(entry.output).grad();

                    // Propagate to inputs
                    get_tensor(entry.inputs[0]).accumulate_grad(&amp;grad_out);
                    get_tensor(entry.inputs[1]).accumulate_grad(&amp;grad_out);
                }
                OpType::Mul =&gt; {
                    let lhs = get_tensor(entry.inputs[0]);
                    let rhs = get_tensor(entry.inputs[1]);
                    let grad_out = get_tensor(entry.output).grad();

                    // d_lhs = grad_out * rhs
                    lhs.accumulate_grad(&amp;(grad_out * rhs.data()));

                    // d_rhs = grad_out * lhs
                    rhs.accumulate_grad(&amp;(grad_out * lhs.data()));
                }
                // ... other operations
            }
        }
    });
}
<span class="boring">}</span></code></pre></pre>
<h2 id="debugging-the-tape"><a class="header" href="#debugging-the-tape">Debugging the Tape</a></h2>
<p>You can inspect the tape for debugging:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[cfg(debug_assertions)]
fn print_tape() {
    TAPE.with(|tape| {
        println!("Tape contents:");
        for (i, entry) in tape.borrow().iter().enumerate() {
            println!("  [{}] {:?}", i, entry);
        }
    });
}

let x = Tensor::from_vec(vec![2.0], true);
let y = &amp;x * &amp;x;

print_tape();
// Output:
//   [0] Mul { inputs: [tensor_0, tensor_0], output: tensor_1 }
<span class="boring">}</span></code></pre></pre>
<h2 id="performance-considerations"><a class="header" href="#performance-considerations">Performance Considerations</a></h2>
<h3 id="tape-overhead"><a class="header" href="#tape-overhead">Tape Overhead</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Aspect</th><th>Cost</th><th>Mitigation</th></tr></thead><tbody>
<tr><td><strong>Recording</strong></td><td>O(1) per operation</td><td>Minimal (just push to Vec)</td></tr>
<tr><td><strong>Storage</strong></td><td>O(n_ops) metadata</td><td>Small (typically &lt;1MB for large models)</td></tr>
<tr><td><strong>Playback</strong></td><td>O(n_ops)</td><td>Necessary for gradients</td></tr>
</tbody></table>
</div>
<h3 id="optimization-no-grad-mode"><a class="header" href="#optimization-no-grad-mode">Optimization: No-Grad Mode</a></h3>
<p>Disable tape for inference:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Inference (no tape recording)
let output = model.forward(&amp;input);  // All tensors have requires_grad=false

// No tape entries created, faster forward pass
<span class="boring">}</span></code></pre></pre>
<h2 id="comparison-with-graph-based-autograd"><a class="header" href="#comparison-with-graph-based-autograd">Comparison with Graph-Based Autograd</a></h2>
<div class="table-wrapper"><table><thead><tr><th>Aspect</th><th>Tape-Based (Entrenar)</th><th>Graph-Based (TensorFlow 1.x)</th></tr></thead><tbody>
<tr><td><strong>Flexibility</strong></td><td>Dynamic (builds each forward)</td><td>Static (compile once)</td></tr>
<tr><td><strong>Debugging</strong></td><td>Easy (step through code)</td><td>Hard (symbolic graph)</td></tr>
<tr><td><strong>Performance</strong></td><td>Good (minimal overhead)</td><td>Excellent (compiled)</td></tr>
<tr><td><strong>Memory</strong></td><td>O(n_ops)</td><td>O(n_tensors + n_ops)</td></tr>
<tr><td><strong>Use Case</strong></td><td>Research, prototyping</td><td>Production at scale</td></tr>
</tbody></table>
</div>
<h2 id="key-takeaways"><a class="header" href="#key-takeaways">Key Takeaways</a></h2>
<ol>
<li><strong>Tape records operations</strong> during forward pass as metadata</li>
<li><strong>Backward replays tape in reverse</strong> to propagate gradients</li>
<li><strong>Gradients accumulate</strong> when tensors have multiple consumers</li>
<li><strong>Metadata stored</strong> for operations needing forward pass values</li>
<li><strong>Dynamic graphs</strong> rebuild tape each forward pass (flexibility)</li>
<li><strong>Memory efficient</strong> - stores IDs and metadata, not full tensors</li>
</ol>
<h2 id="whats-next"><a class="header" href="#whats-next">What's Next?</a></h2>
<ul>
<li><strong><a href="./backward-pass.html">Backward Pass</a></strong> - Detailed gradient propagation rules</li>
<li><strong><a href="./gradient-computation.html">Gradient Computation</a></strong> - Chain rule mechanics</li>
<li><strong><a href="./finite-difference.html">Finite Difference Validation</a></strong> - Testing gradient correctness</li>
</ul>
<hr />
<p><strong>Ready to understand backward passes?</strong> Continue to <a href="./backward-pass.html">Backward Pass</a> →</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../autograd/what-is-autograd.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next prefetch" href="../autograd/tensor-operations.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../autograd/what-is-autograd.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next prefetch" href="../autograd/tensor-operations.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>

        <script src="../ace.js"></script>
        <script src="../editor.js"></script>
        <script src="../mode-rust.js"></script>
        <script src="../theme-dawn.js"></script>
        <script src="../theme-tomorrow_night.js"></script>

        <script src="../elasticlunr.min.js"></script>
        <script src="../mark.min.js"></script>
        <script src="../searcher.js"></script>

        <script src="../clipboard.min.js"></script>
        <script src="../highlight.js"></script>
        <script src="../book.js"></script>

        <!-- Custom JS scripts -->


    </div>
    </body>
</html>
