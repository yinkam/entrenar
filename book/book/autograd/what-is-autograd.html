<!DOCTYPE HTML>
<html lang="en" class="rust sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>What is Automatic Differentiation? - Entrenar - Training &amp; Optimization Library</title>


        <!-- Custom HTML head -->

        <meta name="description" content="A comprehensive guide to building neural network training systems with autograd, optimizers, LoRA/QLoRA, and quantization using EXTREME TDD methodology">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="../favicon.svg">
        <link rel="shortcut icon" href="../favicon.png">
        <link rel="stylesheet" href="../css/variables.css">
        <link rel="stylesheet" href="../css/general.css">
        <link rel="stylesheet" href="../css/chrome.css">
        <link rel="stylesheet" href="../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="../highlight.css">
        <link rel="stylesheet" href="../tomorrow-night.css">
        <link rel="stylesheet" href="../ayu-highlight.css">

        <!-- Custom theme stylesheets -->


        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "../";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "rust";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="../toc.js"></script>
    </head>
    <body>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('rust')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var sidebar = null;
            var sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="../toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Entrenar - Training &amp; Optimization Library</h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/paiml/entrenar" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>
                        <a href="https://github.com/paiml/entrenar/edit/main/book/src/autograd/what-is-autograd.md" title="Suggest an edit" aria-label="Suggest an edit">
                            <i id="git-edit-button" class="fa fa-edit"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="what-is-automatic-differentiation"><a class="header" href="#what-is-automatic-differentiation">What is Automatic Differentiation?</a></h1>
<p><strong>Automatic Differentiation (Autograd)</strong> is a technique for computing derivatives of functions specified by computer programs. It's the foundation of modern deep learning, enabling neural networks to learn through gradient-based optimization.</p>
<h2 id="the-problem-manual-derivatives"><a class="header" href="#the-problem-manual-derivatives">The Problem: Manual Derivatives</a></h2>
<p>Consider a simple neural network layer:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn forward(x: f32, w: f32, b: f32) -&gt; f32 {
    w * x + b  // Linear transformation
}
<span class="boring">}</span></code></pre></pre>
<p>To train this layer, we need gradients: <code>∂loss/∂w</code> and <code>∂loss/∂b</code>.</p>
<h3 id="manual-approach-error-prone"><a class="header" href="#manual-approach-error-prone">Manual Approach (Error-Prone)</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Forward pass
let y_pred = w * x + b;
let loss = (y_pred - y_true).powi(2);  // MSE

// Backward pass (hand-coded derivatives)
let d_loss = 2.0 * (y_pred - y_true);
let d_w = d_loss * x;  // ∂loss/∂w = ∂loss/∂y * ∂y/∂w
let d_b = d_loss * 1.0;  // ∂loss/∂b = ∂loss/∂y * ∂y/∂b

// Update
w -= learning_rate * d_w;
b -= learning_rate * d_b;
<span class="boring">}</span></code></pre></pre>
<p><strong>Problems with manual derivatives:</strong></p>
<ul>
<li>❌ Error-prone (easy to make mistakes in chain rule)</li>
<li>❌ Doesn't scale (complex models have thousands of operations)</li>
<li>❌ Hard to maintain (changing forward pass requires rewriting backward pass)</li>
<li>❌ No validation (how do you know your derivatives are correct?)</li>
</ul>
<h2 id="the-solution-automatic-differentiation"><a class="header" href="#the-solution-automatic-differentiation">The Solution: Automatic Differentiation</a></h2>
<p>Entrenar's autograd engine <strong>automatically computes correct derivatives</strong> for any computation:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use entrenar::{Tensor, backward};

// Forward pass (same as before)
let x = Tensor::from_vec(vec![2.0], false);
let w = Tensor::from_vec(vec![3.0], true);  // requires_grad=true
let b = Tensor::from_vec(vec![1.0], true);

let y_pred = &amp;(&amp;w * &amp;x) + &amp;b;  // y = w*x + b = 7.0
let y_true = Tensor::from_vec(vec![10.0], false);

let diff = &amp;y_pred - &amp;y_true;
let loss = &amp;diff * &amp;diff;  // loss = 9.0

// Backward pass (automatic!)
backward(&amp;loss);

// Gradients computed automatically
println!("∂loss/∂w = {}", w.grad()[0]);  // -12.0 ✅ Correct!
println!("∂loss/∂b = {}", b.grad()[0]);  // -6.0 ✅ Correct!
<span class="boring">}</span></code></pre></pre>
<p><strong>Benefits of autograd:</strong></p>
<ul>
<li>✅ Correct by construction (no manual derivative errors)</li>
<li>✅ Scales to any complexity (transformers, ResNets, etc.)</li>
<li>✅ Easy to maintain (change forward pass, backward automatically updates)</li>
<li>✅ Validated with gradient checking (10K+ test cases)</li>
</ul>
<h2 id="how-autograd-works"><a class="header" href="#how-autograd-works">How Autograd Works</a></h2>
<p>Entrenar uses <strong>reverse-mode automatic differentiation</strong> (also called backpropagation).</p>
<h3 id="three-modes-of-differentiation"><a class="header" href="#three-modes-of-differentiation">Three Modes of Differentiation</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Mode</th><th>Description</th><th>Complexity</th><th>Use Case</th></tr></thead><tbody>
<tr><td><strong>Numerical</strong></td><td>Finite differences: <code>f'(x) ≈ (f(x+ε) - f(x)) / ε</code></td><td>O(n) evaluations</td><td>Gradient checking</td></tr>
<tr><td><strong>Symbolic</strong></td><td>Algebraic manipulation: <code>d/dx(x²) = 2x</code></td><td>Exponential growth</td><td>Computer algebra systems</td></tr>
<tr><td><strong>Automatic</strong></td><td>Chain rule on computation graph</td><td>O(1) per operation</td><td>Deep learning</td></tr>
</tbody></table>
</div>
<h3 id="reverse-mode-differentiation"><a class="header" href="#reverse-mode-differentiation">Reverse-Mode Differentiation</a></h3>
<p>Given a computation <code>y = f(g(h(x)))</code>, we want <code>dy/dx</code>.</p>
<p><strong>Forward Pass</strong> (compute outputs):</p>
<pre><code>x → h(x) → g(h(x)) → f(g(h(x))) = y
</code></pre>
<p><strong>Backward Pass</strong> (compute gradients via chain rule):</p>
<pre><code>dy/dx ← dy/dg * dg/dh ← dy/dg ← dy/dy = 1.0
</code></pre>
<p><strong>Key insight</strong>: We only need to store intermediate values and apply the chain rule in reverse.</p>
<h3 id="example-y--x²"><a class="header" href="#example-y--x²">Example: y = x²</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let x = Tensor::from_vec(vec![3.0], true);
let y = &amp;x * &amp;x;  // y = x²

backward(&amp;y);  // Compute dy/dx

println!("dy/dx = {}", x.grad()[0]);  // 6.0 (= 2*x)
<span class="boring">}</span></code></pre></pre>
<p><strong>What happened:</strong></p>
<ol>
<li>
<p><strong>Forward pass</strong>:</p>
<ul>
<li>Compute <code>y = x * x = 9.0</code></li>
<li>Record operation: <code>Mul(x, x) -&gt; y</code></li>
</ul>
</li>
<li>
<p><strong>Backward pass</strong> (starting from <code>dy/dy = 1.0</code>):</p>
<ul>
<li><code>dy/dx_left = dy/dy * x_right = 1.0 * 3.0 = 3.0</code></li>
<li><code>dy/dx_right = dy/dy * x_left = 1.0 * 3.0 = 3.0</code></li>
<li><code>dy/dx = dy/dx_left + dy/dx_right = 6.0</code> (gradient accumulation)</li>
</ul>
</li>
</ol>
<h2 id="computational-graph"><a class="header" href="#computational-graph">Computational Graph</a></h2>
<p>Autograd builds a <strong>computational graph</strong> representing the sequence of operations:</p>
<pre><code>Example: z = (x + y) * (x - y)

Graph:
       x      y
       │      │
       ├──────┤
       │      │
       ▼      ▼
      Add    Sub
       │      │
       └──────┘
          │
          ▼
         Mul
          │
          ▼
          z
</code></pre>
<h3 id="tape-based-implementation"><a class="header" href="#tape-based-implementation">Tape-Based Implementation</a></h3>
<p>Entrenar uses a <strong>tape</strong> to record operations during the forward pass:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Forward pass (records operations on tape)
let x = Tensor::from_vec(vec![2.0], true);
let y = Tensor::from_vec(vec![3.0], true);

let a = &amp;x + &amp;y;  // Tape: [Add(x, y) -&gt; a]
let b = &amp;x - &amp;y;  // Tape: [Add(x, y) -&gt; a, Sub(x, y) -&gt; b]
let z = &amp;a * &amp;b;  // Tape: [Add(x, y) -&gt; a, Sub(x, y) -&gt; b, Mul(a, b) -&gt; z]

// Backward pass (replay tape in reverse)
backward(&amp;z);  // Process: Mul -&gt; Sub -&gt; Add
<span class="boring">}</span></code></pre></pre>
<p><strong>Tape structure:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>Tape:
  [0] Add { lhs: x_id, rhs: y_id, out: a_id }
  [1] Sub { lhs: x_id, rhs: y_id, out: b_id }
  [2] Mul { lhs: a_id, rhs: b_id, out: z_id }

Backward (reverse order):
  [2] Mul.backward(): da = b*dz, db = a*dz
  [1] Sub.backward(): dx += 1*db, dy += -1*db
  [0] Add.backward(): dx += 1*da, dy += 1*da
<span class="boring">}</span></code></pre></pre>
<h2 id="supported-operations"><a class="header" href="#supported-operations">Supported Operations</a></h2>
<p>Entrenar provides backward passes for all essential neural network operations:</p>
<h3 id="basic-operations"><a class="header" href="#basic-operations">Basic Operations</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Operation</th><th>Forward</th><th>Backward</th></tr></thead><tbody>
<tr><td><strong>Add</strong></td><td><code>z = x + y</code></td><td><code>dx = dz</code>, <code>dy = dz</code></td></tr>
<tr><td><strong>Sub</strong></td><td><code>z = x - y</code></td><td><code>dx = dz</code>, <code>dy = -dz</code></td></tr>
<tr><td><strong>Mul</strong></td><td><code>z = x * y</code></td><td><code>dx = y*dz</code>, <code>dy = x*dz</code></td></tr>
<tr><td><strong>Div</strong></td><td><code>z = x / y</code></td><td><code>dx = dz/y</code>, <code>dy = -x*dz/y²</code></td></tr>
</tbody></table>
</div>
<h3 id="matrix-operations"><a class="header" href="#matrix-operations">Matrix Operations</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Operation</th><th>Forward</th><th>Backward</th></tr></thead><tbody>
<tr><td><strong>MatMul</strong></td><td><code>C = A @ B</code></td><td><code>dA = dC @ B^T</code>, <code>dB = A^T @ dC</code></td></tr>
</tbody></table>
</div>
<h3 id="activations"><a class="header" href="#activations">Activations</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Operation</th><th>Forward</th><th>Backward</th></tr></thead><tbody>
<tr><td><strong>ReLU</strong></td><td><code>max(0, x)</code></td><td><code>dx = (x &gt; 0) ? dy : 0</code></td></tr>
<tr><td><strong>GELU</strong></td><td><code>x * Φ(x)</code></td><td>Chain rule with Gaussian CDF derivative</td></tr>
<tr><td><strong>Swish</strong></td><td><code>x * sigmoid(x)</code></td><td><code>dx = (swish(x) + sigmoid(x) * (1 - swish(x))) * dy</code></td></tr>
</tbody></table>
</div>
<h3 id="normalization"><a class="header" href="#normalization">Normalization</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Operation</th><th>Forward</th><th>Backward</th></tr></thead><tbody>
<tr><td><strong>LayerNorm</strong></td><td><code>(x - μ) / σ</code></td><td>Mean/variance chain rule</td></tr>
</tbody></table>
</div>
<h3 id="attention"><a class="header" href="#attention">Attention</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Operation</th><th>Forward</th><th>Backward</th></tr></thead><tbody>
<tr><td><strong>Attention</strong></td><td><code>softmax(QK^T/√d)V</code></td><td>Q, K, V gradients via chain rule</td></tr>
</tbody></table>
</div>
<h2 id="gradient-validation"><a class="header" href="#gradient-validation">Gradient Validation</a></h2>
<p>Entrenar validates <strong>every backward pass</strong> with finite difference checking:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[test]
fn test_matmul_backward_gradient_check() {
    let a = Tensor::from_vec(vec![1.0, 2.0, 3.0, 4.0], true);
    let b = Tensor::from_vec(vec![5.0, 6.0, 7.0, 8.0], true);

    let c = matmul(&amp;a, &amp;b, 2, 2, 1);
    backward(&amp;c);

    // Finite difference: f'(x) ≈ (f(x+ε) - f(x-ε)) / 2ε
    let epsilon = 1e-3;
    let threshold = 0.2;  // 20% relative error

    check_gradient(&amp;c, &amp;a, epsilon, threshold);  // ✅ Passes
    check_gradient(&amp;c, &amp;b, epsilon, threshold);  // ✅ Passes
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Zero-tolerance policy:</strong></p>
<ul>
<li>10K+ gradient checking test cases</li>
<li>All operations tested with property-based tests</li>
<li>Mathematical correctness guaranteed</li>
</ul>
<h2 id="autograd-vs-manual-derivatives"><a class="header" href="#autograd-vs-manual-derivatives">Autograd vs Manual Derivatives</a></h2>
<div class="table-wrapper"><table><thead><tr><th>Aspect</th><th>Manual</th><th>Autograd</th></tr></thead><tbody>
<tr><td><strong>Correctness</strong></td><td>Error-prone</td><td>Validated with tests</td></tr>
<tr><td><strong>Scalability</strong></td><td>Doesn't scale</td><td>Handles any model size</td></tr>
<tr><td><strong>Maintainability</strong></td><td>Brittle</td><td>Change forward, backward auto-updates</td></tr>
<tr><td><strong>Development Time</strong></td><td>Hours/days</td><td>Seconds</td></tr>
<tr><td><strong>Performance</strong></td><td>Potentially optimal</td><td>Near-optimal (tape overhead minimal)</td></tr>
</tbody></table>
</div>
<h2 id="common-pitfalls"><a class="header" href="#common-pitfalls">Common Pitfalls</a></h2>
<h3 id="1-forgetting-requires_gradtrue"><a class="header" href="#1-forgetting-requires_gradtrue">1. Forgetting <code>requires_grad=true</code></a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let w = Tensor::from_vec(vec![1.0], false);  // ❌ No gradients
let y = &amp;w * &amp;x;
backward(&amp;y);
println!("{}", w.grad()[0]);  // 0.0 (gradient not computed)

// Fix:
let w = Tensor::from_vec(vec![1.0], true);  // ✅ Gradients enabled
<span class="boring">}</span></code></pre></pre>
<h3 id="2-not-zeroing-gradients"><a class="header" href="#2-not-zeroing-gradients">2. Not Zeroing Gradients</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>for epoch in 0..10 {
    let loss = compute_loss(&amp;model, &amp;data);
    backward(&amp;loss);

    optimizer.step(&amp;mut params);
    // ❌ Gradients accumulate across epochs!

    // Fix:
    model.zero_grad();  // ✅ Clear gradients
}
<span class="boring">}</span></code></pre></pre>
<h3 id="3-in-place-operations"><a class="header" href="#3-in-place-operations">3. In-Place Operations</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let mut x = Tensor::from_vec(vec![1.0, 2.0], true);
x.data_mut()[0] = 5.0;  // ❌ In-place modification breaks graph

// Fix: Create new tensor
let x_new = Tensor::from_vec(vec![5.0, 2.0], true);  // ✅
<span class="boring">}</span></code></pre></pre>
<h2 id="whats-next"><a class="header" href="#whats-next">What's Next?</a></h2>
<ul>
<li><strong><a href="./tape-based-graphs.html">Tape-Based Computation Graphs</a></strong> - Deep dive into Entrenar's tape implementation</li>
<li><strong><a href="./tensor-operations.html">Tensor Operations</a></strong> - Explore all supported operations</li>
<li><strong><a href="./backward-pass.html">Backward Pass</a></strong> - Understand gradient computation mechanics</li>
<li><strong><a href="./finite-difference.html">Finite Difference Validation</a></strong> - Learn gradient checking methodology</li>
</ul>
<h2 id="key-takeaways"><a class="header" href="#key-takeaways">Key Takeaways</a></h2>
<ol>
<li><strong>Autograd automates derivative computation</strong> - no manual chain rule</li>
<li><strong>Reverse-mode differentiation</strong> - efficient for deep learning (many inputs, one output)</li>
<li><strong>Tape-based graph</strong> - records operations during forward pass</li>
<li><strong>Validated with tests</strong> - 10K+ gradient checking cases ensure correctness</li>
<li><strong>Zero-tolerance for bugs</strong> - extreme TDD methodology</li>
</ol>
<hr />
<p><strong>Ready to understand the tape?</strong> Continue to <a href="./tape-based-graphs.html">Tape-Based Computation Graphs</a> →</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../architecture/memory-management.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next prefetch" href="../autograd/tape-based-graphs.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../architecture/memory-management.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next prefetch" href="../autograd/tape-based-graphs.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>

        <script src="../ace.js"></script>
        <script src="../editor.js"></script>
        <script src="../mode-rust.js"></script>
        <script src="../theme-dawn.js"></script>
        <script src="../theme-tomorrow_night.js"></script>

        <script src="../elasticlunr.min.js"></script>
        <script src="../mark.min.js"></script>
        <script src="../searcher.js"></script>

        <script src="../clipboard.min.js"></script>
        <script src="../highlight.js"></script>
        <script src="../book.js"></script>

        <!-- Custom JS scripts -->


    </div>
    </body>
</html>
