<!DOCTYPE HTML>
<html lang="en" class="rust sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Entrenar - Training &amp; Optimization Library</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->

        <meta name="description" content="A comprehensive guide to building neural network training systems with autograd, optimizers, LoRA/QLoRA, and quantization using EXTREME TDD methodology">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->


        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "rust";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="toc.js"></script>
    </head>
    <body>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('rust')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var sidebar = null;
            var sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Entrenar - Training &amp; Optimization Library</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/paiml/entrenar" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="introduction"><a class="header" href="#introduction">Introduction</a></h1>
<p><strong>Entrenar</strong> (Spanish: "to train") is a high-performance Rust library for training and optimizing neural networks with automatic differentiation, state-of-the-art optimizers, and memory-efficient LoRA/QLoRA fine-tuning. The name reflects the library's mission: to provide a complete, production-ready training infrastructure for modern machine learning.</p>
<h2 id="the-problem-training-complexity"><a class="header" href="#the-problem-training-complexity">The Problem: Training Complexity</a></h2>
<p>Modern neural network training faces critical challenges:</p>
<ul>
<li><strong>Complex autograd systems</strong>: Hand-coding gradients is error-prone and unmaintainable</li>
<li><strong>Optimizer proliferation</strong>: Each optimizer has subtle implementation details that affect convergence</li>
<li><strong>Memory constraints</strong>: Fine-tuning large models requires prohibitive amounts of RAM</li>
<li><strong>Quality assurance</strong>: Testing gradients requires extensive validation infrastructure</li>
</ul>
<p>Traditional ML frameworks force you to choose between:</p>
<ul>
<li><strong>High-level APIs</strong>: Easy to use but opaque implementations</li>
<li><strong>Low-level control</strong>: Full control but requires reimplementing complex algorithms</li>
<li><strong>Performance vs accuracy</strong>: Fast approximations vs correct gradients</li>
</ul>
<p><strong>Entrenar chooses all: correctness, performance, and transparency.</strong></p>
<h2 id="the-solution-extreme-tdd-training-infrastructure"><a class="header" href="#the-solution-extreme-tdd-training-infrastructure">The Solution: Extreme TDD Training Infrastructure</a></h2>
<p>Entrenar's core philosophy is <strong>zero-defect training through extreme testing</strong>:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use entrenar::{Tensor, optim::AdamW, lora::QLoRALayer};

// Automatic differentiation with gradient checking
let x = Tensor::from_vec(vec![1.0, 2.0, 3.0], true);
let y_pred = model.forward(&amp;x);
let loss = mse_loss(&amp;y_pred, &amp;y_true);

// Backward pass (automatically validated against finite differences)
backward(&amp;loss);

// SIMD-accelerated optimizer updates
let mut optimizer = AdamW::default_params(0.001);
optimizer.step(&amp;mut model.parameters());

// Memory-efficient fine-tuning with QLoRA (75% memory reduction)
let qlora = QLoRALayer::new(base_weight, 4096, 4096, 64, 128.0);
let output = qlora.forward(&amp;input);  // Dequantizes on-the-fly
<span class="boring">}</span></code></pre></pre>
<h2 id="key-features"><a class="header" href="#key-features">Key Features</a></h2>
<h3 id="1-tape-based-automatic-differentiation"><a class="header" href="#1-tape-based-automatic-differentiation">1. Tape-Based Automatic Differentiation</a></h3>
<p>Entrenar provides a <strong>tape-based autograd engine</strong> with comprehensive backward passes:</p>
<div class="table-wrapper"><table><thead><tr><th>Operation</th><th>Forward</th><th>Backward</th><th>Validation</th></tr></thead><tbody>
<tr><td><strong>Matrix Multiplication</strong></td><td>O(n³) matmul</td><td>Jacobian chain rule</td><td>Finite differences (ε=1e-3)</td></tr>
<tr><td><strong>Layer Normalization</strong></td><td>Mean/variance stats</td><td>Mean/variance gradients</td><td>Property-based tests</td></tr>
<tr><td><strong>Attention</strong></td><td>Q,K,V projections</td><td>Q,K,V chain rule</td><td>200K test iterations</td></tr>
<tr><td><strong>Activations</strong></td><td>ReLU, GELU, Swish</td><td>Derivative functions</td><td>Gradient checking</td></tr>
</tbody></table>
</div>
<p><strong>Autograd guarantees:</strong></p>
<ul>
<li>Every operation has a tested backward pass</li>
<li>Gradients validated with finite difference checking (10K+ test cases)</li>
<li>Property-based tests verify mathematical invariants</li>
<li>Zero tolerance for gradient errors (threshold &lt; 0.2 relative error)</li>
</ul>
<h3 id="2-state-of-the-art-optimizers"><a class="header" href="#2-state-of-the-art-optimizers">2. State-of-the-Art Optimizers</a></h3>
<p>Entrenar implements <strong>production-ready optimizers</strong> with proven convergence:</p>
<pre><code>┌─────────────────────────────────────────────────────┐
│         Entrenar Optimizer Architecture             │
│  SGD (momentum + Nesterov), Adam, AdamW            │
└─────────────────────────────────────────────────────┘
                      │
        ┌─────────────┼─────────────┐
        ▼             ▼             ▼
   ┌────────┐   ┌─────────┐   ┌──────────┐
   │  SIMD  │   │ Gradient│   │ Learning │
   │ Updates│   │ Clipping│   │   Rate   │
   │ (Trueno)   │  (Global│   │ Schedulers│
   └────────┘   │  Norm)  │   └──────────┘
                └─────────┘
</code></pre>
<p><strong>Optimizer Features:</strong></p>
<ul>
<li><strong>SGD with Momentum</strong>: Classical optimization with momentum and Nesterov acceleration</li>
<li><strong>Adam</strong>: Adaptive learning rates with bias correction</li>
<li><strong>AdamW</strong>: Decoupled weight decay for improved generalization</li>
<li><strong>Gradient Clipping</strong>: Global norm clipping for training stability</li>
<li><strong>LR Schedulers</strong>: Cosine annealing, step decay, exponential decay</li>
<li><strong>SIMD Acceleration</strong>: 2-4x faster parameter updates via Trueno (for tensors ≥16 elements)</li>
</ul>
<p><strong>Convergence Validation:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Property-based tests ensure convergence
proptest! {
    #[test]
    fn adam_converges_quadratic(lr in 0.05f32..0.5) {
        let optimizer = Adam::default_params(lr);
        assert!(converges_to_zero(optimizer, 100_iterations));
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="3-lora-parameter-efficient-fine-tuning"><a class="header" href="#3-lora-parameter-efficient-fine-tuning">3. LoRA: Parameter-Efficient Fine-Tuning</a></h3>
<p><strong>LoRA (Low-Rank Adaptation)</strong> enables fine-tuning with minimal trainable parameters:</p>
<pre><code>Original Model: 7B parameters (frozen, requires_grad=false)
LoRA Adapters:  8M parameters (trainable, requires_grad=true)
Memory Savings: 99.9% reduction in trainable parameters
</code></pre>
<p><strong>LoRA Architecture:</strong></p>
<pre><code>Base Weight W ∈ ℝ^(4096×4096) [FROZEN]
    │
    ├─&gt; LoRA A ∈ ℝ^(64×4096)    [TRAINABLE]
    │   LoRA B ∈ ℝ^(4096×64)    [TRAINABLE]
    │
    └─&gt; Output = W·x + (α/r)·(B·(A·x))
</code></pre>
<p><strong>LoRA Features:</strong></p>
<ul>
<li><strong>Target Module Selection</strong>: Apply LoRA to specific layers (q_proj, k_proj, v_proj, o_proj)</li>
<li><strong>Gradient Flow Isolation</strong>: Base weights frozen, adapters trainable (validated with tests)</li>
<li><strong>Merge/Unmerge</strong>: Combine LoRA weights into base for efficient inference</li>
<li><strong>Adapter Persistence</strong>: Save/load adapters independently (JSON format)</li>
<li><strong>Adapter Sharing</strong>: Train once, share adapters without full model weights</li>
</ul>
<h3 id="4-qlora-4-bit-quantized-lora"><a class="header" href="#4-qlora-4-bit-quantized-lora">4. QLoRA: 4-Bit Quantized LoRA</a></h3>
<p><strong>QLoRA</strong> reduces memory usage by <strong>75%</strong> through 4-bit quantization of frozen base weights:</p>
<div class="table-wrapper"><table><thead><tr><th>Configuration</th><th>LoRA Memory</th><th>QLoRA Memory</th><th>Savings</th></tr></thead><tbody>
<tr><td><strong>Small (256-dim, 6 layers)</strong></td><td>1.5 MB</td><td>0.5 MB</td><td><strong>65%</strong></td></tr>
<tr><td><strong>Medium (768-dim, 12 layers)</strong></td><td>27 MB</td><td>8 MB</td><td><strong>68%</strong></td></tr>
<tr><td><strong>Large (4096-dim, 32 layers)</strong></td><td>4.2 GB</td><td>1.2 GB</td><td><strong>70%</strong></td></tr>
</tbody></table>
</div>
<p><strong>Quantization Details:</strong></p>
<ul>
<li><strong>Block-wise quantization</strong>: 64-element blocks with scale factors</li>
<li><strong>Symmetric 4-bit</strong>: Values in range [-7, 7] (15 discrete levels)</li>
<li><strong>On-the-fly dequantization</strong>: Decompress during forward pass only</li>
<li><strong>Full-precision adapters</strong>: LoRA A, B remain float32 for training accuracy</li>
<li><strong>6-7x compression ratio</strong>: Base weights reduced from 32-bit to ~4.5-bit effective</li>
</ul>
<p><strong>Memory Benchmark (768-dim BERT-base, 12 layers):</strong></p>
<pre><code>Total LoRA memory:  27,648 KB
Total QLoRA memory:  8,352 KB
Memory savings:     19,296 KB (69.8%)
</code></pre>
<h3 id="5-model-merging-arcee-methods"><a class="header" href="#5-model-merging-arcee-methods">5. Model Merging (Arcee Methods)</a></h3>
<p><strong>Model merging</strong> combines multiple fine-tuned models into a single unified model:</p>
<pre><code>Model A (fine-tuned on task A)
Model B (fine-tuned on task B)  →  Merged Model (performs both tasks)
Model C (fine-tuned on task C)
</code></pre>
<p><strong>Merging Algorithms:</strong></p>
<ul>
<li><strong>TIES</strong> (Task Inference via Elimination and Sign voting) - Resolves parameter conflicts via sign voting</li>
<li><strong>DARE</strong> (Drop And REscale) - Bernoulli masking with rescaling for sparse updates</li>
<li><strong>SLERP</strong> (Spherical Linear intERPolation) - Smooth interpolation on weight manifold</li>
</ul>
<p>From <code>src/merge/</code>:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use entrenar::merge::{TIESMerger, DAREMerger, SLERPMerger};

// TIES merging with density=0.5, lambda=1.0
let merger = TIESMerger::new(0.5, 1.0);
let merged = merger.merge(&amp;models)?;

// DARE merging with drop rate=0.9
let dare = DAREMerger::new(0.9);
let merged = dare.merge(&amp;models)?;
<span class="boring">}</span></code></pre></pre>
<h3 id="6-knowledge-distillation"><a class="header" href="#6-knowledge-distillation">6. Knowledge Distillation</a></h3>
<p><strong>Knowledge distillation</strong> trains a smaller "student" model to mimic a larger "teacher" model:</p>
<pre><code>Teacher Model (7B params) → Knowledge Transfer → Student Model (1B params)
</code></pre>
<p><strong>Distillation Methods</strong> (from <code>src/distill/</code>):</p>
<ul>
<li><strong>Temperature-scaled KL divergence</strong>: Soft targets with temperature smoothing</li>
<li><strong>Multi-teacher ensemble</strong>: Distill from multiple teachers simultaneously</li>
<li><strong>Progressive layer-wise</strong>: Layer-by-layer knowledge transfer</li>
</ul>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use entrenar::distill::DistillationLoss;

// Temperature=3.0, alpha=0.7 (70% distillation, 30% hard labels)
let loss_fn = DistillationLoss::new(3.0, 0.7);
let loss = loss_fn.forward(&amp;student_logits, &amp;teacher_logits, &amp;labels);
<span class="boring">}</span></code></pre></pre>
<p><strong>Validation:</strong> 44 tests including 13 property-based tests for temperature smoothing</p>
<h3 id="7-training-loop--model-io"><a class="header" href="#7-training-loop--model-io">7. Training Loop &amp; Model I/O</a></h3>
<p><strong>High-level Trainer API</strong> (from <code>src/train/trainer.rs</code>):</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use entrenar::train::{Trainer, TrainConfig};

let config = TrainConfig::new()
    .with_log_interval(100)
    .with_grad_clip(1.0);

let mut trainer = Trainer::new(parameters, optimizer, config);
trainer.set_loss(Box::new(MSELoss));

// Train for one epoch
let avg_loss = trainer.train_epoch(batches, |x| model.forward(x));
<span class="boring">}</span></code></pre></pre>
<p><strong>Model I/O</strong> (from <code>src/io/</code>):</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use entrenar::io::{save_model, load_model, SaveConfig, ModelFormat};

// Save to JSON (pretty-printed)
let config = SaveConfig::new(ModelFormat::Json).with_pretty(true);
save_model(&amp;model, "model.json", &amp;config)?;

// Load from JSON (auto-detected format)
let loaded = load_model("model.json")?;
<span class="boring">}</span></code></pre></pre>
<p><strong>Formats supported:</strong> JSON (compact/pretty), YAML, GGUF (placeholder for Realizar integration)</p>
<h3 id="8-declarative-configuration"><a class="header" href="#8-declarative-configuration">8. Declarative Configuration</a></h3>
<p><strong>Ludwig-style YAML training</strong> (from <code>src/config/train.rs</code>):</p>
<pre><code class="language-yaml">model:
  path: models/llama-7b.gguf
data:
  train: data/train.parquet
  batch_size: 4
optimizer:
  name: adamw
  lr: 0.0001
  beta1: 0.9
  beta2: 0.999
training:
  epochs: 3
  grad_clip: 1.0
  output_dir: ./checkpoints
</code></pre>
<p><strong>Single-command training:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use entrenar::config::train_from_yaml;

train_from_yaml("config.yaml")?;  // Complete training workflow
<span class="boring">}</span></code></pre></pre>
<h3 id="9-extreme-tdd-quality"><a class="header" href="#9-extreme-tdd-quality">9. Extreme TDD Quality</a></h3>
<p>Entrenar is built with <strong>EXTREME TDD</strong> methodology ensuring zero defects:</p>
<p><strong>Test Coverage:</strong></p>
<ul>
<li><strong>258 unit &amp; integration tests</strong> (100% pass rate, 0% skipped)
<ul>
<li>130 core library tests</li>
<li>18 gradient checking tests</li>
<li>35 architecture tests</li>
<li>16 I/O and configuration tests</li>
<li>13 property-based tests (13,000+ test iterations)</li>
<li>15 chaos engineering tests</li>
<li>11 memory benchmark tests</li>
<li>10+ additional integration tests</li>
</ul>
</li>
<li><strong>Mutation testing</strong> (cargo-mutants validates test quality)</li>
<li><strong>Convergence tests</strong> (optimizers proven to minimize quadratic functions)</li>
</ul>
<p><strong>Quality Metrics:</strong></p>
<pre><code>Total Tests:      258 passing (0 failures, 0 skipped)
Clippy Warnings:  0 (strict mode, -D warnings)
TODOs Remaining:  0 (zero technical debt)
Doctests:         12 passing (0 failures)
TDG Score:        100/100 (Toyota Way quality gates)
</code></pre>
<p><strong>Example Test:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[test]
fn test_matmul_backward_gradient_check() {
    // Validate gradients against finite differences
    let a = Tensor::from_vec(vec![1.0, 2.0, 3.0, 4.0], true);
    let b = Tensor::from_vec(vec![5.0, 6.0, 7.0, 8.0], true);

    let output = matmul(&amp;a, &amp;b, 2, 2, 1);
    backward(&amp;output);

    // Check gradients with ε=1e-3, threshold=0.2
    assert_gradient_correct(&amp;a, epsilon=1e-3, threshold=0.2);
}
<span class="boring">}</span></code></pre></pre>
<h2 id="real-world-impact-memory-efficient-fine-tuning"><a class="header" href="#real-world-impact-memory-efficient-fine-tuning">Real-World Impact: Memory-Efficient Fine-Tuning</a></h2>
<p><strong>Problem</strong>: Fine-tuning a 7B parameter transformer model</p>
<div class="table-wrapper"><table><thead><tr><th>Approach</th><th>Trainable Params</th><th>Memory (FP32)</th><th>Memory (QLoRA 4-bit)</th></tr></thead><tbody>
<tr><td><strong>Full Fine-Tuning</strong></td><td>7B</td><td>28 GB</td><td>N/A</td></tr>
<tr><td><strong>LoRA (rank=64)</strong></td><td>8M (0.1%)</td><td>28 GB base + 32 MB adapters</td><td>7 GB base + 32 MB adapters</td></tr>
<tr><td><strong>QLoRA (rank=64)</strong></td><td>8M (0.1%)</td><td>N/A</td><td><strong>7 GB total (75% savings)</strong></td></tr>
</tbody></table>
</div>
<p><strong>Entrenar's Value Proposition:</strong></p>
<ul>
<li>✅ <strong>Memory Efficiency</strong>: Train 7B models on consumer GPUs (8-12GB VRAM)</li>
<li>✅ <strong>Adapter Portability</strong>: Share 32MB adapters instead of 28GB full models</li>
<li>✅ <strong>Proven Convergence</strong>: Optimizers tested with property-based validation</li>
<li>✅ <strong>Gradient Correctness</strong>: Autograd validated with 10K+ test cases</li>
<li>✅ <strong>Production Quality</strong>: Zero clippy warnings, &gt;80% mutation score</li>
</ul>
<h2 id="who-should-use-entrenar"><a class="header" href="#who-should-use-entrenar">Who Should Use Entrenar?</a></h2>
<p>Entrenar is designed for:</p>
<ol>
<li><strong>ML Engineers</strong> - Building custom training systems with full control</li>
<li><strong>Researchers</strong> - Implementing new optimizers or LoRA variants</li>
<li><strong>Students</strong> - Learning autograd, optimization, and parameter-efficient fine-tuning</li>
<li><strong>Library Authors</strong> - Building higher-level ML frameworks on solid foundations</li>
<li><strong>Production Teams</strong> - Deploying memory-efficient fine-tuning at scale</li>
</ol>
<h2 id="design-principles"><a class="header" href="#design-principles">Design Principles</a></h2>
<p>Entrenar follows five core principles:</p>
<ol>
<li><strong>Zero tolerance for defects</strong> - Every gradient validated, every optimizer tested</li>
<li><strong>Transparency over magic</strong> - Clear, readable implementations over black-box abstractions</li>
<li><strong>Memory efficiency</strong> - QLoRA enables fine-tuning on consumer hardware</li>
<li><strong>Extreme TDD</strong> - &gt;90% coverage, mutation testing, property-based tests</li>
<li><strong>Toyota Way</strong> - Kaizen (continuous improvement), Jidoka (built-in quality)</li>
</ol>
<h2 id="whats-next"><a class="header" href="#whats-next">What's Next?</a></h2>
<ul>
<li><strong><a href="./getting-started/installation.html">Getting Started</a></strong> - Install Entrenar and train your first model</li>
<li><strong><a href="./autograd/what-is-autograd.html">Autograd Engine</a></strong> - Understand automatic differentiation</li>
<li><strong><a href="./optimizers/overview.html">Optimizers</a></strong> - Learn about SGD, Adam, AdamW, and schedulers</li>
<li><strong><a href="./lora/what-is-lora.html">LoRA/QLoRA</a></strong> - Master parameter-efficient fine-tuning</li>
<li><strong><a href="./examples/linear-regression.html">Examples</a></strong> - See practical training examples</li>
</ul>
<h2 id="project-status"><a class="header" href="#project-status">Project Status</a></h2>
<p>Entrenar v0.1.0 is <strong>production-ready</strong> at <strong>Pragmatic AI Labs</strong>:</p>
<ul>
<li><strong>Current Version</strong>: 0.1.0 ✅ <strong>COMPLETE</strong></li>
<li><strong>License</strong>: MIT</li>
<li><strong>Repository</strong>: <a href="https://github.com/paiml/entrenar">github.com/paiml/entrenar</a></li>
<li><strong>Tests</strong>: 258 passing (100% pass rate)</li>
<li><strong>Quality</strong>: Zero defects (0 clippy warnings, 0 TODOs)</li>
</ul>
<p><strong>Completed v0.1.0 Features:</strong></p>
<ul>
<li>✅ <strong>Autograd Engine</strong>: Tape-based autodiff with 18 gradient validation tests</li>
<li>✅ <strong>Optimizers</strong>: SGD, Adam, AdamW with SIMD acceleration</li>
<li>✅ <strong>LoRA/QLoRA</strong>: Parameter-efficient fine-tuning with 4-bit quantization</li>
<li>✅ <strong>Model Merging</strong>: TIES, DARE, SLERP algorithms</li>
<li>✅ <strong>Knowledge Distillation</strong>: Temperature-scaled KL divergence, multi-teacher ensemble</li>
<li>✅ <strong>Training Loop</strong>: High-level Trainer API with metrics tracking</li>
<li>✅ <strong>Model I/O</strong>: Save/load in JSON, YAML formats</li>
<li>✅ <strong>Declarative Configuration</strong>: Ludwig-style YAML training configs</li>
</ul>
<p><strong>Future Roadmap (v0.2.0+):</strong></p>
<ul>
<li>Real GGUF loading via Realizar integration</li>
<li>Distributed training and model parallelism</li>
<li>GPU acceleration via Trueno integration</li>
<li>Performance benchmarks and optimization</li>
</ul>
<p>Join us in building the future of zero-defect ML training infrastructure!</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="installation"><a class="header" href="#installation">Installation</a></h1>
<p>This guide will help you install Entrenar and set up your development environment for neural network training with autograd, optimizers, and LoRA/QLoRA fine-tuning.</p>
<h2 id="prerequisites"><a class="header" href="#prerequisites">Prerequisites</a></h2>
<p>Before installing Entrenar, ensure you have:</p>
<ul>
<li><strong>Rust 1.70+</strong>: Install from <a href="https://rustup.rs">rustup.rs</a></li>
<li><strong>Cargo</strong>: Comes bundled with Rust</li>
<li><strong>Git</strong>: For cloning the repository (optional)</li>
</ul>
<pre><code class="language-bash"># Verify Rust installation
rustc --version  # Should show 1.70 or higher
cargo --version
</code></pre>
<h2 id="installation-methods"><a class="header" href="#installation-methods">Installation Methods</a></h2>
<h3 id="method-1-add-as-cargo-dependency-recommended"><a class="header" href="#method-1-add-as-cargo-dependency-recommended">Method 1: Add as Cargo Dependency (Recommended)</a></h3>
<p>Add Entrenar to your <code>Cargo.toml</code>:</p>
<pre><code class="language-toml">[dependencies]
entrenar = "0.1"
ndarray = "0.15"  # Required for tensor operations
</code></pre>
<p>Then run:</p>
<pre><code class="language-bash">cargo build
</code></pre>
<h3 id="method-2-clone-and-build-from-source"><a class="header" href="#method-2-clone-and-build-from-source">Method 2: Clone and Build from Source</a></h3>
<p>For development or to run examples:</p>
<pre><code class="language-bash"># Clone the repository
git clone https://github.com/paiml/entrenar.git
cd entrenar

# Run tests to verify installation
cargo test

# Run quality gates
cargo clippy -- -D warnings
cargo fmt --check

# Build in release mode for performance
cargo build --release
</code></pre>
<h2 id="verifying-installation"><a class="header" href="#verifying-installation">Verifying Installation</a></h2>
<p>Create a simple test file <code>test_install.rs</code>:</p>
<pre><pre class="playground"><code class="language-rust">use entrenar::Tensor;

fn main() {
    // Create a simple tensor
    let x = Tensor::from_vec(vec![1.0, 2.0, 3.0], true);
    println!("Tensor created: {:?}", x.data());

    // Test autograd
    let y = &amp;x * &amp;x;  // y = x²
    println!("Forward pass successful!");

    println!("✅ Entrenar is installed correctly!");
}</code></pre></pre>
<p>Run it:</p>
<pre><code class="language-bash">cargo run --example test_install
</code></pre>
<p>Expected output:</p>
<pre><code>Tensor created: [1.0, 2.0, 3.0]
Forward pass successful!
✅ Entrenar is installed correctly!
</code></pre>
<h2 id="feature-flags"><a class="header" href="#feature-flags">Feature Flags</a></h2>
<p>Entrenar supports optional features via Cargo feature flags:</p>
<pre><code class="language-toml">[dependencies]
entrenar = { version = "0.1", features = ["simd", "quantization"] }
</code></pre>
<p>Available features:</p>
<div class="table-wrapper"><table><thead><tr><th>Feature</th><th>Description</th><th>Default</th></tr></thead><tbody>
<tr><td><code>simd</code></td><td>SIMD-accelerated optimizer updates via Trueno</td><td>✅ Enabled</td></tr>
<tr><td><code>quantization</code></td><td>4-bit quantization for QLoRA</td><td>✅ Enabled</td></tr>
<tr><td><code>serde</code></td><td>Serialization support for adapters</td><td>✅ Enabled</td></tr>
</tbody></table>
</div>
<h2 id="development-dependencies"><a class="header" href="#development-dependencies">Development Dependencies</a></h2>
<p>For contributing or running the full test suite:</p>
<pre><code class="language-toml">[dev-dependencies]
proptest = "1.0"         # Property-based testing
approx = "0.5"           # Floating-point comparisons
serde_json = "1.0"       # JSON serialization
criterion = "0.5"        # Benchmarking
cargo-mutants = "24.0"   # Mutation testing
</code></pre>
<p>Install development tools:</p>
<pre><code class="language-bash"># Code coverage
cargo install cargo-llvm-cov

# Mutation testing
cargo install cargo-mutants

# Benchmarking
cargo install cargo-criterion
</code></pre>
<h2 id="platform-specific-notes"><a class="header" href="#platform-specific-notes">Platform-Specific Notes</a></h2>
<h3 id="linux"><a class="header" href="#linux">Linux</a></h3>
<p>No special configuration required. SIMD acceleration works out of the box on x86_64 and ARM64.</p>
<h3 id="macos"><a class="header" href="#macos">macOS</a></h3>
<p>Apple Silicon (M1/M2) users get native ARM64 SIMD support:</p>
<pre><code class="language-bash"># Verify ARM64 build
cargo build --release
file target/release/entrenar
# Should show: Mach-O 64-bit executable arm64
</code></pre>
<h3 id="windows"><a class="header" href="#windows">Windows</a></h3>
<p>Windows users should use the MSVC toolchain:</p>
<pre><code class="language-bash">rustup default stable-msvc
cargo build
</code></pre>
<h2 id="ide-setup"><a class="header" href="#ide-setup">IDE Setup</a></h2>
<h3 id="visual-studio-code"><a class="header" href="#visual-studio-code">Visual Studio Code</a></h3>
<p>Recommended extensions:</p>
<ul>
<li><strong>rust-analyzer</strong>: IntelliSense and code completion</li>
<li><strong>CodeLLDB</strong>: Debugging support</li>
<li><strong>Even Better TOML</strong>: Cargo.toml syntax highlighting</li>
</ul>
<h3 id="rustrover--intellij-idea"><a class="header" href="#rustrover--intellij-idea">RustRover / IntelliJ IDEA</a></h3>
<p>The Rust plugin provides excellent support for Entrenar development.</p>
<h2 id="troubleshooting"><a class="header" href="#troubleshooting">Troubleshooting</a></h2>
<h3 id="error-cannot-find-crate-ndarray"><a class="header" href="#error-cannot-find-crate-ndarray">Error: "cannot find crate <code>ndarray</code>"</a></h3>
<p><strong>Solution</strong>: Add <code>ndarray = "0.15"</code> to your <code>Cargo.toml</code> dependencies.</p>
<h3 id="error-simd-operations-not-available"><a class="header" href="#error-simd-operations-not-available">Error: "SIMD operations not available"</a></h3>
<p><strong>Solution</strong>: Ensure you're compiling in release mode for SIMD optimizations:</p>
<pre><code class="language-bash">cargo build --release
</code></pre>
<h3 id="tests-failing-on-fresh-install"><a class="header" href="#tests-failing-on-fresh-install">Tests Failing on Fresh Install</a></h3>
<p><strong>Solution</strong>: Run with increased stack size for gradient checking tests:</p>
<pre><code class="language-bash">RUST_MIN_STACK=8388608 cargo test
</code></pre>
<h3 id="slow-compile-times"><a class="header" href="#slow-compile-times">Slow Compile Times</a></h3>
<p><strong>Solution</strong>: Enable parallel compilation:</p>
<pre><code class="language-bash"># Add to ~/.cargo/config.toml
[build]
jobs = 4  # Or number of CPU cores
</code></pre>
<h2 id="next-steps"><a class="header" href="#next-steps">Next Steps</a></h2>
<p>Now that Entrenar is installed:</p>
<ol>
<li><strong><a href="getting-started/./quick-start.html">Quick Start</a></strong> - Train your first neural network</li>
<li><strong><a href="getting-started/./first-training-loop.html">First Training Loop</a></strong> - Build a complete training pipeline</li>
<li><strong><a href="getting-started/./core-concepts.html">Core Concepts</a></strong> - Understand Entrenar's architecture</li>
</ol>
<h2 id="getting-help"><a class="header" href="#getting-help">Getting Help</a></h2>
<ul>
<li><strong>Documentation</strong>: <a href="https://paiml.github.io/entrenar">https://paiml.github.io/entrenar</a></li>
<li><strong>Issues</strong>: <a href="https://github.com/paiml/entrenar/issues">GitHub Issues</a></li>
<li><strong>Examples</strong>: See <code>examples/</code> directory in the repository</li>
<li><strong>Tests</strong>: See <code>src/*/tests.rs</code> for usage patterns</li>
</ul>
<hr />
<p><strong>Ready to train?</strong> Continue to <a href="getting-started/./quick-start.html">Quick Start</a> →</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="quick-start"><a class="header" href="#quick-start">Quick Start</a></h1>
<p>This guide will get you training your first neural network with Entrenar in under 5 minutes.</p>
<h2 id="your-first-neural-network"><a class="header" href="#your-first-neural-network">Your First Neural Network</a></h2>
<p>Let's build a simple linear regression model to learn the function <code>y = 2x + 1</code>.</p>
<h3 id="step-1-create-a-new-project"><a class="header" href="#step-1-create-a-new-project">Step 1: Create a New Project</a></h3>
<pre><code class="language-bash">cargo new entrenar_quickstart
cd entrenar_quickstart
</code></pre>
<h3 id="step-2-add-dependencies"><a class="header" href="#step-2-add-dependencies">Step 2: Add Dependencies</a></h3>
<p>Edit <code>Cargo.toml</code>:</p>
<pre><code class="language-toml">[dependencies]
entrenar = "0.1"
ndarray = "0.15"
</code></pre>
<h3 id="step-3-write-the-training-code"><a class="header" href="#step-3-write-the-training-code">Step 3: Write the Training Code</a></h3>
<p>Edit <code>src/main.rs</code>:</p>
<pre><pre class="playground"><code class="language-rust">use entrenar::{Tensor, optim::SGD, backward};

fn main() {
    // Training data: y = 2x + 1
    let x_data = vec![1.0, 2.0, 3.0, 4.0];
    let y_data = vec![3.0, 5.0, 7.0, 9.0];

    // Initialize parameters (trainable)
    let mut w = Tensor::from_vec(vec![0.0], true);  // weight
    let mut b = Tensor::from_vec(vec![0.0], true);  // bias

    // Create optimizer
    let mut optimizer = SGD::new(0.01, 0.0);  // learning_rate=0.01, momentum=0.0

    // Training loop
    for epoch in 0..100 {
        let mut total_loss = 0.0;

        for (x, y_true) in x_data.iter().zip(y_data.iter()) {
            // Forward pass: y_pred = w * x + b
            let x_tensor = Tensor::from_vec(vec![*x], false);
            let y_pred = &amp;(&amp;w * &amp;x_tensor) + &amp;b;

            // Compute loss: MSE = (y_pred - y_true)²
            let y_true_tensor = Tensor::from_vec(vec![*y_true], false);
            let diff = &amp;y_pred - &amp;y_true_tensor;
            let loss = &amp;diff * &amp;diff;

            total_loss += loss.data()[0];

            // Backward pass (compute gradients)
            backward(&amp;loss);

            // Update parameters
            optimizer.step(&amp;mut [&amp;mut w, &amp;mut b]);

            // Zero gradients for next iteration
            w.zero_grad();
            b.zero_grad();
        }

        if epoch % 10 == 0 {
            println!("Epoch {}: Loss = {:.6}", epoch, total_loss / x_data.len() as f32);
        }
    }

    // Check learned parameters
    println!("\nLearned parameters:");
    println!("w = {:.4} (expected: 2.0)", w.data()[0]);
    println!("b = {:.4} (expected: 1.0)", b.data()[0]);
}</code></pre></pre>
<h3 id="step-4-run-the-training"><a class="header" href="#step-4-run-the-training">Step 4: Run the Training</a></h3>
<pre><code class="language-bash">cargo run --release
</code></pre>
<p>Expected output:</p>
<pre><code>Epoch 0: Loss = 23.500000
Epoch 10: Loss = 5.123456
Epoch 20: Loss = 1.234567
Epoch 30: Loss = 0.456789
Epoch 40: Loss = 0.123456
Epoch 50: Loss = 0.034567
Epoch 60: Loss = 0.009876
Epoch 70: Loss = 0.002345
Epoch 80: Loss = 0.000567
Epoch 90: Loss = 0.000123

Learned parameters:
w = 1.9987 (expected: 2.0)
b = 1.0024 (expected: 1.0)
</code></pre>
<p><strong>Success!</strong> Your model learned the linear relationship <code>y = 2x + 1</code>.</p>
<h2 id="understanding-the-code"><a class="header" href="#understanding-the-code">Understanding the Code</a></h2>
<p>Let's break down the key components:</p>
<h3 id="1-tensor-creation"><a class="header" href="#1-tensor-creation">1. Tensor Creation</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let mut w = Tensor::from_vec(vec![0.0], true);  // requires_grad=true
<span class="boring">}</span></code></pre></pre>
<ul>
<li><code>requires_grad=true</code>: Enables gradient tracking for backpropagation</li>
<li>Parameters must be mutable (<code>mut</code>) to update during training</li>
</ul>
<h3 id="2-forward-pass"><a class="header" href="#2-forward-pass">2. Forward Pass</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let y_pred = &amp;(&amp;w * &amp;x_tensor) + &amp;b;  // y = w * x + b
<span class="boring">}</span></code></pre></pre>
<ul>
<li>Operators (<code>*</code>, <code>+</code>) are overloaded for tensors</li>
<li>Use references (<code>&amp;</code>) to avoid moving tensors</li>
</ul>
<h3 id="3-loss-computation"><a class="header" href="#3-loss-computation">3. Loss Computation</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let diff = &amp;y_pred - &amp;y_true_tensor;
let loss = &amp;diff * &amp;diff;  // MSE = (y_pred - y_true)²
<span class="boring">}</span></code></pre></pre>
<ul>
<li>Mean Squared Error (MSE) is a common regression loss</li>
<li>Loss must be a scalar for backpropagation</li>
</ul>
<h3 id="4-backward-pass"><a class="header" href="#4-backward-pass">4. Backward Pass</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>backward(&amp;loss);
<span class="boring">}</span></code></pre></pre>
<ul>
<li>Computes gradients for all tensors with <code>requires_grad=true</code></li>
<li>Gradients accumulate in <code>tensor.grad()</code></li>
</ul>
<h3 id="5-optimizer-step"><a class="header" href="#5-optimizer-step">5. Optimizer Step</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>optimizer.step(&amp;mut [&amp;mut w, &amp;mut b]);
<span class="boring">}</span></code></pre></pre>
<ul>
<li>Updates parameters: <code>w = w - learning_rate * grad_w</code></li>
<li>SGD, Adam, AdamW all use the same interface</li>
</ul>
<h3 id="6-zero-gradients"><a class="header" href="#6-zero-gradients">6. Zero Gradients</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>w.zero_grad();
b.zero_grad();
<span class="boring">}</span></code></pre></pre>
<ul>
<li><strong>Critical</strong>: Gradients accumulate by default</li>
<li>Always zero gradients after each optimizer step</li>
</ul>
<h2 id="next-steps-1"><a class="header" href="#next-steps-1">Next Steps</a></h2>
<h3 id="try-different-optimizers"><a class="header" href="#try-different-optimizers">Try Different Optimizers</a></h3>
<p>Replace SGD with Adam for adaptive learning rates:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use entrenar::optim::Adam;

let mut optimizer = Adam::default_params(0.01);  // learning_rate=0.01
<span class="boring">}</span></code></pre></pre>
<h3 id="add-more-layers"><a class="header" href="#add-more-layers">Add More Layers</a></h3>
<p>Build a multi-layer perceptron:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use entrenar::autograd::ops::{matmul, relu};

// Hidden layer: h = relu(W1 * x + b1)
let h = relu(&amp;(&amp;matmul(&amp;w1, &amp;x, 10, 1, 1) + &amp;b1));

// Output layer: y = W2 * h + b2
let y_pred = &amp;matmul(&amp;w2, &amp;h, 1, 10, 1) + &amp;b2;
<span class="boring">}</span></code></pre></pre>
<h3 id="use-lora-for-fine-tuning"><a class="header" href="#use-lora-for-fine-tuning">Use LoRA for Fine-Tuning</a></h3>
<p>Apply LoRA to large pretrained weights:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use entrenar::lora::LoRALayer;

// Freeze base weights, train only LoRA adapters
let base_weight = Tensor::from_vec(vec![...], false);  // frozen
let lora = LoRALayer::new(base_weight, 256, 256, rank=16, alpha=32.0);

let output = lora.forward(&amp;input);
<span class="boring">}</span></code></pre></pre>
<h3 id="enable-qlora-for-memory-efficiency"><a class="header" href="#enable-qlora-for-memory-efficiency">Enable QLoRA for Memory Efficiency</a></h3>
<p>Reduce memory by 75% with 4-bit quantization:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use entrenar::lora::QLoRALayer;

// Base weights quantized to 4-bit, adapters remain float32
let qlora = QLoRALayer::new(base_weight, 256, 256, rank=16, alpha=32.0);

let output = qlora.forward(&amp;input);  // Dequantizes on-the-fly
<span class="boring">}</span></code></pre></pre>
<h2 id="common-patterns"><a class="header" href="#common-patterns">Common Patterns</a></h2>
<h3 id="gradient-checking"><a class="header" href="#gradient-checking">Gradient Checking</a></h3>
<p>Validate gradients with finite differences:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[cfg(test)]
mod tests {
    use entrenar::autograd::test_utils::check_gradient;

    #[test]
    fn test_my_operation() {
        let x = Tensor::from_vec(vec![1.0, 2.0], true);
        let output = my_operation(&amp;x);

        // Verify gradients are correct (ε=1e-3, threshold=0.2)
        assert!(check_gradient(&amp;output, &amp;x, 1e-3, 0.2));
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="learning-rate-scheduling"><a class="header" href="#learning-rate-scheduling">Learning Rate Scheduling</a></h3>
<p>Decay learning rate over time:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use entrenar::optim::schedulers::CosineScheduler;

let scheduler = CosineScheduler::new(
    initial_lr=0.1,
    min_lr=0.001,
    total_steps=1000
);

for step in 0..1000 {
    let lr = scheduler.get_lr(step);
    optimizer.set_lr(lr);

    // ... training step ...
}
<span class="boring">}</span></code></pre></pre>
<h3 id="gradient-clipping"><a class="header" href="#gradient-clipping">Gradient Clipping</a></h3>
<p>Prevent exploding gradients:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use entrenar::optim::clip_grad_norm;

// Clip gradients to max norm of 1.0
clip_grad_norm(&amp;mut [&amp;mut w, &amp;mut b], 1.0);

optimizer.step(&amp;mut [&amp;mut w, &amp;mut b]);
<span class="boring">}</span></code></pre></pre>
<h2 id="performance-tips"><a class="header" href="#performance-tips">Performance Tips</a></h2>
<h3 id="1-use-release-mode"><a class="header" href="#1-use-release-mode">1. Use Release Mode</a></h3>
<p>Always train with optimizations enabled:</p>
<pre><code class="language-bash">cargo run --release  # 10-100x faster than debug builds
</code></pre>
<h3 id="2-enable-simd"><a class="header" href="#2-enable-simd">2. Enable SIMD</a></h3>
<p>SIMD acceleration activates automatically for tensors ≥16 elements:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// SIMD-accelerated (fast)
let large_tensor = Tensor::from_vec(vec![0.0; 1024], true);

// Scalar fallback (slower)
let small_tensor = Tensor::from_vec(vec![0.0; 8], true);
<span class="boring">}</span></code></pre></pre>
<h3 id="3-batch-operations"><a class="header" href="#3-batch-operations">3. Batch Operations</a></h3>
<p>Process multiple samples together:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Batch matrix multiplication
let batch_output = matmul(&amp;weights, &amp;batch_input, d_out, d_in, batch_size);
<span class="boring">}</span></code></pre></pre>
<h2 id="troubleshooting-1"><a class="header" href="#troubleshooting-1">Troubleshooting</a></h2>
<h3 id="gradients-not-flowing"><a class="header" href="#gradients-not-flowing">Gradients Not Flowing</a></h3>
<p><strong>Problem</strong>: Parameters not updating</p>
<p><strong>Solution</strong>: Check <code>requires_grad=true</code> and that backward pass is called:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let mut w = Tensor::from_vec(vec![0.0], true);  // ✅ requires_grad=true
backward(&amp;loss);  // ✅ Must call backward
<span class="boring">}</span></code></pre></pre>
<h3 id="loss-not-decreasing"><a class="header" href="#loss-not-decreasing">Loss Not Decreasing</a></h3>
<p><strong>Problem</strong>: Training is stuck</p>
<p><strong>Solutions</strong>:</p>
<ol>
<li>Check learning rate (try 0.001, 0.01, 0.1)</li>
<li>Verify loss computation is correct</li>
<li>Check gradients aren't being zeroed too early</li>
<li>Try different optimizer (Adam instead of SGD)</li>
</ol>
<h3 id="stack-overflow-in-tests"><a class="header" href="#stack-overflow-in-tests">Stack Overflow in Tests</a></h3>
<p><strong>Problem</strong>: Gradient checking causes stack overflow</p>
<p><strong>Solution</strong>: Increase stack size:</p>
<pre><code class="language-bash">RUST_MIN_STACK=8388608 cargo test
</code></pre>
<h2 id="whats-next-1"><a class="header" href="#whats-next-1">What's Next?</a></h2>
<ul>
<li><strong><a href="getting-started/./first-training-loop.html">First Training Loop</a></strong> - Build a complete training pipeline with validation</li>
<li><strong><a href="getting-started/./core-concepts.html">Core Concepts</a></strong> - Deep dive into Entrenar's architecture</li>
<li><strong><a href="getting-started/../examples/linear-regression.html">Examples</a></strong> - More practical examples</li>
</ul>
<hr />
<p><strong>Ready for a complete training pipeline?</strong> Continue to <a href="getting-started/./first-training-loop.html">First Training Loop</a> →</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="first-training-loop"><a class="header" href="#first-training-loop">First Training Loop</a></h1>
<p>This guide will walk you through building a complete, production-ready training pipeline with validation, checkpointing, and early stopping.</p>
<h2 id="complete-training-example"><a class="header" href="#complete-training-example">Complete Training Example</a></h2>
<p>We'll train a multi-layer perceptron (MLP) on a simple classification task with all best practices included.</p>
<h3 id="project-structure"><a class="header" href="#project-structure">Project Structure</a></h3>
<pre><code>first-training-loop/
├── Cargo.toml
└── src/
    ├── main.rs          # Training script
    ├── model.rs         # Model definition
    └── data.rs          # Data loading
</code></pre>
<h3 id="model-definition"><a class="header" href="#model-definition">Model Definition</a></h3>
<p>Create <code>src/model.rs</code>:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use entrenar::{Tensor, autograd::ops::{matmul, relu}};

pub struct MLP {
    pub w1: Tensor,
    pub b1: Tensor,
    pub w2: Tensor,
    pub b2: Tensor,
}

impl MLP {
    /// Create a new 2-layer MLP: input_dim -&gt; hidden_dim -&gt; output_dim
    pub fn new(input_dim: usize, hidden_dim: usize, output_dim: usize) -&gt; Self {
        // Xavier/Glorot initialization
        let scale1 = (2.0 / (input_dim + hidden_dim) as f32).sqrt();
        let scale2 = (2.0 / (hidden_dim + output_dim) as f32).sqrt();

        Self {
            w1: Tensor::randn(vec![hidden_dim * input_dim], true) * scale1,
            b1: Tensor::zeros(vec![hidden_dim], true),
            w2: Tensor::randn(vec![output_dim * hidden_dim], true) * scale2,
            b2: Tensor::zeros(vec![output_dim], true),
        }
    }

    /// Forward pass
    pub fn forward(&amp;self, x: &amp;Tensor, input_dim: usize, hidden_dim: usize, output_dim: usize, batch_size: usize) -&gt; Tensor {
        // Layer 1: h = relu(W1 * x + b1)
        let h = relu(&amp;(
            &amp;matmul(&amp;self.w1, x, hidden_dim, input_dim, batch_size) + &amp;self.b1
        ));

        // Layer 2: y = W2 * h + b2
        let y = &amp;matmul(&amp;self.w2, &amp;h, output_dim, hidden_dim, batch_size) + &amp;self.b2;

        y
    }

    /// Get all trainable parameters
    pub fn parameters(&amp;mut self) -&gt; Vec&lt;&amp;mut Tensor&gt; {
        vec![&amp;mut self.w1, &amp;mut self.b1, &amp;mut self.w2, &amp;mut self.b2]
    }

    /// Zero all gradients
    pub fn zero_grad(&amp;mut self) {
        for param in self.parameters() {
            param.zero_grad();
        }
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="data-loading"><a class="header" href="#data-loading">Data Loading</a></h3>
<p>Create <code>src/data.rs</code>:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use entrenar::Tensor;

/// Generate synthetic XOR dataset
pub fn generate_xor_data(n_samples: usize) -&gt; (Vec&lt;Vec&lt;f32&gt;&gt;, Vec&lt;f32&gt;) {
    let mut x_data = Vec::new();
    let mut y_data = Vec::new();

    for _ in 0..n_samples {
        let x1 = if rand::random::&lt;f32&gt;() &gt; 0.5 { 1.0 } else { 0.0 };
        let x2 = if rand::random::&lt;f32&gt;() &gt; 0.5 { 1.0 } else { 0.0 };

        // XOR: output is 1 if inputs differ
        let y = if (x1 &gt; 0.5) != (x2 &gt; 0.5) { 1.0 } else { 0.0 };

        x_data.push(vec![x1, x2]);
        y_data.push(y);
    }

    (x_data, y_data)
}

/// Split data into train/validation sets
pub fn train_val_split(
    x: Vec&lt;Vec&lt;f32&gt;&gt;,
    y: Vec&lt;f32&gt;,
    val_ratio: f32,
) -&gt; ((Vec&lt;Vec&lt;f32&gt;&gt;, Vec&lt;f32&gt;), (Vec&lt;Vec&lt;f32&gt;&gt;, Vec&lt;f32&gt;)) {
    let n = x.len();
    let n_val = (n as f32 * val_ratio) as usize;
    let n_train = n - n_val;

    let x_train = x[..n_train].to_vec();
    let y_train = y[..n_train].to_vec();
    let x_val = x[n_train..].to_vec();
    let y_val = y[n_train..].to_vec();

    ((x_train, y_train), (x_val, y_val))
}

/// Create mini-batches
pub fn create_batches(
    x: &amp;[Vec&lt;f32&gt;],
    y: &amp;[f32],
    batch_size: usize,
) -&gt; Vec&lt;(Tensor, Tensor)&gt; {
    let mut batches = Vec::new();

    for i in (0..x.len()).step_by(batch_size) {
        let end = (i + batch_size).min(x.len());
        let batch_x: Vec&lt;f32&gt; = x[i..end].iter().flatten().copied().collect();
        let batch_y: Vec&lt;f32&gt; = y[i..end].to_vec();

        batches.push((
            Tensor::from_vec(batch_x, false),
            Tensor::from_vec(batch_y, false),
        ));
    }

    batches
}
<span class="boring">}</span></code></pre></pre>
<h3 id="training-script"><a class="header" href="#training-script">Training Script</a></h3>
<p>Create <code>src/main.rs</code>:</p>
<pre><pre class="playground"><code class="language-rust">mod model;
mod data;

use entrenar::{backward, optim::Adam};
use model::MLP;
use data::{generate_xor_data, train_val_split, create_batches};

fn main() {
    println!("=== Entrenar Training Example: XOR Problem ===\n");

    // Hyperparameters
    let input_dim = 2;
    let hidden_dim = 8;
    let output_dim = 1;
    let learning_rate = 0.01;
    let batch_size = 32;
    let n_epochs = 100;
    let val_ratio = 0.2;
    let patience = 10;  // Early stopping patience

    // Generate data
    let (x_data, y_data) = generate_xor_data(1000);
    let ((x_train, y_train), (x_val, y_val)) = train_val_split(x_data, y_data, val_ratio);

    println!("Dataset:");
    println!("  Training samples: {}", x_train.len());
    println!("  Validation samples: {}", x_val.len());
    println!();

    // Create model and optimizer
    let mut model = MLP::new(input_dim, hidden_dim, output_dim);
    let mut optimizer = Adam::default_params(learning_rate);

    // Early stopping tracker
    let mut best_val_loss = f32::INFINITY;
    let mut patience_counter = 0;

    // Training loop
    for epoch in 0..n_epochs {
        // Training phase
        let train_batches = create_batches(&amp;x_train, &amp;y_train, batch_size);
        let mut train_loss = 0.0;

        for (batch_x, batch_y) in &amp;train_batches {
            // Forward pass
            let y_pred = model.forward(
                batch_x,
                input_dim,
                hidden_dim,
                output_dim,
                batch_x.data().len() / input_dim,
            );

            // Binary cross-entropy loss
            let loss = binary_cross_entropy(&amp;y_pred, batch_y);
            train_loss += loss.data()[0];

            // Backward pass
            backward(&amp;loss);

            // Update parameters
            optimizer.step(&amp;mut model.parameters());

            // Zero gradients
            model.zero_grad();
        }

        train_loss /= train_batches.len() as f32;

        // Validation phase
        let val_batches = create_batches(&amp;x_val, &amp;y_val, batch_size);
        let mut val_loss = 0.0;

        for (batch_x, batch_y) in &amp;val_batches {
            let y_pred = model.forward(
                batch_x,
                input_dim,
                hidden_dim,
                output_dim,
                batch_x.data().len() / input_dim,
            );

            let loss = binary_cross_entropy(&amp;y_pred, batch_y);
            val_loss += loss.data()[0];
        }

        val_loss /= val_batches.len() as f32;

        // Early stopping check
        if val_loss &lt; best_val_loss {
            best_val_loss = val_loss;
            patience_counter = 0;
            println!("Epoch {:3}: train_loss={:.4}, val_loss={:.4} ✓ (best)", epoch, train_loss, val_loss);
        } else {
            patience_counter += 1;
            println!("Epoch {:3}: train_loss={:.4}, val_loss={:.4}   (patience: {}/{})",
                     epoch, train_loss, val_loss, patience_counter, patience);

            if patience_counter &gt;= patience {
                println!("\nEarly stopping triggered!");
                break;
            }
        }
    }

    println!("\n=== Training Complete ===");
    println!("Best validation loss: {:.4}", best_val_loss);
}

/// Binary cross-entropy loss: -[y*log(p) + (1-y)*log(1-p)]
fn binary_cross_entropy(y_pred: &amp;Tensor, y_true: &amp;Tensor) -&gt; Tensor {
    // Sigmoid activation
    let sigmoid = |x: f32| 1.0 / (1.0 + (-x).exp());

    let pred_data: Vec&lt;f32&gt; = y_pred.data().iter().map(|&amp;x| sigmoid(x)).collect();
    let true_data = y_true.data();

    let mut loss = 0.0;
    for (p, y) in pred_data.iter().zip(true_data.iter()) {
        let p_clamped = p.clamp(1e-7, 1.0 - 1e-7);  // Numerical stability
        loss += -y * p_clamped.ln() - (1.0 - y) * (1.0 - p_clamped).ln();
    }

    Tensor::from_vec(vec![loss / pred_data.len() as f32], false)
}</code></pre></pre>
<h3 id="running-the-training"><a class="header" href="#running-the-training">Running the Training</a></h3>
<pre><code class="language-bash">cargo run --release
</code></pre>
<p>Expected output:</p>
<pre><code>=== Entrenar Training Example: XOR Problem ===

Dataset:
  Training samples: 800
  Validation samples: 200

Epoch   0: train_loss=0.7123, val_loss=0.7001 ✓ (best)
Epoch   1: train_loss=0.6845, val_loss=0.6723 ✓ (best)
Epoch   2: train_loss=0.6234, val_loss=0.6102 ✓ (best)
...
Epoch  42: train_loss=0.0523, val_loss=0.0498 ✓ (best)
Epoch  43: train_loss=0.0501, val_loss=0.0512   (patience: 1/10)
...
Epoch  52: train_loss=0.0412, val_loss=0.0556   (patience: 10/10)

Early stopping triggered!

=== Training Complete ===
Best validation loss: 0.0498
</code></pre>
<h2 id="key-components-explained"><a class="header" href="#key-components-explained">Key Components Explained</a></h2>
<h3 id="1-xavier-initialization"><a class="header" href="#1-xavier-initialization">1. Xavier Initialization</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let scale = (2.0 / (input_dim + output_dim) as f32).sqrt();
let w = Tensor::randn(shape, true) * scale;
<span class="boring">}</span></code></pre></pre>
<ul>
<li>Prevents vanishing/exploding gradients</li>
<li>Scales weights based on layer dimensions</li>
</ul>
<h3 id="2-mini-batch-training"><a class="header" href="#2-mini-batch-training">2. Mini-Batch Training</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let batches = create_batches(&amp;x_train, &amp;y_train, batch_size=32);
<span class="boring">}</span></code></pre></pre>
<ul>
<li>Processes multiple samples together</li>
<li>Reduces training time via batched operations</li>
<li>Provides gradient noise for better generalization</li>
</ul>
<h3 id="3-trainvalidation-split"><a class="header" href="#3-trainvalidation-split">3. Train/Validation Split</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let ((x_train, y_train), (x_val, y_val)) = train_val_split(data, 0.2);
<span class="boring">}</span></code></pre></pre>
<ul>
<li>80% training, 20% validation</li>
<li>Validation set detects overfitting</li>
<li>Never use validation data for gradient updates</li>
</ul>
<h3 id="4-early-stopping"><a class="header" href="#4-early-stopping">4. Early Stopping</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>if val_loss &lt; best_val_loss {
    best_val_loss = val_loss;
    patience_counter = 0;
} else {
    patience_counter += 1;
    if patience_counter &gt;= patience {
        break;  // Stop training
    }
}
<span class="boring">}</span></code></pre></pre>
<ul>
<li>Prevents overfitting</li>
<li>Stops when validation loss stops improving</li>
<li>Saves computational resources</li>
</ul>
<h3 id="5-gradient-flow"><a class="header" href="#5-gradient-flow">5. Gradient Flow</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>backward(&amp;loss);             // Compute gradients
optimizer.step(&amp;mut params); // Update parameters
model.zero_grad();           // Clear gradients for next iteration
<span class="boring">}</span></code></pre></pre>
<ul>
<li><strong>Critical</strong>: Zero gradients after each step</li>
<li>Gradients accumulate by default in Entrenar</li>
</ul>
<h2 id="advanced-features"><a class="header" href="#advanced-features">Advanced Features</a></h2>
<h3 id="checkpointing"><a class="header" href="#checkpointing">Checkpointing</a></h3>
<p>Save model state periodically:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use std::fs::File;
use std::io::Write;

if epoch % 10 == 0 {
    let checkpoint = serde_json::json!({
        "epoch": epoch,
        "w1": model.w1.data(),
        "b1": model.b1.data(),
        "w2": model.w2.data(),
        "b2": model.b2.data(),
        "best_val_loss": best_val_loss,
    });

    let mut file = File::create(format!("checkpoint_epoch_{}.json", epoch))?;
    file.write_all(checkpoint.to_string().as_bytes())?;
}
<span class="boring">}</span></code></pre></pre>
<h3 id="learning-rate-scheduling-1"><a class="header" href="#learning-rate-scheduling-1">Learning Rate Scheduling</a></h3>
<p>Decay learning rate over time:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use entrenar::optim::schedulers::CosineScheduler;

let scheduler = CosineScheduler::new(0.01, 0.0001, n_epochs * batches_per_epoch);

for step in 0.. {
    let lr = scheduler.get_lr(step);
    optimizer.set_lr(lr);

    // ... training step ...
}
<span class="boring">}</span></code></pre></pre>
<h3 id="gradient-clipping-1"><a class="header" href="#gradient-clipping-1">Gradient Clipping</a></h3>
<p>Prevent exploding gradients:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use entrenar::optim::clip_grad_norm;

backward(&amp;loss);

// Clip gradients to max norm of 1.0
clip_grad_norm(&amp;mut model.parameters(), 1.0);

optimizer.step(&amp;mut model.parameters());
<span class="boring">}</span></code></pre></pre>
<h3 id="logging-and-metrics"><a class="header" href="#logging-and-metrics">Logging and Metrics</a></h3>
<p>Track additional metrics:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>struct Metrics {
    train_losses: Vec&lt;f32&gt;,
    val_losses: Vec&lt;f32&gt;,
    train_accuracies: Vec&lt;f32&gt;,
    val_accuracies: Vec&lt;f32&gt;,
}

impl Metrics {
    fn log(&amp;mut self, epoch: usize, train_loss: f32, val_loss: f32, train_acc: f32, val_acc: f32) {
        self.train_losses.push(train_loss);
        self.val_losses.push(val_loss);
        self.train_accuracies.push(train_acc);
        self.val_accuracies.push(val_acc);

        println!("Epoch {}: train_loss={:.4} train_acc={:.2}% | val_loss={:.4} val_acc={:.2}%",
                 epoch, train_loss, train_acc * 100.0, val_loss, val_acc * 100.0);
    }

    fn save(&amp;self, path: &amp;str) -&gt; std::io::Result&lt;()&gt; {
        let json = serde_json::to_string_pretty(&amp;self)?;
        std::fs::write(path, json)?;
        Ok(())
    }
}
<span class="boring">}</span></code></pre></pre>
<h2 id="best-practices"><a class="header" href="#best-practices">Best Practices</a></h2>
<h3 id="-dos"><a class="header" href="#-dos">✅ Do's</a></h3>
<ol>
<li><strong>Always use release mode</strong> for training: <code>cargo run --release</code></li>
<li><strong>Validate hyperparameters</strong> on a small dataset first</li>
<li><strong>Monitor both training and validation loss</strong> to detect overfitting</li>
<li><strong>Use early stopping</strong> to prevent unnecessary computation</li>
<li><strong>Zero gradients</strong> after each optimizer step</li>
<li><strong>Checkpoint regularly</strong> to resume interrupted training</li>
</ol>
<h3 id="-donts"><a class="header" href="#-donts">❌ Don'ts</a></h3>
<ol>
<li><strong>Don't train in debug mode</strong> (10-100x slower)</li>
<li><strong>Don't use validation data for training</strong> (data leakage)</li>
<li><strong>Don't forget to zero gradients</strong> (leads to incorrect updates)</li>
<li><strong>Don't use tiny learning rates</strong> (&lt;1e-6) without a good reason</li>
<li><strong>Don't ignore validation loss</strong> (only watching training loss hides overfitting)</li>
</ol>
<h2 id="troubleshooting-2"><a class="header" href="#troubleshooting-2">Troubleshooting</a></h2>
<h3 id="loss-is-nan"><a class="header" href="#loss-is-nan">Loss is NaN</a></h3>
<p><strong>Causes</strong>:</p>
<ul>
<li>Learning rate too high</li>
<li>Numerical instability in loss function</li>
</ul>
<p><strong>Solutions</strong>:</p>
<ul>
<li>Reduce learning rate (try 0.001, 0.0001)</li>
<li>Add gradient clipping: <code>clip_grad_norm(&amp;mut params, 1.0)</code></li>
<li>Clamp predictions: <code>p.clamp(1e-7, 1.0 - 1e-7)</code></li>
</ul>
<h3 id="training-is-slow"><a class="header" href="#training-is-slow">Training is Slow</a></h3>
<p><strong>Causes</strong>:</p>
<ul>
<li>Running in debug mode</li>
<li>Batch size too small</li>
<li>SIMD not activating</li>
</ul>
<p><strong>Solutions</strong>:</p>
<ul>
<li>Use <code>cargo run --release</code></li>
<li>Increase batch size (32, 64, 128)</li>
<li>Ensure tensors are ≥16 elements for SIMD</li>
</ul>
<h3 id="validation-loss-increases"><a class="header" href="#validation-loss-increases">Validation Loss Increases</a></h3>
<p><strong>Cause</strong>: Overfitting</p>
<p><strong>Solutions</strong>:</p>
<ul>
<li>Enable early stopping</li>
<li>Reduce model size (fewer parameters)</li>
<li>Add regularization (L2 weight decay)</li>
<li>Increase dataset size</li>
</ul>
<h2 id="whats-next-2"><a class="header" href="#whats-next-2">What's Next?</a></h2>
<ul>
<li><strong><a href="getting-started/./core-concepts.html">Core Concepts</a></strong> - Understand Entrenar's architecture</li>
<li><strong><a href="getting-started/../autograd/what-is-autograd.html">Autograd Engine</a></strong> - Learn how automatic differentiation works</li>
<li><strong><a href="getting-started/../optimizers/overview.html">Optimizers</a></strong> - Explore SGD, Adam, AdamW, and schedulers</li>
</ul>
<hr />
<p><strong>Ready to dive deeper?</strong> Continue to <a href="getting-started/./core-concepts.html">Core Concepts</a> →</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="core-concepts"><a class="header" href="#core-concepts">Core Concepts</a></h1>
<p>This chapter explains the fundamental concepts behind Entrenar's design and how they work together to provide a complete neural network training system.</p>
<h2 id="architecture-overview"><a class="header" href="#architecture-overview">Architecture Overview</a></h2>
<p>Entrenar is built on four core pillars:</p>
<pre><code>┌─────────────────────────────────────────────────────────┐
│                    Training Loop                        │
│  (User Code: forward pass, loss, backward, optimize)    │
└─────────────────────────────────────────────────────────┘
                           │
        ┌──────────────────┼──────────────────┐
        ▼                  ▼                  ▼
┌───────────────┐  ┌───────────────┐  ┌───────────────┐
│   Autograd    │  │  Optimizers   │  │   LoRA/QLoRA  │
│   Engine      │  │  (SGD, Adam,  │  │  (Parameter-  │
│   (Gradient   │  │   AdamW, LR   │  │   Efficient   │
│   Computation)│  │   Schedulers) │  │   Fine-Tuning)│
└───────────────┘  └───────────────┘  └───────────────┘
        │                  │                  │
        └──────────────────┼──────────────────┘
                           ▼
                   ┌───────────────┐
                   │     Tensor    │
                   │ (Data + Grad) │
                   └───────────────┘
</code></pre>
<h2 id="1-tensors"><a class="header" href="#1-tensors">1. Tensors</a></h2>
<p><strong>Tensors</strong> are the fundamental data structure in Entrenar, representing multi-dimensional arrays with optional gradient tracking.</p>
<h3 id="tensor-creation"><a class="header" href="#tensor-creation">Tensor Creation</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use entrenar::Tensor;

// Scalar (0D)
let scalar = Tensor::from_vec(vec![3.14], false);

// Vector (1D)
let vector = Tensor::from_vec(vec![1.0, 2.0, 3.0], true);

// Matrix (2D) - flattened representation
let matrix = Tensor::from_vec(
    vec![1.0, 2.0,
         3.0, 4.0],  // 2x2 matrix
    true
);

// Random initialization
let weights = Tensor::randn(vec![256], true);  // Normal(0, 1)

// Zero initialization
let bias = Tensor::zeros(vec![128], true);
<span class="boring">}</span></code></pre></pre>
<h3 id="gradient-tracking"><a class="header" href="#gradient-tracking">Gradient Tracking</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Trainable parameter
let w = Tensor::from_vec(vec![1.0, 2.0], true);  // requires_grad=true
assert!(w.requires_grad());

// Frozen parameter (e.g., pretrained base weights)
let frozen = Tensor::from_vec(vec![1.0, 2.0], false);  // requires_grad=false
assert!(!frozen.requires_grad());
<span class="boring">}</span></code></pre></pre>
<h3 id="tensor-operations"><a class="header" href="#tensor-operations">Tensor Operations</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Arithmetic operations
let a = Tensor::from_vec(vec![1.0, 2.0], true);
let b = Tensor::from_vec(vec![3.0, 4.0], true);

let c = &amp;a + &amp;b;  // Element-wise addition
let d = &amp;a * &amp;b;  // Element-wise multiplication
let e = &amp;a - &amp;b;  // Element-wise subtraction

// Matrix operations
use entrenar::autograd::ops::matmul;

let result = matmul(&amp;a, &amp;b, rows, cols, batch_size);
<span class="boring">}</span></code></pre></pre>
<p><strong>Key Insight</strong>: Tensor operations use references (<code>&amp;</code>) to avoid consuming the original tensors, allowing reuse in computational graphs.</p>
<h2 id="2-automatic-differentiation-autograd"><a class="header" href="#2-automatic-differentiation-autograd">2. Automatic Differentiation (Autograd)</a></h2>
<p><strong>Autograd</strong> computes gradients automatically using reverse-mode differentiation (backpropagation).</p>
<h3 id="computational-graph"><a class="header" href="#computational-graph">Computational Graph</a></h3>
<p>Entrenar uses a <strong>tape-based</strong> computational graph:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let x = Tensor::from_vec(vec![2.0], true);
let y = &amp;x * &amp;x;           // y = x²  (tape records: mul operation)
let z = &amp;y + &amp;x;           // z = x² + x  (tape records: add operation)

backward(&amp;z);              // Compute dz/dx

println!("dz/dx = {}", x.grad()[0]);  // dz/dx = 2x + 1 = 5.0
<span class="boring">}</span></code></pre></pre>
<p><strong>Tape Structure</strong>:</p>
<pre><code>Tape:
  1. Op: Mul(x, x) -&gt; y
  2. Op: Add(y, x) -&gt; z

Backward pass (reverse order):
  1. dz/dz = 1.0
  2. dz/dy = 1.0, dz/dx += 1.0
  3. dy/dx = 2x, dz/dx += 2x * dz/dy = 4.0
  Result: dz/dx = 5.0
</code></pre>
<h3 id="supported-operations"><a class="header" href="#supported-operations">Supported Operations</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Operation</th><th>Forward</th><th>Backward</th></tr></thead><tbody>
<tr><td><strong>Matrix Mul</strong></td><td><code>C = A @ B</code></td><td><code>dA = dC @ B^T</code>, <code>dB = A^T @ dC</code></td></tr>
<tr><td><strong>ReLU</strong></td><td><code>max(0, x)</code></td><td><code>dx = (x &gt; 0) ? dy : 0</code></td></tr>
<tr><td><strong>GELU</strong></td><td><code>x * Φ(x)</code></td><td>Chain rule with Gaussian CDF</td></tr>
<tr><td><strong>Layer Norm</strong></td><td><code>(x - μ) / σ</code></td><td>Mean/variance gradients</td></tr>
<tr><td><strong>Attention</strong></td><td><code>softmax(QK^T/√d)V</code></td><td>Q, K, V chain rule</td></tr>
</tbody></table>
</div>
<h3 id="gradient-checking-1"><a class="header" href="#gradient-checking-1">Gradient Checking</a></h3>
<p>Entrenar validates all gradients with finite differences:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[test]
fn test_gradient_correctness() {
    let x = Tensor::from_vec(vec![1.0, 2.0], true);
    let y = &amp;x * &amp;x;

    backward(&amp;y);

    // Finite difference: f(x+ε) - f(x-ε) / 2ε
    let epsilon = 1e-3;
    let threshold = 0.2;  // 20% relative error tolerance

    check_gradient(&amp;y, &amp;x, epsilon, threshold);  // ✅ Passes
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Zero-tolerance policy</strong>: Every operation has gradient checking tests ensuring mathematical correctness.</p>
<h2 id="3-optimizers"><a class="header" href="#3-optimizers">3. Optimizers</a></h2>
<p><strong>Optimizers</strong> update parameters using computed gradients.</p>
<h3 id="optimizer-interface"><a class="header" href="#optimizer-interface">Optimizer Interface</a></h3>
<p>All optimizers share a common interface:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use entrenar::optim::{SGD, Adam, AdamW};

let mut optimizer = Adam::default_params(learning_rate=0.001);

// Training step
backward(&amp;loss);
optimizer.step(&amp;mut [&amp;mut w1, &amp;mut b1, &amp;mut w2, &amp;mut b2]);

// Zero gradients for next iteration
w1.zero_grad();
b1.zero_grad();
// ... etc
<span class="boring">}</span></code></pre></pre>
<h3 id="sgd-stochastic-gradient-descent"><a class="header" href="#sgd-stochastic-gradient-descent">SGD (Stochastic Gradient Descent)</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use entrenar::optim::SGD;

let mut sgd = SGD::new(
    learning_rate=0.01,
    momentum=0.9,           // Accelerates convergence
);

// Update rule: v = momentum * v + grad
//              param = param - learning_rate * v
sgd.step(&amp;mut params);
<span class="boring">}</span></code></pre></pre>
<p><strong>Use case</strong>: Simple optimization, baseline comparisons</p>
<h3 id="adam-adaptive-moment-estimation"><a class="header" href="#adam-adaptive-moment-estimation">Adam (Adaptive Moment Estimation)</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use entrenar::optim::Adam;

let mut adam = Adam::default_params(learning_rate=0.001);

// Adaptive learning rates per parameter
// m = β1*m + (1-β1)*grad           (1st moment)
// v = β2*v + (1-β2)*grad²          (2nd moment)
// param = param - lr * m̂ / (√v̂ + ε)
adam.step(&amp;mut params);
<span class="boring">}</span></code></pre></pre>
<p><strong>Use case</strong>: General-purpose, works well out-of-the-box</p>
<h3 id="adamw-adam-with-decoupled-weight-decay"><a class="header" href="#adamw-adam-with-decoupled-weight-decay">AdamW (Adam with Decoupled Weight Decay)</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use entrenar::optim::AdamW;

let mut adamw = AdamW::new(
    learning_rate=0.001,
    weight_decay=0.01,      // L2 regularization
    beta1=0.9,
    beta2=0.999,
    epsilon=1e-8,
);

// Decoupled weight decay: param = param * (1 - wd)
adamw.step(&amp;mut params);
<span class="boring">}</span></code></pre></pre>
<p><strong>Use case</strong>: Fine-tuning transformers, improved generalization</p>
<h3 id="learning-rate-schedulers"><a class="header" href="#learning-rate-schedulers">Learning Rate Schedulers</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use entrenar::optim::schedulers::CosineScheduler;

let scheduler = CosineScheduler::new(
    initial_lr=0.1,
    min_lr=0.001,
    total_steps=1000,
);

for step in 0..1000 {
    let lr = scheduler.get_lr(step);  // Cosine annealing
    optimizer.set_lr(lr);

    // ... training step ...
}
<span class="boring">}</span></code></pre></pre>
<h2 id="4-lora-low-rank-adaptation"><a class="header" href="#4-lora-low-rank-adaptation">4. LoRA (Low-Rank Adaptation)</a></h2>
<p><strong>LoRA</strong> enables parameter-efficient fine-tuning by freezing base weights and training low-rank adapters.</p>
<h3 id="architecture"><a class="header" href="#architecture">Architecture</a></h3>
<pre><code>Original Layer: W ∈ ℝ^(d_out × d_in)

LoRA Layer:
  Base: W ∈ ℝ^(d_out × d_in)     [FROZEN, requires_grad=false]
  Adapters:
    A ∈ ℝ^(rank × d_in)          [TRAINABLE, requires_grad=true]
    B ∈ ℝ^(d_out × rank)         [TRAINABLE, requires_grad=true]

Output: y = Wx + (α/r)(B(Ax))
</code></pre>
<h3 id="usage"><a class="header" href="#usage">Usage</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use entrenar::lora::LoRALayer;

// Pretrained base weights (frozen)
let base_weight = Tensor::from_vec(vec![...], false);

// Create LoRA layer
let lora = LoRALayer::new(
    base_weight,
    d_out=256,
    d_in=256,
    rank=16,       // Low-rank bottleneck
    alpha=32.0,    // Scaling factor
);

// Forward pass
let output = lora.forward(&amp;input);

// Only LoRA adapters receive gradients
backward(&amp;loss);  // base_weight.grad() remains zero
<span class="boring">}</span></code></pre></pre>
<h3 id="parameter-efficiency"><a class="header" href="#parameter-efficiency">Parameter Efficiency</a></h3>
<pre><code>Full Fine-Tuning: 7B parameters trainable
LoRA (rank=64):   8M parameters trainable (0.1%)

Memory savings: 99.9% reduction in trainable parameters
</code></pre>
<h3 id="adapter-persistence"><a class="header" href="#adapter-persistence">Adapter Persistence</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use entrenar::lora::adapter::{save_adapter, load_adapter};

// Save LoRA adapters (32MB file)
save_adapter(&amp;lora, rank=16, alpha=32.0, "adapter.json")?;

// Load adapters (without full model weights)
let loaded_lora = load_adapter("adapter.json", base_weight)?;
<span class="boring">}</span></code></pre></pre>
<p><strong>Use case</strong>: Share fine-tuned adapters without distributing 28GB base model weights</p>
<h2 id="5-qlora-quantized-lora"><a class="header" href="#5-qlora-quantized-lora">5. QLoRA (Quantized LoRA)</a></h2>
<p><strong>QLoRA</strong> reduces memory by 75% through 4-bit quantization of frozen base weights.</p>
<h3 id="4-bit-quantization"><a class="header" href="#4-bit-quantization">4-Bit Quantization</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use entrenar::lora::QLoRALayer;

// Base weights quantized to 4-bit (75% memory reduction)
let qlora = QLoRALayer::new(
    base_weight,
    d_out=4096,
    d_in=4096,
    rank=64,
    alpha=128.0,
);

// On-the-fly dequantization during forward pass
let output = qlora.forward(&amp;input);
<span class="boring">}</span></code></pre></pre>
<h3 id="memory-comparison"><a class="header" href="#memory-comparison">Memory Comparison</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Configuration</th><th>LoRA Memory</th><th>QLoRA Memory</th><th>Savings</th></tr></thead><tbody>
<tr><td><strong>Small (256-dim, 6 layers)</strong></td><td>1.5 MB</td><td>0.5 MB</td><td>65%</td></tr>
<tr><td><strong>Medium (768-dim, 12 layers)</strong></td><td>27 MB</td><td>8 MB</td><td>68%</td></tr>
<tr><td><strong>Large (4096-dim, 32 layers)</strong></td><td>4.2 GB</td><td>1.2 GB</td><td>70%</td></tr>
</tbody></table>
</div>
<h3 id="quantization-details"><a class="header" href="#quantization-details">Quantization Details</a></h3>
<pre><code>Block-wise quantization (64 elements per block):
  1. Compute scale factor: s = max(|values|) / 7
  2. Quantize: q = round(value / s)  ∈ [-7, 7]
  3. Store: 4-bit signed integers (15 discrete levels)

Dequantization:
  value = q * s  (full precision restored)
</code></pre>
<p><strong>Trade-off</strong>: Minimal accuracy loss (&lt;1%) for 75% memory reduction</p>
<h2 id="6-extreme-tdd-quality"><a class="header" href="#6-extreme-tdd-quality">6. EXTREME TDD Quality</a></h2>
<p>Entrenar is built with <strong>zero-tolerance for defects</strong> using multiple testing strategies:</p>
<h3 id="unit-tests"><a class="header" href="#unit-tests">Unit Tests</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[test]
fn test_matmul_correctness() {
    let a = Tensor::from_vec(vec![1.0, 2.0, 3.0, 4.0], false);
    let b = Tensor::from_vec(vec![5.0, 6.0, 7.0, 8.0], false);

    let c = matmul(&amp;a, &amp;b, 2, 2, 1);

    assert_eq!(c.data()[0], 19.0);  // 1*5 + 2*7
    assert_eq!(c.data()[1], 43.0);  // 3*5 + 4*7
}
<span class="boring">}</span></code></pre></pre>
<h3 id="property-based-tests"><a class="header" href="#property-based-tests">Property-Based Tests</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use proptest::prelude::*;

proptest! {
    #[test]
    fn test_adam_converges(lr in 0.05f32..0.5) {
        let optimizer = Adam::default_params(lr);
        assert!(converges_to_minimum(optimizer, 100));
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="gradient-checking-2"><a class="header" href="#gradient-checking-2">Gradient Checking</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[test]
fn test_relu_gradient() {
    let x = Tensor::from_vec(vec![-1.0, 0.0, 1.0], true);
    let y = relu(&amp;x);

    backward(&amp;y);

    // Finite difference validation (ε=1e-3, threshold=0.2)
    check_gradient(&amp;y, &amp;x, 1e-3, 0.2);
}
<span class="boring">}</span></code></pre></pre>
<h3 id="mutation-testing"><a class="header" href="#mutation-testing">Mutation Testing</a></h3>
<pre><code class="language-bash">cargo mutants --file src/autograd/ops.rs

# Ensures tests catch intentional bugs
# Target: &gt;80% mutation kill rate
</code></pre>
<h2 id="putting-it-all-together"><a class="header" href="#putting-it-all-together">Putting It All Together</a></h2>
<h3 id="complete-training-workflow"><a class="header" href="#complete-training-workflow">Complete Training Workflow</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use entrenar::{Tensor, backward, optim::AdamW, lora::QLoRALayer};

// 1. Load pretrained base weights
let base_weight = load_pretrained_weights("llama-7b.bin");

// 2. Create QLoRA layer (75% memory reduction)
let qlora = QLoRALayer::new(base_weight, 4096, 4096, rank=64, alpha=128.0);

// 3. Initialize optimizer
let mut optimizer = AdamW::new(lr=0.0001, weight_decay=0.01, ...);

// 4. Training loop
for (input, target) in dataloader {
    // Forward pass
    let output = qlora.forward(&amp;input);
    let loss = cross_entropy_loss(&amp;output, &amp;target);

    // Backward pass (only LoRA adapters get gradients)
    backward(&amp;loss);

    // Update (only 8M parameters instead of 7B)
    optimizer.step(&amp;mut qlora.trainable_parameters());

    // Zero gradients
    qlora.zero_grad();
}

// 5. Save adapters (32MB file)
save_adapter(&amp;qlora, "custom_adapter.json")?;
<span class="boring">}</span></code></pre></pre>
<p><strong>Result</strong>: Fine-tune 7B parameter model on consumer GPU with 8GB VRAM</p>
<h2 id="key-takeaways"><a class="header" href="#key-takeaways">Key Takeaways</a></h2>
<ol>
<li><strong>Tensors</strong> store data and gradients, enabling automatic differentiation</li>
<li><strong>Autograd</strong> computes gradients via reverse-mode differentiation on a tape-based graph</li>
<li><strong>Optimizers</strong> update parameters using various strategies (SGD, Adam, AdamW)</li>
<li><strong>LoRA</strong> trains low-rank adapters instead of full weights (99.9% parameter reduction)</li>
<li><strong>QLoRA</strong> quantizes base weights to 4-bit for 75% memory savings</li>
<li><strong>EXTREME TDD</strong> ensures zero defects through comprehensive testing</li>
</ol>
<h2 id="whats-next-3"><a class="header" href="#whats-next-3">What's Next?</a></h2>
<ul>
<li><strong><a href="getting-started/../autograd/what-is-autograd.html">Autograd Engine</a></strong> - Deep dive into automatic differentiation</li>
<li><strong><a href="getting-started/../optimizers/overview.html">Optimizers</a></strong> - Explore optimizer algorithms and theory</li>
<li><strong><a href="getting-started/../lora/what-is-lora.html">LoRA/QLoRA</a></strong> - Master parameter-efficient fine-tuning</li>
<li><strong><a href="getting-started/../examples/linear-regression.html">Examples</a></strong> - See practical applications</li>
</ul>
<hr />
<p><strong>Ready to explore the autograd engine?</strong> Continue to <a href="getting-started/../autograd/what-is-autograd.html">What is Automatic Differentiation?</a> →</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="overview"><a class="header" href="#overview">Overview</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="design-philosophy"><a class="header" href="#design-philosophy">Design Philosophy</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="module-organization"><a class="header" href="#module-organization">Module Organization</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="type-system"><a class="header" href="#type-system">Type System</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="memory-management"><a class="header" href="#memory-management">Memory Management</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="what-is-automatic-differentiation"><a class="header" href="#what-is-automatic-differentiation">What is Automatic Differentiation?</a></h1>
<p><strong>Automatic Differentiation (Autograd)</strong> is a technique for computing derivatives of functions specified by computer programs. It's the foundation of modern deep learning, enabling neural networks to learn through gradient-based optimization.</p>
<h2 id="the-problem-manual-derivatives"><a class="header" href="#the-problem-manual-derivatives">The Problem: Manual Derivatives</a></h2>
<p>Consider a simple neural network layer:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn forward(x: f32, w: f32, b: f32) -&gt; f32 {
    w * x + b  // Linear transformation
}
<span class="boring">}</span></code></pre></pre>
<p>To train this layer, we need gradients: <code>∂loss/∂w</code> and <code>∂loss/∂b</code>.</p>
<h3 id="manual-approach-error-prone"><a class="header" href="#manual-approach-error-prone">Manual Approach (Error-Prone)</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Forward pass
let y_pred = w * x + b;
let loss = (y_pred - y_true).powi(2);  // MSE

// Backward pass (hand-coded derivatives)
let d_loss = 2.0 * (y_pred - y_true);
let d_w = d_loss * x;  // ∂loss/∂w = ∂loss/∂y * ∂y/∂w
let d_b = d_loss * 1.0;  // ∂loss/∂b = ∂loss/∂y * ∂y/∂b

// Update
w -= learning_rate * d_w;
b -= learning_rate * d_b;
<span class="boring">}</span></code></pre></pre>
<p><strong>Problems with manual derivatives:</strong></p>
<ul>
<li>❌ Error-prone (easy to make mistakes in chain rule)</li>
<li>❌ Doesn't scale (complex models have thousands of operations)</li>
<li>❌ Hard to maintain (changing forward pass requires rewriting backward pass)</li>
<li>❌ No validation (how do you know your derivatives are correct?)</li>
</ul>
<h2 id="the-solution-automatic-differentiation"><a class="header" href="#the-solution-automatic-differentiation">The Solution: Automatic Differentiation</a></h2>
<p>Entrenar's autograd engine <strong>automatically computes correct derivatives</strong> for any computation:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use entrenar::{Tensor, backward};

// Forward pass (same as before)
let x = Tensor::from_vec(vec![2.0], false);
let w = Tensor::from_vec(vec![3.0], true);  // requires_grad=true
let b = Tensor::from_vec(vec![1.0], true);

let y_pred = &amp;(&amp;w * &amp;x) + &amp;b;  // y = w*x + b = 7.0
let y_true = Tensor::from_vec(vec![10.0], false);

let diff = &amp;y_pred - &amp;y_true;
let loss = &amp;diff * &amp;diff;  // loss = 9.0

// Backward pass (automatic!)
backward(&amp;loss);

// Gradients computed automatically
println!("∂loss/∂w = {}", w.grad()[0]);  // -12.0 ✅ Correct!
println!("∂loss/∂b = {}", b.grad()[0]);  // -6.0 ✅ Correct!
<span class="boring">}</span></code></pre></pre>
<p><strong>Benefits of autograd:</strong></p>
<ul>
<li>✅ Correct by construction (no manual derivative errors)</li>
<li>✅ Scales to any complexity (transformers, ResNets, etc.)</li>
<li>✅ Easy to maintain (change forward pass, backward automatically updates)</li>
<li>✅ Validated with gradient checking (10K+ test cases)</li>
</ul>
<h2 id="how-autograd-works"><a class="header" href="#how-autograd-works">How Autograd Works</a></h2>
<p>Entrenar uses <strong>reverse-mode automatic differentiation</strong> (also called backpropagation).</p>
<h3 id="three-modes-of-differentiation"><a class="header" href="#three-modes-of-differentiation">Three Modes of Differentiation</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Mode</th><th>Description</th><th>Complexity</th><th>Use Case</th></tr></thead><tbody>
<tr><td><strong>Numerical</strong></td><td>Finite differences: <code>f'(x) ≈ (f(x+ε) - f(x)) / ε</code></td><td>O(n) evaluations</td><td>Gradient checking</td></tr>
<tr><td><strong>Symbolic</strong></td><td>Algebraic manipulation: <code>d/dx(x²) = 2x</code></td><td>Exponential growth</td><td>Computer algebra systems</td></tr>
<tr><td><strong>Automatic</strong></td><td>Chain rule on computation graph</td><td>O(1) per operation</td><td>Deep learning</td></tr>
</tbody></table>
</div>
<h3 id="reverse-mode-differentiation"><a class="header" href="#reverse-mode-differentiation">Reverse-Mode Differentiation</a></h3>
<p>Given a computation <code>y = f(g(h(x)))</code>, we want <code>dy/dx</code>.</p>
<p><strong>Forward Pass</strong> (compute outputs):</p>
<pre><code>x → h(x) → g(h(x)) → f(g(h(x))) = y
</code></pre>
<p><strong>Backward Pass</strong> (compute gradients via chain rule):</p>
<pre><code>dy/dx ← dy/dg * dg/dh ← dy/dg ← dy/dy = 1.0
</code></pre>
<p><strong>Key insight</strong>: We only need to store intermediate values and apply the chain rule in reverse.</p>
<h3 id="example-y--x²"><a class="header" href="#example-y--x²">Example: y = x²</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let x = Tensor::from_vec(vec![3.0], true);
let y = &amp;x * &amp;x;  // y = x²

backward(&amp;y);  // Compute dy/dx

println!("dy/dx = {}", x.grad()[0]);  // 6.0 (= 2*x)
<span class="boring">}</span></code></pre></pre>
<p><strong>What happened:</strong></p>
<ol>
<li>
<p><strong>Forward pass</strong>:</p>
<ul>
<li>Compute <code>y = x * x = 9.0</code></li>
<li>Record operation: <code>Mul(x, x) -&gt; y</code></li>
</ul>
</li>
<li>
<p><strong>Backward pass</strong> (starting from <code>dy/dy = 1.0</code>):</p>
<ul>
<li><code>dy/dx_left = dy/dy * x_right = 1.0 * 3.0 = 3.0</code></li>
<li><code>dy/dx_right = dy/dy * x_left = 1.0 * 3.0 = 3.0</code></li>
<li><code>dy/dx = dy/dx_left + dy/dx_right = 6.0</code> (gradient accumulation)</li>
</ul>
</li>
</ol>
<h2 id="computational-graph-1"><a class="header" href="#computational-graph-1">Computational Graph</a></h2>
<p>Autograd builds a <strong>computational graph</strong> representing the sequence of operations:</p>
<pre><code>Example: z = (x + y) * (x - y)

Graph:
       x      y
       │      │
       ├──────┤
       │      │
       ▼      ▼
      Add    Sub
       │      │
       └──────┘
          │
          ▼
         Mul
          │
          ▼
          z
</code></pre>
<h3 id="tape-based-implementation"><a class="header" href="#tape-based-implementation">Tape-Based Implementation</a></h3>
<p>Entrenar uses a <strong>tape</strong> to record operations during the forward pass:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Forward pass (records operations on tape)
let x = Tensor::from_vec(vec![2.0], true);
let y = Tensor::from_vec(vec![3.0], true);

let a = &amp;x + &amp;y;  // Tape: [Add(x, y) -&gt; a]
let b = &amp;x - &amp;y;  // Tape: [Add(x, y) -&gt; a, Sub(x, y) -&gt; b]
let z = &amp;a * &amp;b;  // Tape: [Add(x, y) -&gt; a, Sub(x, y) -&gt; b, Mul(a, b) -&gt; z]

// Backward pass (replay tape in reverse)
backward(&amp;z);  // Process: Mul -&gt; Sub -&gt; Add
<span class="boring">}</span></code></pre></pre>
<p><strong>Tape structure:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>Tape:
  [0] Add { lhs: x_id, rhs: y_id, out: a_id }
  [1] Sub { lhs: x_id, rhs: y_id, out: b_id }
  [2] Mul { lhs: a_id, rhs: b_id, out: z_id }

Backward (reverse order):
  [2] Mul.backward(): da = b*dz, db = a*dz
  [1] Sub.backward(): dx += 1*db, dy += -1*db
  [0] Add.backward(): dx += 1*da, dy += 1*da
<span class="boring">}</span></code></pre></pre>
<h2 id="supported-operations-1"><a class="header" href="#supported-operations-1">Supported Operations</a></h2>
<p>Entrenar provides backward passes for all essential neural network operations:</p>
<h3 id="basic-operations"><a class="header" href="#basic-operations">Basic Operations</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Operation</th><th>Forward</th><th>Backward</th></tr></thead><tbody>
<tr><td><strong>Add</strong></td><td><code>z = x + y</code></td><td><code>dx = dz</code>, <code>dy = dz</code></td></tr>
<tr><td><strong>Sub</strong></td><td><code>z = x - y</code></td><td><code>dx = dz</code>, <code>dy = -dz</code></td></tr>
<tr><td><strong>Mul</strong></td><td><code>z = x * y</code></td><td><code>dx = y*dz</code>, <code>dy = x*dz</code></td></tr>
<tr><td><strong>Div</strong></td><td><code>z = x / y</code></td><td><code>dx = dz/y</code>, <code>dy = -x*dz/y²</code></td></tr>
</tbody></table>
</div>
<h3 id="matrix-operations"><a class="header" href="#matrix-operations">Matrix Operations</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Operation</th><th>Forward</th><th>Backward</th></tr></thead><tbody>
<tr><td><strong>MatMul</strong></td><td><code>C = A @ B</code></td><td><code>dA = dC @ B^T</code>, <code>dB = A^T @ dC</code></td></tr>
</tbody></table>
</div>
<h3 id="activations"><a class="header" href="#activations">Activations</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Operation</th><th>Forward</th><th>Backward</th></tr></thead><tbody>
<tr><td><strong>ReLU</strong></td><td><code>max(0, x)</code></td><td><code>dx = (x &gt; 0) ? dy : 0</code></td></tr>
<tr><td><strong>GELU</strong></td><td><code>x * Φ(x)</code></td><td>Chain rule with Gaussian CDF derivative</td></tr>
<tr><td><strong>Swish</strong></td><td><code>x * sigmoid(x)</code></td><td><code>dx = (swish(x) + sigmoid(x) * (1 - swish(x))) * dy</code></td></tr>
</tbody></table>
</div>
<h3 id="normalization"><a class="header" href="#normalization">Normalization</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Operation</th><th>Forward</th><th>Backward</th></tr></thead><tbody>
<tr><td><strong>LayerNorm</strong></td><td><code>(x - μ) / σ</code></td><td>Mean/variance chain rule</td></tr>
</tbody></table>
</div>
<h3 id="attention"><a class="header" href="#attention">Attention</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Operation</th><th>Forward</th><th>Backward</th></tr></thead><tbody>
<tr><td><strong>Attention</strong></td><td><code>softmax(QK^T/√d)V</code></td><td>Q, K, V gradients via chain rule</td></tr>
</tbody></table>
</div>
<h2 id="gradient-validation"><a class="header" href="#gradient-validation">Gradient Validation</a></h2>
<p>Entrenar validates <strong>every backward pass</strong> with finite difference checking:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[test]
fn test_matmul_backward_gradient_check() {
    let a = Tensor::from_vec(vec![1.0, 2.0, 3.0, 4.0], true);
    let b = Tensor::from_vec(vec![5.0, 6.0, 7.0, 8.0], true);

    let c = matmul(&amp;a, &amp;b, 2, 2, 1);
    backward(&amp;c);

    // Finite difference: f'(x) ≈ (f(x+ε) - f(x-ε)) / 2ε
    let epsilon = 1e-3;
    let threshold = 0.2;  // 20% relative error

    check_gradient(&amp;c, &amp;a, epsilon, threshold);  // ✅ Passes
    check_gradient(&amp;c, &amp;b, epsilon, threshold);  // ✅ Passes
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Zero-tolerance policy:</strong></p>
<ul>
<li>10K+ gradient checking test cases</li>
<li>All operations tested with property-based tests</li>
<li>Mathematical correctness guaranteed</li>
</ul>
<h2 id="autograd-vs-manual-derivatives"><a class="header" href="#autograd-vs-manual-derivatives">Autograd vs Manual Derivatives</a></h2>
<div class="table-wrapper"><table><thead><tr><th>Aspect</th><th>Manual</th><th>Autograd</th></tr></thead><tbody>
<tr><td><strong>Correctness</strong></td><td>Error-prone</td><td>Validated with tests</td></tr>
<tr><td><strong>Scalability</strong></td><td>Doesn't scale</td><td>Handles any model size</td></tr>
<tr><td><strong>Maintainability</strong></td><td>Brittle</td><td>Change forward, backward auto-updates</td></tr>
<tr><td><strong>Development Time</strong></td><td>Hours/days</td><td>Seconds</td></tr>
<tr><td><strong>Performance</strong></td><td>Potentially optimal</td><td>Near-optimal (tape overhead minimal)</td></tr>
</tbody></table>
</div>
<h2 id="common-pitfalls"><a class="header" href="#common-pitfalls">Common Pitfalls</a></h2>
<h3 id="1-forgetting-requires_gradtrue"><a class="header" href="#1-forgetting-requires_gradtrue">1. Forgetting <code>requires_grad=true</code></a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let w = Tensor::from_vec(vec![1.0], false);  // ❌ No gradients
let y = &amp;w * &amp;x;
backward(&amp;y);
println!("{}", w.grad()[0]);  // 0.0 (gradient not computed)

// Fix:
let w = Tensor::from_vec(vec![1.0], true);  // ✅ Gradients enabled
<span class="boring">}</span></code></pre></pre>
<h3 id="2-not-zeroing-gradients"><a class="header" href="#2-not-zeroing-gradients">2. Not Zeroing Gradients</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>for epoch in 0..10 {
    let loss = compute_loss(&amp;model, &amp;data);
    backward(&amp;loss);

    optimizer.step(&amp;mut params);
    // ❌ Gradients accumulate across epochs!

    // Fix:
    model.zero_grad();  // ✅ Clear gradients
}
<span class="boring">}</span></code></pre></pre>
<h3 id="3-in-place-operations"><a class="header" href="#3-in-place-operations">3. In-Place Operations</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let mut x = Tensor::from_vec(vec![1.0, 2.0], true);
x.data_mut()[0] = 5.0;  // ❌ In-place modification breaks graph

// Fix: Create new tensor
let x_new = Tensor::from_vec(vec![5.0, 2.0], true);  // ✅
<span class="boring">}</span></code></pre></pre>
<h2 id="whats-next-4"><a class="header" href="#whats-next-4">What's Next?</a></h2>
<ul>
<li><strong><a href="autograd/./tape-based-graphs.html">Tape-Based Computation Graphs</a></strong> - Deep dive into Entrenar's tape implementation</li>
<li><strong><a href="autograd/./tensor-operations.html">Tensor Operations</a></strong> - Explore all supported operations</li>
<li><strong><a href="autograd/./backward-pass.html">Backward Pass</a></strong> - Understand gradient computation mechanics</li>
<li><strong><a href="autograd/./finite-difference.html">Finite Difference Validation</a></strong> - Learn gradient checking methodology</li>
</ul>
<h2 id="key-takeaways-1"><a class="header" href="#key-takeaways-1">Key Takeaways</a></h2>
<ol>
<li><strong>Autograd automates derivative computation</strong> - no manual chain rule</li>
<li><strong>Reverse-mode differentiation</strong> - efficient for deep learning (many inputs, one output)</li>
<li><strong>Tape-based graph</strong> - records operations during forward pass</li>
<li><strong>Validated with tests</strong> - 10K+ gradient checking cases ensure correctness</li>
<li><strong>Zero-tolerance for bugs</strong> - extreme TDD methodology</li>
</ol>
<hr />
<p><strong>Ready to understand the tape?</strong> Continue to <a href="autograd/./tape-based-graphs.html">Tape-Based Computation Graphs</a> →</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="tape-based-computation-graphs"><a class="header" href="#tape-based-computation-graphs">Tape-Based Computation Graphs</a></h1>
<p>Entrenar uses a <strong>tape-based</strong> approach to record computational graphs during the forward pass and replay them in reverse during backpropagation. This chapter explains how the tape works and why it's efficient.</p>
<h2 id="the-tape-metaphor"><a class="header" href="#the-tape-metaphor">The Tape Metaphor</a></h2>
<p>Think of the tape like a cassette recorder:</p>
<ul>
<li><strong>Forward pass</strong>: Record each operation onto the tape</li>
<li><strong>Backward pass</strong>: Rewind and play back in reverse</li>
<li><strong>Gradient computation</strong>: Each operation knows how to propagate gradients</li>
</ul>
<pre><code>Forward (Recording):
  x → [Op1] → a → [Op2] → b → [Op3] → output
  Tape: [Op1, Op2, Op3]

Backward (Playback):
  dx ← [Op1*] ← da ← [Op2*] ← db ← [Op3*] ← dout=1.0
  Process tape in reverse: Op3* → Op2* → Op1*
</code></pre>
<h2 id="tape-structure"><a class="header" href="#tape-structure">Tape Structure</a></h2>
<p>Entrenar's tape stores operation metadata, not full tensors:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>struct TapeEntry {
    operation: OpType,      // What operation (Add, Mul, MatMul, etc.)
    inputs: Vec&lt;TensorId&gt;,  // Input tensor IDs
    output: TensorId,       // Output tensor ID
    metadata: OpMetadata,   // Operation-specific data
}

enum OpType {
    Add,
    Mul,
    MatMul { rows, cols, batch },
    ReLU,
    LayerNorm,
    // ... etc
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Key insight</strong>: We don't store actual tensor data on the tape, only <strong>references</strong> (IDs) and operation metadata.</p>
<h2 id="example-recording-operations"><a class="header" href="#example-recording-operations">Example: Recording Operations</a></h2>
<p>Let's trace a simple computation:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use entrenar::{Tensor, backward};

let x = Tensor::from_vec(vec![2.0], true);  // ID: 0
let y = Tensor::from_vec(vec![3.0], true);  // ID: 1

let a = &amp;x + &amp;y;  // ID: 2, records Add(0, 1) -&gt; 2
let b = &amp;a * &amp;x;  // ID: 3, records Mul(2, 0) -&gt; 3

backward(&amp;b);
<span class="boring">}</span></code></pre></pre>
<p><strong>Tape after forward pass:</strong></p>
<pre><code>Tape = [
  Entry {
    operation: Add,
    inputs: [tensor_0_id, tensor_1_id],  // x, y
    output: tensor_2_id,                  // a
    metadata: {},
  },
  Entry {
    operation: Mul,
    inputs: [tensor_2_id, tensor_0_id],  // a, x
    output: tensor_3_id,                  // b
    metadata: {},
  },
]
</code></pre>
<h2 id="backward-pass-replaying-the-tape"><a class="header" href="#backward-pass-replaying-the-tape">Backward Pass: Replaying the Tape</a></h2>
<p>During <code>backward(&amp;b)</code>, Entrenar processes the tape in <strong>reverse order</strong>:</p>
<h3 id="step-1-initialize-output-gradient"><a class="header" href="#step-1-initialize-output-gradient">Step 1: Initialize Output Gradient</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// db/db = 1.0 (seed gradient)
b.set_grad(vec![1.0]);
<span class="boring">}</span></code></pre></pre>
<h3 id="step-2-process-tape-entry-1-mul"><a class="header" href="#step-2-process-tape-entry-1-mul">Step 2: Process Tape Entry 1 (Mul)</a></h3>
<pre><code>Entry: Mul(a, x) -&gt; b
Current: db = 1.0

Backward rule for Mul:
  da = db * x = 1.0 * 2.0 = 2.0
  dx += db * a = 1.0 * 5.0 = 5.0  (accumulate)

Update gradients:
  a.grad = [2.0]
  x.grad = [5.0]
</code></pre>
<h3 id="step-3-process-tape-entry-0-add"><a class="header" href="#step-3-process-tape-entry-0-add">Step 3: Process Tape Entry 0 (Add)</a></h3>
<pre><code>Entry: Add(x, y) -&gt; a
Current: da = 2.0

Backward rule for Add:
  dx += da * 1 = 2.0
  dy = da * 1 = 2.0

Update gradients:
  x.grad = [5.0 + 2.0] = [7.0]  (accumulated!)
  y.grad = [2.0]
</code></pre>
<h3 id="final-gradients"><a class="header" href="#final-gradients">Final Gradients</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>println!("db/dx = {}", x.grad()[0]);  // 7.0 ✅
println!("db/dy = {}", y.grad()[0]);  // 2.0 ✅
<span class="boring">}</span></code></pre></pre>
<p><strong>Verification</strong> (manual chain rule):</p>
<pre><code>b = a * x = (x + y) * x = x² + xy
db/dx = 2x + y = 2(2) + 3 = 7 ✅
db/dy = x = 2 ✅
</code></pre>
<h2 id="gradient-accumulation"><a class="header" href="#gradient-accumulation">Gradient Accumulation</a></h2>
<p>Notice that <code>x</code> appears twice in the computation graph:</p>
<pre><code>    y
    │
    ▼
x ─┬─&gt; Add -&gt; a ─┐
   │              │
   └──────────────┴─&gt; Mul -&gt; b
</code></pre>
<p><strong>Gradients must accumulate</strong> when a tensor has multiple consumers:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// First use: x in Add
dx_from_add = da * 1 = 2.0

// Second use: x in Mul
dx_from_mul = db * a = 5.0

// Total gradient (sum of paths)
dx_total = dx_from_add + dx_from_mul = 7.0
<span class="boring">}</span></code></pre></pre>
<p>Entrenar handles this automatically via <code>+=</code> in gradient updates:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>x.grad_mut()[i] += gradient_contribution;  // Accumulation
<span class="boring">}</span></code></pre></pre>
<h2 id="operation-metadata"><a class="header" href="#operation-metadata">Operation Metadata</a></h2>
<p>Some operations need extra context for backward passes:</p>
<h3 id="matrix-multiplication"><a class="header" href="#matrix-multiplication">Matrix Multiplication</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>Entry {
    operation: MatMul,
    inputs: [a_id, b_id],
    output: c_id,
    metadata: MatMulMeta {
        rows: 128,
        cols: 64,
        batch: 32,
    },
}
<span class="boring">}</span></code></pre></pre>
<p>During backward:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Need dimensions to compute dA = dC @ B^T
let dA = matmul(dC, B_transpose, rows, cols, batch);
<span class="boring">}</span></code></pre></pre>
<h3 id="layer-normalization"><a class="header" href="#layer-normalization">Layer Normalization</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>Entry {
    operation: LayerNorm,
    inputs: [x_id],
    output: y_id,
    metadata: LayerNormMeta {
        mean: 0.5,      // Saved from forward pass
        variance: 0.25,
    },
}
<span class="boring">}</span></code></pre></pre>
<p>During backward:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Need mean/variance from forward pass to compute gradients
let dx = layernorm_backward(dy, x, saved_mean, saved_variance);
<span class="boring">}</span></code></pre></pre>
<h2 id="memory-efficiency"><a class="header" href="#memory-efficiency">Memory Efficiency</a></h2>
<p>Tape-based autograd is memory efficient because:</p>
<h3 id="1-store-operations-not-tensors"><a class="header" href="#1-store-operations-not-tensors">1. Store Operations, Not Tensors</a></h3>
<p><strong>Bad</strong> (store full tensors):</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Memory: O(n_ops * tensor_size)
struct TapeEntry {
    input_data: Vec&lt;f32&gt;,  // ❌ Wasteful
    output_data: Vec&lt;f32&gt;, // ❌ Wasteful
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Good</strong> (store IDs):</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Memory: O(n_ops)
struct TapeEntry {
    input_ids: Vec&lt;TensorId&gt;,  // ✅ Just integers
    output_id: TensorId,        // ✅ Just one integer
}
<span class="boring">}</span></code></pre></pre>
<h3 id="2-tensors-managed-separately"><a class="header" href="#2-tensors-managed-separately">2. Tensors Managed Separately</a></h3>
<p>Tensors are reference-counted (<code>Rc&lt;RefCell&lt;TensorData&gt;&gt;</code>):</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let x = Tensor::from_vec(vec![1.0, 2.0], true);
let y = &amp;x * &amp;x;  // y shares data with x via Rc

// When y is computed, x's data is still available
// Tape only stores IDs, not copies of data
<span class="boring">}</span></code></pre></pre>
<h3 id="3-tape-is-cleared-after-backward"><a class="header" href="#3-tape-is-cleared-after-backward">3. Tape is Cleared After Backward</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>backward(&amp;loss);  // Processes tape

// Tape is consumed and cleared
// Memory freed for next forward pass
<span class="boring">}</span></code></pre></pre>
<h2 id="dynamic-graphs"><a class="header" href="#dynamic-graphs">Dynamic Graphs</a></h2>
<p>Entrenar's tape enables <strong>dynamic computational graphs</strong> - the graph can change every forward pass:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>for epoch in 0..100 {
    let output = if epoch &lt; 50 {
        // First 50 epochs: simple model
        &amp;w1 * &amp;x + &amp;b1
    } else {
        // Last 50 epochs: complex model
        let h = relu(&amp;(&amp;w1 * &amp;x + &amp;b1));
        &amp;w2 * &amp;h + &amp;b2
    };

    backward(&amp;output);  // Different tape each epoch!
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Contrast with static graphs</strong> (TensorFlow 1.x):</p>
<ul>
<li>Static: Define graph once, compile, reuse</li>
<li>Dynamic (Entrenar): Build new graph every forward pass</li>
</ul>
<p><strong>Trade-offs</strong>:</p>
<ul>
<li>✅ Dynamic: Flexible (control flow, debugging)</li>
<li>✅ Static: Faster (compiled optimizations)</li>
<li>Entrenar chooses flexibility (similar to PyTorch)</li>
</ul>
<h2 id="tape-implementation-details"><a class="header" href="#tape-implementation-details">Tape Implementation Details</a></h2>
<h3 id="tape-creation"><a class="header" href="#tape-creation">Tape Creation</a></h3>
<p>When you create a tensor with <code>requires_grad=true</code>:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let x = Tensor::from_vec(vec![1.0], true);
<span class="boring">}</span></code></pre></pre>
<p>Entrenar initializes:</p>
<ol>
<li>Tensor data storage</li>
<li>Gradient storage (same size as data)</li>
<li>Registration for tape recording</li>
</ol>
<h3 id="operation-recording"><a class="header" href="#operation-recording">Operation Recording</a></h3>
<p>Every operation checks if recording is needed:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn add(lhs: &amp;Tensor, rhs: &amp;Tensor) -&gt; Tensor {
    // Forward computation
    let result_data = lhs.data() + rhs.data();

    // Check if we need to record
    if lhs.requires_grad() || rhs.requires_grad() {
        let result = Tensor::new(result_data, true);

        // Record on tape
        TAPE.with(|tape| {
            tape.borrow_mut().push(TapeEntry {
                operation: OpType::Add,
                inputs: vec![lhs.id(), rhs.id()],
                output: result.id(),
                metadata: {},
            });
        });

        result
    } else {
        // No gradients needed, skip tape
        Tensor::new(result_data, false)
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="backward-execution"><a class="header" href="#backward-execution">Backward Execution</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub fn backward(loss: &amp;Tensor) {
    // Seed gradient: dloss/dloss = 1.0
    loss.set_grad(vec![1.0]);

    // Get tape entries
    TAPE.with(|tape| {
        let entries = tape.borrow_mut().drain(..).collect::&lt;Vec&lt;_&gt;&gt;();

        // Process in reverse
        for entry in entries.into_iter().rev() {
            match entry.operation {
                OpType::Add =&gt; {
                    // Get output gradient
                    let grad_out = get_tensor(entry.output).grad();

                    // Propagate to inputs
                    get_tensor(entry.inputs[0]).accumulate_grad(&amp;grad_out);
                    get_tensor(entry.inputs[1]).accumulate_grad(&amp;grad_out);
                }
                OpType::Mul =&gt; {
                    let lhs = get_tensor(entry.inputs[0]);
                    let rhs = get_tensor(entry.inputs[1]);
                    let grad_out = get_tensor(entry.output).grad();

                    // d_lhs = grad_out * rhs
                    lhs.accumulate_grad(&amp;(grad_out * rhs.data()));

                    // d_rhs = grad_out * lhs
                    rhs.accumulate_grad(&amp;(grad_out * lhs.data()));
                }
                // ... other operations
            }
        }
    });
}
<span class="boring">}</span></code></pre></pre>
<h2 id="debugging-the-tape"><a class="header" href="#debugging-the-tape">Debugging the Tape</a></h2>
<p>You can inspect the tape for debugging:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[cfg(debug_assertions)]
fn print_tape() {
    TAPE.with(|tape| {
        println!("Tape contents:");
        for (i, entry) in tape.borrow().iter().enumerate() {
            println!("  [{}] {:?}", i, entry);
        }
    });
}

let x = Tensor::from_vec(vec![2.0], true);
let y = &amp;x * &amp;x;

print_tape();
// Output:
//   [0] Mul { inputs: [tensor_0, tensor_0], output: tensor_1 }
<span class="boring">}</span></code></pre></pre>
<h2 id="performance-considerations"><a class="header" href="#performance-considerations">Performance Considerations</a></h2>
<h3 id="tape-overhead"><a class="header" href="#tape-overhead">Tape Overhead</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Aspect</th><th>Cost</th><th>Mitigation</th></tr></thead><tbody>
<tr><td><strong>Recording</strong></td><td>O(1) per operation</td><td>Minimal (just push to Vec)</td></tr>
<tr><td><strong>Storage</strong></td><td>O(n_ops) metadata</td><td>Small (typically &lt;1MB for large models)</td></tr>
<tr><td><strong>Playback</strong></td><td>O(n_ops)</td><td>Necessary for gradients</td></tr>
</tbody></table>
</div>
<h3 id="optimization-no-grad-mode"><a class="header" href="#optimization-no-grad-mode">Optimization: No-Grad Mode</a></h3>
<p>Disable tape for inference:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Inference (no tape recording)
let output = model.forward(&amp;input);  // All tensors have requires_grad=false

// No tape entries created, faster forward pass
<span class="boring">}</span></code></pre></pre>
<h2 id="comparison-with-graph-based-autograd"><a class="header" href="#comparison-with-graph-based-autograd">Comparison with Graph-Based Autograd</a></h2>
<div class="table-wrapper"><table><thead><tr><th>Aspect</th><th>Tape-Based (Entrenar)</th><th>Graph-Based (TensorFlow 1.x)</th></tr></thead><tbody>
<tr><td><strong>Flexibility</strong></td><td>Dynamic (builds each forward)</td><td>Static (compile once)</td></tr>
<tr><td><strong>Debugging</strong></td><td>Easy (step through code)</td><td>Hard (symbolic graph)</td></tr>
<tr><td><strong>Performance</strong></td><td>Good (minimal overhead)</td><td>Excellent (compiled)</td></tr>
<tr><td><strong>Memory</strong></td><td>O(n_ops)</td><td>O(n_tensors + n_ops)</td></tr>
<tr><td><strong>Use Case</strong></td><td>Research, prototyping</td><td>Production at scale</td></tr>
</tbody></table>
</div>
<h2 id="key-takeaways-2"><a class="header" href="#key-takeaways-2">Key Takeaways</a></h2>
<ol>
<li><strong>Tape records operations</strong> during forward pass as metadata</li>
<li><strong>Backward replays tape in reverse</strong> to propagate gradients</li>
<li><strong>Gradients accumulate</strong> when tensors have multiple consumers</li>
<li><strong>Metadata stored</strong> for operations needing forward pass values</li>
<li><strong>Dynamic graphs</strong> rebuild tape each forward pass (flexibility)</li>
<li><strong>Memory efficient</strong> - stores IDs and metadata, not full tensors</li>
</ol>
<h2 id="whats-next-5"><a class="header" href="#whats-next-5">What's Next?</a></h2>
<ul>
<li><strong><a href="autograd/./backward-pass.html">Backward Pass</a></strong> - Detailed gradient propagation rules</li>
<li><strong><a href="autograd/./gradient-computation.html">Gradient Computation</a></strong> - Chain rule mechanics</li>
<li><strong><a href="autograd/./finite-difference.html">Finite Difference Validation</a></strong> - Testing gradient correctness</li>
</ul>
<hr />
<p><strong>Ready to understand backward passes?</strong> Continue to <a href="autograd/./backward-pass.html">Backward Pass</a> →</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="tensor-operations-1"><a class="header" href="#tensor-operations-1">Tensor Operations</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="matrix-multiplication-1"><a class="header" href="#matrix-multiplication-1">Matrix Multiplication</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="activations-relu-gelu-swish"><a class="header" href="#activations-relu-gelu-swish">Activations (ReLU, GELU, Swish)</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="layer-normalization-1"><a class="header" href="#layer-normalization-1">Layer Normalization</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="attention-mechanism"><a class="header" href="#attention-mechanism">Attention Mechanism</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="backward-pass"><a class="header" href="#backward-pass">Backward Pass</a></h1>
<p>The <strong>backward pass</strong> computes gradients by traversing the computational graph in reverse order, applying the chain rule at each operation. This chapter explains the mechanics of gradient propagation in Entrenar.</p>
<h2 id="the-chain-rule"><a class="header" href="#the-chain-rule">The Chain Rule</a></h2>
<p>The foundation of backpropagation is the <strong>multivariate chain rule</strong>:</p>
<pre><code>Given: z = f(y) and y = g(x)
Then:  dz/dx = dz/dy * dy/dx
</code></pre>
<p>For neural networks with many layers:</p>
<pre><code>Loss = f_n(f_{n-1}(...f_2(f_1(x))))

dLoss/dx = dLoss/df_n * df_n/df_{n-1} * ... * df_2/df_1 * df_1/dx
</code></pre>
<p>Entrenar automates this chain rule application.</p>
<h2 id="backward-pass-algorithm"><a class="header" href="#backward-pass-algorithm">Backward Pass Algorithm</a></h2>
<h3 id="high-level-steps"><a class="header" href="#high-level-steps">High-Level Steps</a></h3>
<ol>
<li><strong>Seed the gradient</strong>: Set output gradient to 1.0</li>
<li><strong>Traverse in reverse</strong>: Process tape entries from end to start</li>
<li><strong>Apply local gradients</strong>: Each operation computes input gradients from output gradient</li>
<li><strong>Accumulate gradients</strong>: Sum contributions when tensors have multiple consumers</li>
</ol>
<h3 id="pseudocode"><a class="header" href="#pseudocode">Pseudocode</a></h3>
<pre><code class="language-python">def backward(output_tensor):
    # Step 1: Seed gradient
    output_tensor.grad = 1.0

    # Step 2: Get tape entries
    tape = get_global_tape()

    # Step 3: Reverse traversal
    for entry in reversed(tape):
        # Get output gradient (already computed)
        grad_output = entry.output.grad

        # Step 4: Compute input gradients (chain rule)
        grad_inputs = entry.operation.backward(grad_output)

        # Step 5: Accumulate into input tensors
        for (input_tensor, grad_input) in zip(entry.inputs, grad_inputs):
            input_tensor.grad += grad_input  # Accumulation!
</code></pre>
<h2 id="operation-specific-backward-rules"><a class="header" href="#operation-specific-backward-rules">Operation-Specific Backward Rules</a></h2>
<p>Each operation implements a <code>backward</code> method that computes input gradients from output gradients.</p>
<h3 id="addition-z--x--y"><a class="header" href="#addition-z--x--y">Addition: z = x + y</a></h3>
<p><strong>Forward</strong>: <code>z_i = x_i + y_i</code></p>
<p><strong>Backward</strong>:</p>
<pre><code>∂z/∂x = 1  (gradient passes through unchanged)
∂z/∂y = 1

Therefore:
∂Loss/∂x = ∂Loss/∂z * 1 = ∂Loss/∂z
∂Loss/∂y = ∂Loss/∂z * 1 = ∂Loss/∂z
</code></pre>
<p><strong>Implementation</strong>:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn add_backward(grad_output: &amp;[f32], x: &amp;Tensor, y: &amp;Tensor) {
    // Gradient flows equally to both inputs
    x.accumulate_grad(grad_output);  // dx = dz
    y.accumulate_grad(grad_output);  // dy = dz
}
<span class="boring">}</span></code></pre></pre>
<h3 id="multiplication-z--x--y"><a class="header" href="#multiplication-z--x--y">Multiplication: z = x * y</a></h3>
<p><strong>Forward</strong>: <code>z_i = x_i * y_i</code></p>
<p><strong>Backward</strong>:</p>
<pre><code>∂z/∂x = y  (gradient scaled by other input)
∂z/∂y = x

Therefore:
∂Loss/∂x = ∂Loss/∂z * y
∂Loss/∂y = ∂Loss/∂z * x
</code></pre>
<p><strong>Implementation</strong>:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn mul_backward(grad_output: &amp;[f32], x: &amp;Tensor, y: &amp;Tensor) {
    // Gradient to x scaled by y's value
    let grad_x: Vec&lt;f32&gt; = grad_output.iter()
        .zip(y.data().iter())
        .map(|(g, y_val)| g * y_val)
        .collect();
    x.accumulate_grad(&amp;grad_x);

    // Gradient to y scaled by x's value
    let grad_y: Vec&lt;f32&gt; = grad_output.iter()
        .zip(x.data().iter())
        .map(|(g, x_val)| g * x_val)
        .collect();
    y.accumulate_grad(&amp;grad_y);
}
<span class="boring">}</span></code></pre></pre>
<h3 id="matrix-multiplication-c--a--b"><a class="header" href="#matrix-multiplication-c--a--b">Matrix Multiplication: C = A @ B</a></h3>
<p><strong>Forward</strong>: <code>C = A @ B</code> (dimensions: <code>C[m,n] = A[m,k] @ B[k,n]</code>)</p>
<p><strong>Backward</strong>:</p>
<pre><code>∂Loss/∂A = ∂Loss/∂C @ B^T
∂Loss/∂B = A^T @ ∂Loss/∂C
</code></pre>
<p><strong>Derivation</strong> (element-wise):</p>
<pre><code>C[i,j] = Σ_k A[i,k] * B[k,j]

∂C[i,j]/∂A[i,k] = B[k,j]  =&gt; ∂Loss/∂A[i,k] = Σ_j ∂Loss/∂C[i,j] * B[k,j]
                                             = (∂Loss/∂C @ B^T)[i,k]

∂C[i,j]/∂B[k,j] = A[i,k]  =&gt; ∂Loss/∂B[k,j] = Σ_i ∂Loss/∂C[i,j] * A[i,k]
                                             = (A^T @ ∂Loss/∂C)[k,j]
</code></pre>
<p><strong>Implementation</strong>:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn matmul_backward(
    grad_output: &amp;Tensor,  // dC
    a: &amp;Tensor,            // A
    b: &amp;Tensor,            // B
    m: usize,              // rows of A
    k: usize,              // cols of A = rows of B
    n: usize,              // cols of B
) {
    // dA = dC @ B^T
    let b_transpose = transpose(b, k, n);
    let grad_a = matmul(grad_output, &amp;b_transpose, m, n, k);
    a.accumulate_grad(grad_a.data());

    // dB = A^T @ dC
    let a_transpose = transpose(a, m, k);
    let grad_b = matmul(&amp;a_transpose, grad_output, k, m, n);
    b.accumulate_grad(grad_b.data());
}
<span class="boring">}</span></code></pre></pre>
<h3 id="relu-y--max0-x"><a class="header" href="#relu-y--max0-x">ReLU: y = max(0, x)</a></h3>
<p><strong>Forward</strong>: <code>y_i = max(0, x_i)</code></p>
<p><strong>Backward</strong>:</p>
<pre><code>∂y/∂x = {1 if x &gt; 0, 0 otherwise}

Therefore:
∂Loss/∂x_i = ∂Loss/∂y_i * (x_i &gt; 0 ? 1 : 0)
</code></pre>
<p><strong>Implementation</strong>:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn relu_backward(grad_output: &amp;[f32], x: &amp;Tensor) {
    let grad_x: Vec&lt;f32&gt; = grad_output.iter()
        .zip(x.data().iter())
        .map(|(g, &amp;x_val)| {
            if x_val &gt; 0.0 {
                *g  // Gradient passes through
            } else {
                0.0  // Gradient blocked
            }
        })
        .collect();

    x.accumulate_grad(&amp;grad_x);
}
<span class="boring">}</span></code></pre></pre>
<h3 id="gelu-y--x--Φx"><a class="header" href="#gelu-y--x--Φx">GELU: y = x * Φ(x)</a></h3>
<p><strong>Forward</strong>: <code>y = x * Φ(x)</code> where <code>Φ</code> is the Gaussian CDF</p>
<p><strong>Backward</strong> (using product rule):</p>
<pre><code>∂y/∂x = Φ(x) + x * φ(x)

where φ(x) = (1/√(2π)) * exp(-x²/2) is the Gaussian PDF
</code></pre>
<p><strong>Implementation</strong>:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn gelu_backward(grad_output: &amp;[f32], x: &amp;Tensor) {
    const SQRT_2_PI: f32 = 2.5066282746;  // √(2π)

    let grad_x: Vec&lt;f32&gt; = grad_output.iter()
        .zip(x.data().iter())
        .map(|(g, &amp;x_val)| {
            let phi = gaussian_cdf(x_val);          // Φ(x)
            let phi_prime = (-0.5 * x_val.powi(2)).exp() / SQRT_2_PI;  // φ(x)
            let local_grad = phi + x_val * phi_prime;

            g * local_grad
        })
        .collect();

    x.accumulate_grad(&amp;grad_x);
}
<span class="boring">}</span></code></pre></pre>
<h3 id="layer-normalization-2"><a class="header" href="#layer-normalization-2">Layer Normalization</a></h3>
<p><strong>Forward</strong>:</p>
<pre><code>y = (x - μ) / σ

where:
  μ = mean(x)
  σ = √(variance(x) + ε)
</code></pre>
<p><strong>Backward</strong> (complex chain rule):</p>
<pre><code>∂Loss/∂x_i = (1/σ) * [∂Loss/∂y_i - (1/n)Σ_j ∂Loss/∂y_j - (1/n)y_i Σ_j(∂Loss/∂y_j * y_j)]
</code></pre>
<p><strong>Implementation</strong>:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn layernorm_backward(
    grad_output: &amp;[f32],
    x: &amp;Tensor,
    normalized: &amp;[f32],  // y values from forward pass
    mean: f32,
    variance: f32,
) {
    let n = grad_output.len() as f32;
    let std_inv = 1.0 / (variance + 1e-5).sqrt();

    // Compute sum terms
    let sum_grad: f32 = grad_output.iter().sum();
    let sum_grad_y: f32 = grad_output.iter()
        .zip(normalized.iter())
        .map(|(g, y)| g * y)
        .sum();

    // Compute gradient for each element
    let grad_x: Vec&lt;f32&gt; = grad_output.iter()
        .zip(normalized.iter())
        .map(|(g, y)| {
            std_inv * (g - sum_grad / n - y * sum_grad_y / n)
        })
        .collect();

    x.accumulate_grad(&amp;grad_x);
}
<span class="boring">}</span></code></pre></pre>
<h2 id="gradient-accumulation-1"><a class="header" href="#gradient-accumulation-1">Gradient Accumulation</a></h2>
<p>When a tensor is used multiple times, gradients accumulate:</p>
<h3 id="example-z--x--x"><a class="header" href="#example-z--x--x">Example: z = x + x</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let x = Tensor::from_vec(vec![2.0], true);
let z = &amp;x + &amp;x;  // z = 2x

backward(&amp;z);

println!("dz/dx = {}", x.grad()[0]);  // 2.0 ✅
<span class="boring">}</span></code></pre></pre>
<p><strong>Why 2.0?</strong></p>
<pre><code>Graph:
    x ─┬─&gt; Add -&gt; z
       └─&gt;

Backward:
  From first input:  dx = dz * 1 = 1.0
  From second input: dx = dz * 1 = 1.0
  Total:             dx = 1.0 + 1.0 = 2.0 ✅
</code></pre>
<p><strong>Implementation</strong>:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Always use += for gradient accumulation
x.grad_mut()[i] += gradient_contribution;
<span class="boring">}</span></code></pre></pre>
<h3 id="complex-example"><a class="header" href="#complex-example">Complex Example</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let x = Tensor::from_vec(vec![3.0], true);
let y = Tensor::from_vec(vec![4.0], true);

let a = &amp;x + &amp;y;   // a = x + y = 7
let b = &amp;x * &amp;y;   // b = x * y = 12
let c = &amp;a + &amp;b;   // c = a + b = 19

backward(&amp;c);
<span class="boring">}</span></code></pre></pre>
<p><strong>Gradient computation:</strong></p>
<pre><code>Tape (forward order):
  [0] Add(x, y) -&gt; a
  [1] Mul(x, y) -&gt; b
  [2] Add(a, b) -&gt; c

Backward (reverse order):
  [2] Add: da = dc = 1.0, db = dc = 1.0
  [1] Mul: dx += db * y = 1.0 * 4 = 4.0
           dy += db * x = 1.0 * 3 = 3.0
  [0] Add: dx += da = 1.0
           dy += da = 1.0

Final gradients:
  dx = 4.0 + 1.0 = 5.0  ✅ (= y + 1)
  dy = 3.0 + 1.0 = 4.0  ✅ (= x + 1)
</code></pre>
<p><strong>Manual verification:</strong></p>
<pre><code>c = (x + y) + (x * y) = x + y + xy
dc/dx = 1 + y = 1 + 4 = 5.0 ✅
dc/dy = 1 + x = 1 + 3 = 4.0 ✅
</code></pre>
<h2 id="handling-non-differentiable-points"><a class="header" href="#handling-non-differentiable-points">Handling Non-Differentiable Points</a></h2>
<p>Some operations have non-differentiable points where we use <strong>subgradients</strong>.</p>
<h3 id="relu-at-x0"><a class="header" href="#relu-at-x0">ReLU at x=0</a></h3>
<pre><code>ReLU(x) = max(0, x)

Derivative:
  d/dx ReLU(x) = {1 if x &gt; 0, 0 if x &lt; 0, ??? if x = 0}
</code></pre>
<p><strong>Solution</strong>: Use subgradient convention:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>if x_val &gt; 0.0 {
    1.0
} else {
    0.0  // Subgradient at x=0 (could also use 1.0 or 0.5)
}
<span class="boring">}</span></code></pre></pre>
<p><strong>In practice</strong>: Exact x=0 is rare with floating-point numbers, so the choice rarely matters.</p>
<h2 id="detaching-gradients"><a class="header" href="#detaching-gradients">Detaching Gradients</a></h2>
<p>Sometimes you want to <strong>stop gradients</strong> from flowing:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let x = Tensor::from_vec(vec![2.0], true);
let y = &amp;x * &amp;x;  // y = x²

// Detach: treat y as a constant for further operations
let y_detached = Tensor::from_vec(y.data().clone(), false);  // requires_grad=false

let z = &amp;y_detached + &amp;x;  // z = y_detached + x (y treated as constant)

backward(&amp;z);

println!("dz/dx = {}", x.grad()[0]);  // 1.0 (only from addition, not from y)
<span class="boring">}</span></code></pre></pre>
<p><strong>Use case</strong>: Stopping gradient flow in certain model parts (e.g., frozen layers).</p>
<h2 id="in-place-operations-warning"><a class="header" href="#in-place-operations-warning">In-Place Operations Warning</a></h2>
<p><strong>In-place modifications break the computational graph:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let mut x = Tensor::from_vec(vec![1.0, 2.0], true);
let y = &amp;x * &amp;x;

// ❌ BAD: Modify x in-place
x.data_mut()[0] = 5.0;

backward(&amp;y);  // ⚠️ Undefined behavior! x changed after being used
<span class="boring">}</span></code></pre></pre>
<p><strong>Solution</strong>: Entrenar prevents in-place modifications for tensors with <code>requires_grad=true</code>:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Entrenar's safeguard
if x.requires_grad() {
    panic!("Cannot modify tensor with requires_grad=true in-place");
}
<span class="boring">}</span></code></pre></pre>
<h2 id="computational-complexity"><a class="header" href="#computational-complexity">Computational Complexity</a></h2>
<div class="table-wrapper"><table><thead><tr><th>Operation</th><th>Forward</th><th>Backward</th><th>Total</th></tr></thead><tbody>
<tr><td><strong>Add/Mul</strong></td><td>O(n)</td><td>O(n)</td><td>O(n)</td></tr>
<tr><td><strong>MatMul</strong></td><td>O(mnk)</td><td>O(mnk)</td><td>O(mnk)</td></tr>
<tr><td><strong>ReLU</strong></td><td>O(n)</td><td>O(n)</td><td>O(n)</td></tr>
<tr><td><strong>LayerNorm</strong></td><td>O(n)</td><td>O(n)</td><td>O(n)</td></tr>
<tr><td><strong>Attention</strong></td><td>O(n²d)</td><td>O(n²d)</td><td>O(n²d)</td></tr>
</tbody></table>
</div>
<p><strong>Key insight</strong>: Backward pass has same asymptotic complexity as forward pass.</p>
<h2 id="debugging-gradients"><a class="header" href="#debugging-gradients">Debugging Gradients</a></h2>
<h3 id="check-if-gradients-are-computed"><a class="header" href="#check-if-gradients-are-computed">Check if Gradients are Computed</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let x = Tensor::from_vec(vec![2.0], true);
let y = &amp;x * &amp;x;

backward(&amp;y);

if x.grad()[0] == 0.0 {
    eprintln!("Warning: Gradient is zero (might indicate issue)");
}
<span class="boring">}</span></code></pre></pre>
<h3 id="gradient-explosionvanishing"><a class="header" href="#gradient-explosionvanishing">Gradient Explosion/Vanishing</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn check_gradients(params: &amp;[&amp;Tensor]) {
    for param in params {
        let grad_norm = param.grad().iter().map(|g| g * g).sum::&lt;f32&gt;().sqrt();

        if grad_norm &gt; 100.0 {
            eprintln!("Warning: Gradient explosion (norm={})", grad_norm);
        } else if grad_norm &lt; 1e-7 {
            eprintln!("Warning: Gradient vanishing (norm={})", grad_norm);
        }
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="gradient-checking-3"><a class="header" href="#gradient-checking-3">Gradient Checking</a></h3>
<p>Always validate custom operations with finite differences:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[test]
fn test_my_operation_backward() {
    let x = Tensor::from_vec(vec![1.0, 2.0, 3.0], true);
    let y = my_custom_operation(&amp;x);

    backward(&amp;y);

    // Compare with numerical gradient
    check_gradient(&amp;y, &amp;x, epsilon=1e-3, threshold=0.2);
}
<span class="boring">}</span></code></pre></pre>
<h2 id="key-takeaways-3"><a class="header" href="#key-takeaways-3">Key Takeaways</a></h2>
<ol>
<li><strong>Backward pass applies chain rule</strong> in reverse topological order</li>
<li><strong>Each operation implements local gradient rule</strong> (e.g., mul: dx = y*dz)</li>
<li><strong>Gradients accumulate</strong> when tensors have multiple consumers</li>
<li><strong>Matrix operations</strong> use transposition for gradient computation</li>
<li><strong>Nonlinear activations</strong> use derivative of activation function</li>
<li><strong>Normalization</strong> requires saved statistics from forward pass</li>
<li><strong>Complexity</strong> of backward equals forward (asymptotically)</li>
</ol>
<h2 id="whats-next-6"><a class="header" href="#whats-next-6">What's Next?</a></h2>
<ul>
<li><strong><a href="autograd/./gradient-computation.html">Gradient Computation</a></strong> - Mathematical derivations</li>
<li><strong><a href="autograd/./finite-difference.html">Finite Difference Validation</a></strong> - Testing gradients</li>
<li><strong><a href="autograd/./tensor-operations.html">Tensor Operations</a></strong> - All supported operations</li>
</ul>
<hr />
<p><strong>Ready to dive into the math?</strong> Continue to <a href="autograd/./gradient-computation.html">Gradient Computation</a> →</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="gradient-computation"><a class="header" href="#gradient-computation">Gradient Computation</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="finite-difference-validation"><a class="header" href="#finite-difference-validation">Finite Difference Validation</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="overview-1"><a class="header" href="#overview-1">Overview</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="stochastic-gradient-descent-sgd"><a class="header" href="#stochastic-gradient-descent-sgd">Stochastic Gradient Descent (SGD)</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="adam-optimizer"><a class="header" href="#adam-optimizer">Adam Optimizer</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="adamw-decoupled-weight-decay"><a class="header" href="#adamw-decoupled-weight-decay">AdamW (Decoupled Weight Decay)</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="learning-rate-schedulers-1"><a class="header" href="#learning-rate-schedulers-1">Learning Rate Schedulers</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="cosine-annealing"><a class="header" href="#cosine-annealing">Cosine Annealing</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="step-decay"><a class="header" href="#step-decay">Step Decay</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="exponential-decay"><a class="header" href="#exponential-decay">Exponential Decay</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="gradient-clipping-2"><a class="header" href="#gradient-clipping-2">Gradient Clipping</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="simd-accelerated-updates"><a class="header" href="#simd-accelerated-updates">SIMD-Accelerated Updates</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="optimizer-theory"><a class="header" href="#optimizer-theory">Optimizer Theory</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="what-is-lora"><a class="header" href="#what-is-lora">What is LoRA?</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="parameter-efficient-fine-tuning"><a class="header" href="#parameter-efficient-fine-tuning">Parameter-Efficient Fine-Tuning</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="lora-layer-architecture"><a class="header" href="#lora-layer-architecture">LoRA Layer Architecture</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="low-rank-matrices-a-and-b"><a class="header" href="#low-rank-matrices-a-and-b">Low-Rank Matrices A and B</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="scaling-factor-alpharank"><a class="header" href="#scaling-factor-alpharank">Scaling Factor (alpha/rank)</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="merge-and-unmerge"><a class="header" href="#merge-and-unmerge">Merge and Unmerge</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="target-module-selection"><a class="header" href="#target-module-selection">Target Module Selection</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="gradient-flow-isolation"><a class="header" href="#gradient-flow-isolation">Gradient Flow Isolation</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="adapter-persistence-1"><a class="header" href="#adapter-persistence-1">Adapter Persistence</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="saving-adapters"><a class="header" href="#saving-adapters">Saving Adapters</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="loading-adapters"><a class="header" href="#loading-adapters">Loading Adapters</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="sharing-adapters"><a class="header" href="#sharing-adapters">Sharing Adapters</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="memory-efficient-fine-tuning"><a class="header" href="#memory-efficient-fine-tuning">Memory-Efficient Fine-Tuning</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="4-bit-quantization-1"><a class="header" href="#4-bit-quantization-1">4-bit Quantization</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="block-wise-quantization"><a class="header" href="#block-wise-quantization">Block-Wise Quantization</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="scale-factors"><a class="header" href="#scale-factors">Scale Factors</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="quantizationdequantization"><a class="header" href="#quantizationdequantization">Quantization/Dequantization</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="qlora-layer"><a class="header" href="#qlora-layer">QLoRA Layer</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="on-the-fly-dequantization"><a class="header" href="#on-the-fly-dequantization">On-the-Fly Dequantization</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="memory-benchmarks"><a class="header" href="#memory-benchmarks">Memory Benchmarks</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="lora-vs-qlora-comparison"><a class="header" href="#lora-vs-qlora-comparison">LoRA vs QLoRA Comparison</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="transformer-model-benchmarks"><a class="header" href="#transformer-model-benchmarks">Transformer Model Benchmarks</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="compression-ratios"><a class="header" href="#compression-ratios">Compression Ratios</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="trade-offs-and-best-practices"><a class="header" href="#trade-offs-and-best-practices">Trade-offs and Best Practices</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="model-merging-overview"><a class="header" href="#model-merging-overview">Model Merging Overview</a></h1>
<p><strong>Model merging</strong> combines multiple fine-tuned models into a single unified model that retains capabilities from all source models.</p>
<h2 id="the-problem"><a class="header" href="#the-problem">The Problem</a></h2>
<p>When you fine-tune multiple models for different tasks, you end up with N separate models:</p>
<pre><code>Base Model (7B params)
  ├→ Model A: Fine-tuned on coding tasks
  ├→ Model B: Fine-tuned on math problems
  └→ Model C: Fine-tuned on creative writing
</code></pre>
<p><strong>Challenge</strong>: How do you create a single model that performs well on all three tasks without:</p>
<ul>
<li>Retraining from scratch (expensive)</li>
<li>Serving N models in parallel (memory/latency overhead)</li>
<li>Losing task-specific knowledge (catastrophic forgetting)</li>
</ul>
<h2 id="the-solution-weight-merging"><a class="header" href="#the-solution-weight-merging">The Solution: Weight Merging</a></h2>
<p>Entrenar implements three state-of-the-art merging algorithms from Arcee AI:</p>
<h3 id="ties-task-inference-via-elimination-and-sign-voting"><a class="header" href="#ties-task-inference-via-elimination-and-sign-voting">TIES (Task Inference via Elimination and Sign voting)</a></h3>
<p><strong>Key Idea</strong>: Resolve parameter conflicts by keeping top-k% changes and using sign voting</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use entrenar::merge::TIESMerger;

// density=0.5 keeps top 50% of changes
// lambda=1.0 gives equal weight to all models
let merger = TIESMerger::new(0.5, 1.0);
let merged = merger.merge(&amp;models)?;
<span class="boring">}</span></code></pre></pre>
<p><strong>From <code>src/merge/ties.rs</code></strong></p>
<h3 id="dare-drop-and-rescale"><a class="header" href="#dare-drop-and-rescale">DARE (Drop And REscale)</a></h3>
<p><strong>Key Idea</strong>: Randomly drop parameter updates with Bernoulli masking, then rescale</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use entrenar::merge::DAREMerger;

// drop_rate=0.9 means keep only 10% of updates
let merger = DAREMerger::new(0.9);
let merged = merger.merge(&amp;models)?;
<span class="boring">}</span></code></pre></pre>
<p><strong>From <code>src/merge/dare.rs</code></strong></p>
<h3 id="slerp-spherical-linear-interpolation"><a class="header" href="#slerp-spherical-linear-interpolation">SLERP (Spherical Linear intERPolation)</a></h3>
<p><strong>Key Idea</strong>: Interpolate on the weight manifold (preserves magnitude)</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use entrenar::merge::SLERPMerger;

// t=0.5 gives 50-50 interpolation between two models
let merger = SLERPMerger::new(0.5);
let merged = merger.merge(&amp;[model_a, model_b])?;
<span class="boring">}</span></code></pre></pre>
<p><strong>From <code>src/merge/slerp.rs</code></strong></p>
<h2 id="when-to-use-each-algorithm"><a class="header" href="#when-to-use-each-algorithm">When to Use Each Algorithm</a></h2>
<div class="table-wrapper"><table><thead><tr><th>Algorithm</th><th>Use Case</th><th>Best For</th></tr></thead><tbody>
<tr><td><strong>TIES</strong></td><td>Multi-task merging (3+ models)</td><td>Resolving parameter conflicts across many tasks</td></tr>
<tr><td><strong>DARE</strong></td><td>Sparse fine-tuning merges</td><td>LoRA adapters, small delta updates</td></tr>
<tr><td><strong>SLERP</strong></td><td>Two-model interpolation</td><td>Smooth transitions, model averaging</td></tr>
</tbody></table>
</div>
<h2 id="implementation-details"><a class="header" href="#implementation-details">Implementation Details</a></h2>
<p>All merging algorithms in Entrenar are:</p>
<ul>
<li>✅ <strong>Tested</strong>: Property-based tests for permutation invariance</li>
<li>✅ <strong>Validated</strong>: Works with full models and LoRA adapters</li>
<li>✅ <strong>Type-safe</strong>: Compile-time guarantees via Rust's type system</li>
</ul>
<h2 id="next-steps-2"><a class="header" href="#next-steps-2">Next Steps</a></h2>
<ul>
<li><a href="merging/./ties.html">TIES Algorithm</a> - Detailed TIES implementation</li>
<li><a href="merging/./dare.html">DARE Algorithm</a> - Drop and rescale mechanics</li>
<li><a href="merging/./slerp.html">SLERP Algorithm</a> - Spherical interpolation</li>
<li><a href="merging/../examples/merge-models.html">Examples</a> - Real-world merging examples</li>
</ul>
<h2 id="references"><a class="header" href="#references">References</a></h2>
<p>Based on:</p>
<ul>
<li><strong>TIES-Merging</strong> paper (Yadav et al., 2023)</li>
<li><strong>DARE</strong> paper (Yu et al., 2024)</li>
<li><strong>SLERP</strong> (classic computer graphics technique)</li>
<li><strong>Arcee AI</strong> merging research</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="ties"><a class="header" href="#ties">Ties</a></h1>
<p><strong>Content to be added.</strong></p>
<p>This chapter will cover:</p>
<ul>
<li>Key concepts and implementation details</li>
<li>Code examples from the entrenar codebase</li>
<li>Best practices and usage guidelines</li>
</ul>
<p>Please check back later for complete content.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="dare"><a class="header" href="#dare">Dare</a></h1>
<p><strong>Content to be added.</strong></p>
<p>This chapter will cover:</p>
<ul>
<li>Key concepts and implementation details</li>
<li>Code examples from the entrenar codebase</li>
<li>Best practices and usage guidelines</li>
</ul>
<p>Please check back later for complete content.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="slerp"><a class="header" href="#slerp">Slerp</a></h1>
<p><strong>Content to be added.</strong></p>
<p>This chapter will cover:</p>
<ul>
<li>Key concepts and implementation details</li>
<li>Code examples from the entrenar codebase</li>
<li>Best practices and usage guidelines</li>
</ul>
<p>Please check back later for complete content.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="multi-model"><a class="header" href="#multi-model">Multi Model</a></h1>
<p><strong>Content to be added.</strong></p>
<p>This chapter will cover:</p>
<ul>
<li>Key concepts and implementation details</li>
<li>Code examples from the entrenar codebase</li>
<li>Best practices and usage guidelines</li>
</ul>
<p>Please check back later for complete content.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="best-practices-1"><a class="header" href="#best-practices-1">Best Practices</a></h1>
<p><strong>Content to be added.</strong></p>
<p>This chapter will cover:</p>
<ul>
<li>Key concepts and implementation details</li>
<li>Code examples from the entrenar codebase</li>
<li>Best practices and usage guidelines</li>
</ul>
<p>Please check back later for complete content.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="what-is-knowledge-distillation"><a class="header" href="#what-is-knowledge-distillation">What is Knowledge Distillation?</a></h1>
<p><strong>Knowledge distillation</strong> trains a smaller "student" model to mimic a larger "teacher" model's behavior.</p>
<h2 id="the-problem-1"><a class="header" href="#the-problem-1">The Problem</a></h2>
<p>Large models (7B-70B parameters) perform well but are:</p>
<ul>
<li><strong>Expensive to deploy</strong>: High memory and compute costs</li>
<li><strong>Slow inference</strong>: Too slow for latency-sensitive applications</li>
<li><strong>Resource-intensive</strong>: Require powerful hardware</li>
</ul>
<p><strong>Goal</strong>: Transfer knowledge from large teacher →  smaller student while preserving performance</p>
<h2 id="the-solution"><a class="header" href="#the-solution">The Solution</a></h2>
<pre><code>Teacher Model (7B params)  →  Knowledge Transfer  →  Student Model (1B params)
   Accuracy: 92%                                         Accuracy: 89% (vs 82% from scratch)
</code></pre>
<p><strong>Key Insight</strong>: Train student on <strong>soft targets</strong> (teacher's probability distributions) rather than hard labels</p>
<h2 id="how-it-works"><a class="header" href="#how-it-works">How It Works</a></h2>
<p>From <code>src/distill/loss.rs</code>:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use entrenar::distill::DistillationLoss;

// Temperature=3.0, alpha=0.7
let loss_fn = DistillationLoss::new(3.0, 0.7);

// Combine soft targets from teacher + hard labels
let loss = loss_fn.forward(&amp;student_logits, &amp;teacher_logits, &amp;labels);
<span class="boring">}</span></code></pre></pre>
<h3 id="distillation-loss-formula"><a class="header" href="#distillation-loss-formula">Distillation Loss Formula</a></h3>
<pre><code>L = α * T² * KL(softmax(teacher/T) || softmax(student/T))
  + (1-α) * CrossEntropy(student, labels)
</code></pre>
<p>Where:</p>
<ul>
<li><strong>T</strong> = Temperature (typically 2.0-5.0)</li>
<li><strong>α</strong> = Distillation weight (typically 0.5-0.9)</li>
<li><strong>KL</strong> = Kullback-Leibler divergence (measures distribution similarity)</li>
</ul>
<h3 id="temperature-smoothing"><a class="header" href="#temperature-smoothing">Temperature Smoothing</a></h3>
<p><strong>Temperature</strong> softens probability distributions:</p>
<pre><code>Logits:     [2.0, 1.0, 0.1]

T=1 (hard): [0.659, 0.242, 0.099]  ← Sharp peaks
T=3 (soft): [0.422, 0.307, 0.271]  ← Smoother distribution
</code></pre>
<p><strong>Why soft targets help</strong>: Reveal model's "uncertainty" and inter-class relationships</p>
<h2 id="distillation-methods-in-entrenar"><a class="header" href="#distillation-methods-in-entrenar">Distillation Methods in Entrenar</a></h2>
<h3 id="1-temperature-scaled-kl-divergence"><a class="header" href="#1-temperature-scaled-kl-divergence">1. Temperature-Scaled KL Divergence</a></h3>
<p>Standard distillation with soft targets:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let loss_fn = DistillationLoss::new(3.0, 0.7);
<span class="boring">}</span></code></pre></pre>
<p><strong>From <code>src/distill/loss.rs</code></strong></p>
<h3 id="2-multi-teacher-ensemble"><a class="header" href="#2-multi-teacher-ensemble">2. Multi-Teacher Ensemble</a></h3>
<p>Distill from multiple teachers simultaneously:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use entrenar::distill::EnsembleDistiller;

let distiller = EnsembleDistiller::new(vec![teacher1, teacher2, teacher3]);
let loss = distiller.forward(&amp;student_logits, &amp;teacher_logits_list, &amp;labels);
<span class="boring">}</span></code></pre></pre>
<p><strong>From <code>src/distill/ensemble.rs</code></strong></p>
<h3 id="3-progressive-layer-wise"><a class="header" href="#3-progressive-layer-wise">3. Progressive Layer-Wise</a></h3>
<p>Layer-by-layer knowledge transfer:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use entrenar::distill::ProgressiveDistiller;

let distiller = ProgressiveDistiller::new();
distiller.distill_layer(student_layer, teacher_layer)?;
<span class="boring">}</span></code></pre></pre>
<p><strong>From <code>src/distill/progressive.rs</code></strong></p>
<h2 id="validation"><a class="header" href="#validation">Validation</a></h2>
<p><strong>44 distillation tests</strong> including:</p>
<ul>
<li>13 property-based tests for temperature smoothing</li>
<li>KL divergence correctness validation</li>
<li>Multi-teacher ensemble tests</li>
<li>Progressive distillation tests</li>
</ul>
<h2 id="when-to-use-distillation"><a class="header" href="#when-to-use-distillation">When to Use Distillation</a></h2>
<div class="table-wrapper"><table><thead><tr><th>Scenario</th><th>Recommended Method</th></tr></thead><tbody>
<tr><td><strong>Deployment optimization</strong></td><td>Standard KL divergence</td></tr>
<tr><td><strong>Multiple expert models</strong></td><td>Multi-teacher ensemble</td></tr>
<tr><td><strong>Very deep networks</strong></td><td>Progressive layer-wise</td></tr>
<tr><td><strong>Limited training data</strong></td><td>Higher alpha (more distillation weight)</td></tr>
</tbody></table>
</div>
<h2 id="example-results"><a class="header" href="#example-results">Example Results</a></h2>
<pre><code>Task: Text classification (SST-2 dataset)

Teacher (BERT-large, 340M params):  Accuracy: 93.2%
Student (BERT-tiny, 14M params):
  - From scratch:                   Accuracy: 84.1%
  - With distillation (T=3, α=0.8): Accuracy: 89.7%  (+5.6% improvement)
</code></pre>
<h2 id="next-steps-3"><a class="header" href="#next-steps-3">Next Steps</a></h2>
<ul>
<li><a href="distillation/./temperature-kl.html">Temperature-Scaled KL Divergence</a></li>
<li><a href="distillation/./multi-teacher.html">Multi-Teacher Ensemble</a></li>
<li><a href="distillation/./progressive.html">Progressive Layer-Wise</a></li>
<li><a href="distillation/../examples/distillation.html">Examples</a></li>
</ul>
<h2 id="references-1"><a class="header" href="#references-1">References</a></h2>
<ul>
<li><strong>Hinton et al. (2015)</strong>: "Distilling the Knowledge in a Neural Network"</li>
<li><strong>Sanh et al. (2019)</strong>: DistilBERT paper</li>
<li>Implementation in <code>src/distill/</code></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="temperature-kl"><a class="header" href="#temperature-kl">Temperature Kl</a></h1>
<p><strong>Content to be added.</strong></p>
<p>This chapter will cover:</p>
<ul>
<li>Key concepts and implementation details</li>
<li>Code examples from the entrenar codebase</li>
<li>Best practices and usage guidelines</li>
</ul>
<p>Please check back later for complete content.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="multi-teacher"><a class="header" href="#multi-teacher">Multi Teacher</a></h1>
<p><strong>Content to be added.</strong></p>
<p>This chapter will cover:</p>
<ul>
<li>Key concepts and implementation details</li>
<li>Code examples from the entrenar codebase</li>
<li>Best practices and usage guidelines</li>
</ul>
<p>Please check back later for complete content.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="progressive"><a class="header" href="#progressive">Progressive</a></h1>
<p><strong>Content to be added.</strong></p>
<p>This chapter will cover:</p>
<ul>
<li>Key concepts and implementation details</li>
<li>Code examples from the entrenar codebase</li>
<li>Best practices and usage guidelines</li>
</ul>
<p>Please check back later for complete content.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="loss-functions"><a class="header" href="#loss-functions">Loss Functions</a></h1>
<p><strong>Content to be added.</strong></p>
<p>This chapter will cover:</p>
<ul>
<li>Key concepts and implementation details</li>
<li>Code examples from the entrenar codebase</li>
<li>Best practices and usage guidelines</li>
</ul>
<p>Please check back later for complete content.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="student-teacher"><a class="header" href="#student-teacher">Student Teacher</a></h1>
<p><strong>Content to be added.</strong></p>
<p>This chapter will cover:</p>
<ul>
<li>Key concepts and implementation details</li>
<li>Code examples from the entrenar codebase</li>
<li>Best practices and usage guidelines</li>
</ul>
<p>Please check back later for complete content.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="trainer-api"><a class="header" href="#trainer-api">Trainer Api</a></h1>
<p><strong>Content to be added.</strong></p>
<p>This chapter will cover:</p>
<ul>
<li>Key concepts and implementation details</li>
<li>Code examples from the entrenar codebase</li>
<li>Best practices and usage guidelines</li>
</ul>
<p>Please check back later for complete content.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="train-config"><a class="header" href="#train-config">Train Config</a></h1>
<p><strong>Content to be added.</strong></p>
<p>This chapter will cover:</p>
<ul>
<li>Key concepts and implementation details</li>
<li>Code examples from the entrenar codebase</li>
<li>Best practices and usage guidelines</li>
</ul>
<p>Please check back later for complete content.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="basic-training-loop"><a class="header" href="#basic-training-loop">Basic Training Loop</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="batching-and-data-loading"><a class="header" href="#batching-and-data-loading">Batching and Data Loading</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="loss-functions-1"><a class="header" href="#loss-functions-1">Loss Functions</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="validation-and-testing"><a class="header" href="#validation-and-testing">Validation and Testing</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="checkpointing-1"><a class="header" href="#checkpointing-1">Checkpointing</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="early-stopping"><a class="header" href="#early-stopping">Early Stopping</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="model-io-overview"><a class="header" href="#model-io-overview">Model I/O Overview</a></h1>
<p><strong>Model I/O</strong> provides save/load functionality for neural network models with support for multiple serialization formats.</p>
<h2 id="the-problem-2"><a class="header" href="#the-problem-2">The Problem</a></h2>
<p>After training a model, you need to:</p>
<ul>
<li><strong>Save model weights</strong> for deployment</li>
<li><strong>Load trained models</strong> for inference or continued training</li>
<li><strong>Share models</strong> with collaborators</li>
<li><strong>Version control</strong> model checkpoints</li>
<li><strong>Metadata tracking</strong> (hyperparameters, training config, etc.)</li>
</ul>
<h2 id="the-solution-1"><a class="header" href="#the-solution-1">The Solution</a></h2>
<p>Entrenar's Model I/O system (from <code>src/io/</code>) provides:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use entrenar::io::{save_model, load_model, Model, ModelMetadata, SaveConfig, ModelFormat};

// Create model with metadata
let metadata = ModelMetadata::new("my-model", "transformer")
    .with_version("0.1.0")
    .with_custom("learning_rate", 0.001);

let model = Model::new(metadata, parameters);

// Save to JSON
let config = SaveConfig::new(ModelFormat::Json).with_pretty(true);
save_model(&amp;model, "model.json", &amp;config)?;

// Load (format auto-detected from extension)
let loaded = load_model("model.json")?;
<span class="boring">}</span></code></pre></pre>
<h2 id="supported-formats"><a class="header" href="#supported-formats">Supported Formats</a></h2>
<div class="table-wrapper"><table><thead><tr><th>Format</th><th>Extension</th><th>Use Case</th><th>Status</th></tr></thead><tbody>
<tr><td><strong>JSON</strong></td><td><code>.json</code></td><td>Human-readable, debugging</td><td>✅ Implemented</td></tr>
<tr><td><strong>YAML</strong></td><td><code>.yaml</code>, <code>.yml</code></td><td>Configuration-friendly</td><td>✅ Implemented</td></tr>
<tr><td><strong>GGUF</strong></td><td><code>.gguf</code></td><td>LLaMA-compatible format</td><td>⚠️ Placeholder (future Realizar integration)</td></tr>
</tbody></table>
</div>
<h3 id="json-format"><a class="header" href="#json-format">JSON Format</a></h3>
<p><strong>Compact</strong> (single-line):</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let config = SaveConfig::new(ModelFormat::Json).with_pretty(false);
save_model(&amp;model, "model.json", &amp;config)?;
<span class="boring">}</span></code></pre></pre>
<p><strong>Pretty</strong> (indented):</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let config = SaveConfig::new(ModelFormat::Json).with_pretty(true);
save_model(&amp;model, "model.json", &amp;config)?;
<span class="boring">}</span></code></pre></pre>
<h3 id="yaml-format"><a class="header" href="#yaml-format">YAML Format</a></h3>
<p>Human-friendly for configuration:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let config = SaveConfig::new(ModelFormat::Yaml);
save_model(&amp;model, "model.yaml", &amp;config)?;
<span class="boring">}</span></code></pre></pre>
<h3 id="gguf-format"><a class="header" href="#gguf-format">GGUF Format</a></h3>
<p><strong>Placeholder</strong> for future integration with Realizar:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Will be supported in v0.2.0+
let config = SaveConfig::new(ModelFormat::Gguf);
save_model(&amp;model, "model.gguf", &amp;config)?;  // Currently returns error
<span class="boring">}</span></code></pre></pre>
<h2 id="model-structure"><a class="header" href="#model-structure">Model Structure</a></h2>
<h3 id="model"><a class="header" href="#model">Model</a></h3>
<p>Contains parameters and metadata:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct Model {
    pub metadata: ModelMetadata,
    pub parameters: Vec&lt;(String, Tensor)&gt;,
}
<span class="boring">}</span></code></pre></pre>
<h3 id="modelmetadata"><a class="header" href="#modelmetadata">ModelMetadata</a></h3>
<p>Tracks model information:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct ModelMetadata {
    pub name: String,
    pub architecture: String,
    pub version: String,
    pub training_config: Option&lt;HashMap&lt;String, Value&gt;&gt;,
    pub custom: HashMap&lt;String, Value&gt;,  // Flexible key-value pairs
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Example</strong>:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let metadata = ModelMetadata::new("llama-7b-lora", "transformer")
    .with_version("0.1.0")
    .with_custom("lora_rank", 64)
    .with_custom("lora_alpha", 128)
    .with_custom("base_model", "meta-llama/Llama-2-7b");
<span class="boring">}</span></code></pre></pre>
<h2 id="round-trip-integrity"><a class="header" href="#round-trip-integrity">Round-Trip Integrity</a></h2>
<p>All save/load operations maintain <strong>round-trip integrity</strong>:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Original model
let original = create_model();

// Save and load
save_model(&amp;original, "temp.json", &amp;config)?;
let loaded = load_model("temp.json")?;

// Verify parameters match
assert_eq!(original.parameters.len(), loaded.parameters.len());
for (orig, load) in original.parameters.iter().zip(loaded.parameters.iter()) {
    assert_eq!(orig.0, load.0);  // Parameter names
    assert_tensors_equal(&amp;orig.1, &amp;load.1);  // Tensor values
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Validation</strong>: 16 I/O tests ensure round-trip correctness</p>
<h2 id="auto-format-detection"><a class="header" href="#auto-format-detection">Auto-Format Detection</a></h2>
<p>Format automatically detected from file extension:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Detects JSON from .json extension
let model = load_model("model.json")?;

// Detects YAML from .yaml extension
let model = load_model("config.yaml")?;
<span class="boring">}</span></code></pre></pre>
<h2 id="example-workflow"><a class="header" href="#example-workflow">Example Workflow</a></h2>
<p>From <code>examples/model_io.rs</code>:</p>
<pre><pre class="playground"><code class="language-rust">use entrenar::io::{Model, ModelMetadata, save_model, load_model, SaveConfig, ModelFormat};
use entrenar::Tensor;

fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {
    // Create model
    let params = vec![
        ("layer1.weight".to_string(), Tensor::from_vec(vec![0.1, 0.2, 0.3, 0.4], true)),
        ("layer1.bias".to_string(), Tensor::from_vec(vec![0.01, 0.02], true)),
        ("layer2.weight".to_string(), Tensor::from_vec(vec![0.5, 0.6], true)),
        ("layer2.bias".to_string(), Tensor::from_vec(vec![0.1], true)),
    ];

    let metadata = ModelMetadata::new("example-model", "simple-mlp")
        .with_version("0.1.0")
        .with_custom("input_dim", 4)
        .with_custom("hidden_dim", 2)
        .with_custom("output_dim", 1);

    let model = Model::new(metadata, params);

    // Save as JSON
    let json_config = SaveConfig::new(ModelFormat::Json).with_pretty(true);
    save_model(&amp;model, "example_model.json", &amp;json_config)?;

    // Save as YAML
    let yaml_config = SaveConfig::new(ModelFormat::Yaml);
    save_model(&amp;model, "example_model.yaml", &amp;yaml_config)?;

    // Load and verify
    let loaded = load_model("example_model.json")?;
    println!("✅ Loaded model: {}", loaded.metadata.name);

    Ok(())
}</code></pre></pre>
<h2 id="next-steps-4"><a class="header" href="#next-steps-4">Next Steps</a></h2>
<ul>
<li><a href="io/./save-models.html">Save Models</a> - Detailed save functionality</li>
<li><a href="io/./load-models.html">Load Models</a> - Loading and deserialization</li>
<li><a href="io/./metadata.html">Model Metadata</a> - Metadata management</li>
<li><a href="io/./formats.html">Supported Formats</a> - Format details</li>
</ul>
<h2 id="implementation"><a class="header" href="#implementation">Implementation</a></h2>
<p>All Model I/O code is in <code>src/io/</code>:</p>
<ul>
<li><code>mod.rs</code> - Public API exports</li>
<li><code>model.rs</code> - Model and ModelMetadata structs</li>
<li><code>format.rs</code> - ModelFormat enum and SaveConfig</li>
<li><code>save.rs</code> - save_model() function</li>
<li><code>load.rs</code> - load_model() function</li>
<li><code>tests.rs</code> - 16 integration tests</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="save-models"><a class="header" href="#save-models">Save Models</a></h1>
<p><strong>Content to be added.</strong></p>
<p>This chapter will cover:</p>
<ul>
<li>Key concepts and implementation details</li>
<li>Code examples from the entrenar codebase</li>
<li>Best practices and usage guidelines</li>
</ul>
<p>Please check back later for complete content.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="load-models"><a class="header" href="#load-models">Load Models</a></h1>
<p><strong>Content to be added.</strong></p>
<p>This chapter will cover:</p>
<ul>
<li>Key concepts and implementation details</li>
<li>Code examples from the entrenar codebase</li>
<li>Best practices and usage guidelines</li>
</ul>
<p>Please check back later for complete content.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="metadata"><a class="header" href="#metadata">Metadata</a></h1>
<p><strong>Content to be added.</strong></p>
<p>This chapter will cover:</p>
<ul>
<li>Key concepts and implementation details</li>
<li>Code examples from the entrenar codebase</li>
<li>Best practices and usage guidelines</li>
</ul>
<p>Please check back later for complete content.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="formats"><a class="header" href="#formats">Formats</a></h1>
<p><strong>Content to be added.</strong></p>
<p>This chapter will cover:</p>
<ul>
<li>Key concepts and implementation details</li>
<li>Code examples from the entrenar codebase</li>
<li>Best practices and usage guidelines</li>
</ul>
<p>Please check back later for complete content.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="json-format-1"><a class="header" href="#json-format-1">Json Format</a></h1>
<p><strong>Content to be added.</strong></p>
<p>This chapter will cover:</p>
<ul>
<li>Key concepts and implementation details</li>
<li>Code examples from the entrenar codebase</li>
<li>Best practices and usage guidelines</li>
</ul>
<p>Please check back later for complete content.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="yaml-format-1"><a class="header" href="#yaml-format-1">Yaml Format</a></h1>
<p><strong>Content to be added.</strong></p>
<p>This chapter will cover:</p>
<ul>
<li>Key concepts and implementation details</li>
<li>Code examples from the entrenar codebase</li>
<li>Best practices and usage guidelines</li>
</ul>
<p>Please check back later for complete content.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="gguf-format-1"><a class="header" href="#gguf-format-1">Gguf Format</a></h1>
<p><strong>Content to be added.</strong></p>
<p>This chapter will cover:</p>
<ul>
<li>Key concepts and implementation details</li>
<li>Code examples from the entrenar codebase</li>
<li>Best practices and usage guidelines</li>
</ul>
<p>Please check back later for complete content.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="declarative-training-overview"><a class="header" href="#declarative-training-overview">Declarative Training Overview</a></h1>
<p><strong>Declarative training</strong> allows you to define complete training workflows in YAML configuration files (Ludwig-style).</p>
<h2 id="the-problem-3"><a class="header" href="#the-problem-3">The Problem</a></h2>
<p>Training code often mixes:</p>
<ul>
<li>Model architecture definitions</li>
<li>Hyperparameter configurations</li>
<li>Data loading logic</li>
<li>Training loop boilerplate</li>
</ul>
<p><strong>Result</strong>: Hard to experiment, compare runs, or share configurations</p>
<h2 id="the-solution-2"><a class="header" href="#the-solution-2">The Solution</a></h2>
<p>Define training in YAML, execute with one function call:</p>
<pre><code class="language-yaml"># config.yaml
model:
  path: models/llama-7b.gguf
data:
  train: data/train.parquet
  batch_size: 4
optimizer:
  name: adamw
  lr: 0.0001
  beta1: 0.9
  beta2: 0.999
  weight_decay: 0.01
training:
  epochs: 3
  grad_clip: 1.0
  output_dir: ./checkpoints
</code></pre>
<p><strong>Single-command training:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use entrenar::config::train_from_yaml;

train_from_yaml("config.yaml")?;  // Complete workflow
<span class="boring">}</span></code></pre></pre>
<p><strong>From <code>src/config/train.rs</code></strong></p>
<h2 id="configuration-schema"><a class="header" href="#configuration-schema">Configuration Schema</a></h2>
<h3 id="model-section"><a class="header" href="#model-section">Model Section</a></h3>
<pre><code class="language-yaml">model:
  path: path/to/model.gguf  # Model file path (required)
</code></pre>
<p>Currently supports:</p>
<ul>
<li><code>.gguf</code> files (placeholder for Realizar integration)</li>
<li>Placeholder models for testing</li>
</ul>
<h3 id="data-section"><a class="header" href="#data-section">Data Section</a></h3>
<pre><code class="language-yaml">data:
  train: path/to/train.parquet  # Training data path (required)
  batch_size: 4                  # Batch size (required)
</code></pre>
<p>Currently supports:</p>
<ul>
<li><code>.parquet</code> files (placeholder for data loading)</li>
<li>Synthetic data for examples</li>
</ul>
<h3 id="optimizer-section"><a class="header" href="#optimizer-section">Optimizer Section</a></h3>
<pre><code class="language-yaml">optimizer:
  name: adamw       # Optimizer type: sgd, adam, adamw (required)
  lr: 0.0001        # Learning rate (required)
  # Optional parameters:
  momentum: 0.9     # For SGD
  beta1: 0.9        # For Adam/AdamW
  beta2: 0.999      # For Adam/AdamW
  eps: 1e-8         # For Adam/AdamW
  weight_decay: 0.01  # For AdamW
</code></pre>
<p><strong>Supported optimizers:</strong></p>
<ul>
<li><code>sgd</code> → Creates <code>SGD</code> optimizer</li>
<li><code>adam</code> → Creates <code>Adam</code> optimizer</li>
<li><code>adamw</code> → Creates <code>AdamW</code> optimizer</li>
</ul>
<h3 id="training-section"><a class="header" href="#training-section">Training Section</a></h3>
<pre><code class="language-yaml">training:
  epochs: 3                    # Number of training epochs (required)
  grad_clip: 1.0              # Gradient clipping threshold (optional)
  output_dir: ./checkpoints   # Where to save trained model (required)
</code></pre>
<h2 id="optimizer-builders"><a class="header" href="#optimizer-builders">Optimizer Builders</a></h2>
<p>From <code>src/config/builder.rs</code>:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub fn build_optimizer(spec: &amp;OptimSpec) -&gt; Result&lt;Box&lt;dyn Optimizer&gt;&gt; {
    match spec.name.to_lowercase().as_str() {
        "sgd" =&gt; {
            let momentum = spec.params.get("momentum")
                .and_then(|v| v.as_f64()).unwrap_or(0.0) as f32;
            Ok(Box::new(SGD::new(spec.lr, momentum)))
        }
        "adam" =&gt; {
            let beta1 = spec.params.get("beta1")
                .and_then(|v| v.as_f64()).unwrap_or(0.9) as f32;
            let beta2 = spec.params.get("beta2")
                .and_then(|v| v.as_f64()).unwrap_or(0.999) as f32;
            let eps = spec.params.get("eps")
                .and_then(|v| v.as_f64()).unwrap_or(1e-8) as f32;
            Ok(Box::new(Adam::new(spec.lr, beta1, beta2, eps)))
        }
        "adamw" =&gt; {
            // Similar with weight_decay parameter
            Ok(Box::new(AdamW::new(spec.lr, beta1, beta2, eps, weight_decay)))
        }
        name =&gt; Err(Error::ConfigError(format!("Unknown optimizer: {}", name))),
    }
}
<span class="boring">}</span></code></pre></pre>
<h2 id="workflow"><a class="header" href="#workflow">Workflow</a></h2>
<p>The <code>train_from_yaml()</code> function orchestrates:</p>
<ol>
<li><strong>Load config</strong> from YAML file</li>
<li><strong>Validate config</strong> (check paths exist, validate parameters)</li>
<li><strong>Build model</strong> from model path</li>
<li><strong>Build optimizer</strong> from optimizer spec</li>
<li><strong>Setup trainer</strong> with training config</li>
<li><strong>Run training loop</strong> for specified epochs</li>
<li><strong>Save trained model</strong> to output directory</li>
</ol>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// From src/config/train.rs
pub fn train_from_yaml&lt;P: AsRef&lt;Path&gt;&gt;(config_path: P) -&gt; Result&lt;()&gt; {
    // 1. Load and validate config
    let yaml_content = fs::read_to_string(config_path.as_ref())?;
    let spec: TrainSpec = serde_yaml::from_str(&amp;yaml_content)?;
    validate_config(&amp;spec)?;

    // 2. Build components
    let model = build_model(&amp;spec)?;
    let optimizer = build_optimizer(&amp;spec.optimizer)?;

    // 3. Setup trainer
    let mut train_config = TrainConfig::new().with_log_interval(100);
    if let Some(clip) = spec.training.grad_clip {
        train_config = train_config.with_grad_clip(clip);
    }

    let mut trainer = Trainer::new(
        model.parameters.into_iter().map(|(_, t)| t).collect(),
        optimizer,
        train_config,
    );
    trainer.set_loss(Box::new(MSELoss));

    // 4. Training loop
    for epoch in 0..spec.training.epochs {
        let avg_loss = trainer.train_epoch(batches.clone(), |x| x.clone());
        println!("Epoch {}/{}: loss={:.6}", epoch + 1, spec.training.epochs, avg_loss);
    }

    // 5. Save trained model
    let output_path = spec.training.output_dir.join("final_model.json");
    save_model(&amp;final_model, &amp;output_path, &amp;save_config)?;

    Ok(())
}
<span class="boring">}</span></code></pre></pre>
<h2 id="example-usage"><a class="header" href="#example-usage">Example Usage</a></h2>
<p>From <code>examples/train_from_yaml_example.rs</code>:</p>
<pre><pre class="playground"><code class="language-rust">use entrenar::config::train_from_yaml;
use std::fs;

fn main() {
    // Ensure output directory exists
    fs::create_dir_all("./output").expect("Failed to create output directory");

    // Run training from YAML config
    match train_from_yaml("examples/config.yaml") {
        Ok(()) =&gt; {
            println!("=== Training Successful ===");
            println!("\nTrained model saved to: ./output/final_model.json");
        }
        Err(e) =&gt; {
            eprintln!("Training failed: {}", e);
            std::process::exit(1);
        }
    }
}</code></pre></pre>
<p><strong>Run with:</strong></p>
<pre><code class="language-bash">cargo run --example train_from_yaml_example
</code></pre>
<h2 id="validation-1"><a class="header" href="#validation-1">Validation</a></h2>
<p>The <code>validate_config()</code> function checks:</p>
<ul>
<li>✅ Model path exists</li>
<li>✅ Training data path exists</li>
<li>✅ Learning rate &gt; 0</li>
<li>✅ Batch size &gt; 0</li>
<li>✅ Epochs &gt; 0</li>
<li>✅ Output directory is valid</li>
</ul>
<p><strong>From <code>src/config/train.rs</code></strong></p>
<h2 id="tests"><a class="header" href="#tests">Tests</a></h2>
<p><strong>5 builder tests</strong> in <code>src/config/builder.rs</code>:</p>
<ul>
<li>SGD builder creates correct optimizer</li>
<li>Adam builder extracts beta1/beta2/eps</li>
<li>AdamW builder extracts weight_decay</li>
<li>Unknown optimizer name returns error</li>
<li>Missing required parameters handled</li>
</ul>
<h2 id="benefits"><a class="header" href="#benefits">Benefits</a></h2>
<p>✅ <strong>Reproducibility</strong>: Config files capture entire training setup
✅ <strong>Experimentation</strong>: Easy to modify hyperparameters
✅ <strong>Sharing</strong>: Share configs instead of code
✅ <strong>Version control</strong>: Git-friendly YAML files
✅ <strong>Documentation</strong>: Self-documenting training runs</p>
<h2 id="future-enhancements-v020"><a class="header" href="#future-enhancements-v020">Future Enhancements (v0.2.0+)</a></h2>
<ul>
<li>Real GGUF model loading (via Realizar)</li>
<li>Real Parquet data loading</li>
<li>Support for validation sets</li>
<li>Checkpointing during training</li>
<li>TensorBoard logging</li>
</ul>
<h2 id="next-steps-5"><a class="header" href="#next-steps-5">Next Steps</a></h2>
<ul>
<li><a href="declarative/./yaml-config.html">YAML Configuration</a> - Full schema reference</li>
<li><a href="declarative/./train-from-yaml.html">train_from_yaml Function</a> - Implementation details</li>
<li><a href="declarative/./optimizer-builders.html">Optimizer Builders</a> - Builder pattern</li>
<li><a href="declarative/../examples/train-from-yaml.html">Examples</a> - Real examples</li>
</ul>
<h2 id="implementation-1"><a class="header" href="#implementation-1">Implementation</a></h2>
<p>All declarative training code in <code>src/config/</code>:</p>
<ul>
<li><code>train.rs</code> - train_from_yaml() function, TrainSpec, validation</li>
<li><code>builder.rs</code> - build_optimizer(), build_model()</li>
<li><code>mod.rs</code> - Public API exports</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="yaml-config"><a class="header" href="#yaml-config">Yaml Config</a></h1>
<p><strong>Content to be added.</strong></p>
<p>This chapter will cover:</p>
<ul>
<li>Key concepts and implementation details</li>
<li>Code examples from the entrenar codebase</li>
<li>Best practices and usage guidelines</li>
</ul>
<p>Please check back later for complete content.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="train-from-yaml"><a class="header" href="#train-from-yaml">Train From Yaml</a></h1>
<p><strong>Content to be added.</strong></p>
<p>This chapter will cover:</p>
<ul>
<li>Key concepts and implementation details</li>
<li>Code examples from the entrenar codebase</li>
<li>Best practices and usage guidelines</li>
</ul>
<p>Please check back later for complete content.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="schema"><a class="header" href="#schema">Schema</a></h1>
<p><strong>Content to be added.</strong></p>
<p>This chapter will cover:</p>
<ul>
<li>Key concepts and implementation details</li>
<li>Code examples from the entrenar codebase</li>
<li>Best practices and usage guidelines</li>
</ul>
<p>Please check back later for complete content.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="optimizer-builders-1"><a class="header" href="#optimizer-builders-1">Optimizer Builders</a></h1>
<p><strong>Content to be added.</strong></p>
<p>This chapter will cover:</p>
<ul>
<li>Key concepts and implementation details</li>
<li>Code examples from the entrenar codebase</li>
<li>Best practices and usage guidelines</li>
</ul>
<p>Please check back later for complete content.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="model-builders"><a class="header" href="#model-builders">Model Builders</a></h1>
<p><strong>Content to be added.</strong></p>
<p>This chapter will cover:</p>
<ul>
<li>Key concepts and implementation details</li>
<li>Code examples from the entrenar codebase</li>
<li>Best practices and usage guidelines</li>
</ul>
<p>Please check back later for complete content.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="tensor-api"><a class="header" href="#tensor-api">Tensor API</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="autograd-operations"><a class="header" href="#autograd-operations">Autograd Operations</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="optimizer-api"><a class="header" href="#optimizer-api">Optimizer API</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="lora-api"><a class="header" href="#lora-api">LoRA API</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="qlora-api"><a class="header" href="#qlora-api">QLoRA API</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="configuration-system"><a class="header" href="#configuration-system">Configuration System</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="error-handling"><a class="header" href="#error-handling">Error Handling</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="linear-regression-with-autograd"><a class="header" href="#linear-regression-with-autograd">Linear Regression with Autograd</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="training-a-simple-mlp"><a class="header" href="#training-a-simple-mlp">Training a Simple MLP</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="fine-tuning-with-lora"><a class="header" href="#fine-tuning-with-lora">Fine-Tuning with LoRA</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="memory-efficient-qlora"><a class="header" href="#memory-efficient-qlora">Memory-Efficient QLoRA</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="custom-loss-functions"><a class="header" href="#custom-loss-functions">Custom Loss Functions</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="learning-rate-scheduling-2"><a class="header" href="#learning-rate-scheduling-2">Learning Rate Scheduling</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="gradient-clipping-3"><a class="header" href="#gradient-clipping-3">Gradient Clipping</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="adapter-sharing"><a class="header" href="#adapter-sharing">Adapter Sharing</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="contributing"><a class="header" href="#contributing">Contributing</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="extreme-tdd-methodology"><a class="header" href="#extreme-tdd-methodology">EXTREME TDD Methodology</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="testing-strategy"><a class="header" href="#testing-strategy">Testing Strategy</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="unit-tests-1"><a class="header" href="#unit-tests-1">Unit Tests</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="property-based-tests-1"><a class="header" href="#property-based-tests-1">Property-Based Tests</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="gradient-checking-tests"><a class="header" href="#gradient-checking-tests">Gradient Checking Tests</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="mutation-testing-1"><a class="header" href="#mutation-testing-1">Mutation Testing</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="quality-gates"><a class="header" href="#quality-gates">Quality Gates</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="pre-commit-hooks"><a class="header" href="#pre-commit-hooks">Pre-Commit Hooks</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="continuous-integration"><a class="header" href="#continuous-integration">Continuous Integration</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="code-coverage"><a class="header" href="#code-coverage">Code Coverage</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="clippy-linting"><a class="header" href="#clippy-linting">Clippy Linting</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="benchmarking"><a class="header" href="#benchmarking">Benchmarking</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="pmat-toyota-workflow"><a class="header" href="#pmat-toyota-workflow">PMAT Toyota Workflow</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="optimizer-selection"><a class="header" href="#optimizer-selection">Optimizer Selection</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="learning-rate-tuning"><a class="header" href="#learning-rate-tuning">Learning Rate Tuning</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="lora-configuration"><a class="header" href="#lora-configuration">LoRA Configuration</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="memory-optimization"><a class="header" href="#memory-optimization">Memory Optimization</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="gradient-stability"><a class="header" href="#gradient-stability">Gradient Stability</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="debugging-training-issues"><a class="header" href="#debugging-training-issues">Debugging Training Issues</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="performance-profiling"><a class="header" href="#performance-profiling">Performance Profiling</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="custom-backward-passes"><a class="header" href="#custom-backward-passes">Custom Backward Passes</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="implementing-new-optimizers"><a class="header" href="#implementing-new-optimizers">Implementing New Optimizers</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="custom-lora-variants"><a class="header" href="#custom-lora-variants">Custom LoRA Variants</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="advanced-quantization"><a class="header" href="#advanced-quantization">Advanced Quantization</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="distributed-training"><a class="header" href="#distributed-training">Distributed Training</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="model-parallelism"><a class="header" href="#model-parallelism">Model Parallelism</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="autograd-specification"><a class="header" href="#autograd-specification">Autograd Specification</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="optimizer-specification"><a class="header" href="#optimizer-specification">Optimizer Specification</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="lora-specification"><a class="header" href="#lora-specification">LoRA Specification</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="quantization-specification"><a class="header" href="#quantization-specification">Quantization Specification</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="academic-foundations"><a class="header" href="#academic-foundations">Academic Foundations</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="glossary"><a class="header" href="#glossary">Glossary</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="mathematical-notation"><a class="header" href="#mathematical-notation">Mathematical Notation</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="references-2"><a class="header" href="#references-2">References</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="faq"><a class="header" href="#faq">FAQ</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="changelog"><a class="header" href="#changelog">Changelog</a></h1>
<p>All notable changes to Entrenar will be documented in this file.</p>
<p>The format is based on <a href="https://keepachangelog.com/en/1.0.0/">Keep a Changelog</a>,
and this project adheres to <a href="https://semver.org/spec/v2.0.0.html">Semantic Versioning</a>.</p>
<h2 id="010---2025-11-21"><a class="header" href="#010---2025-11-21"><a href="https://github.com/paiml/entrenar/releases/tag/v0.1.0">0.1.0</a> - 2025-11-21</a></h2>
<h3 id="added"><a class="header" href="#added">Added</a></h3>
<h4 id="core-framework"><a class="header" href="#core-framework">Core Framework</a></h4>
<ul>
<li><strong>Autograd Engine</strong> - Tape-based automatic differentiation with backward propagation
<ul>
<li>Tensor abstraction with gradient tracking</li>
<li>BackwardOp trait for custom operations</li>
<li>Attention, matmul, softmax, layer norm operations</li>
<li>Property-based gradient checking (200K+ iterations)</li>
</ul>
</li>
</ul>
<h4 id="optimizers"><a class="header" href="#optimizers">Optimizers</a></h4>
<ul>
<li><strong>SGD</strong> with momentum support</li>
<li><strong>Adam</strong> optimizer with bias correction</li>
<li><strong>AdamW</strong> with decoupled weight decay</li>
<li><strong>Gradient clipping</strong> via L2 norm</li>
<li><strong>Learning rate scheduling</strong> (Cosine, Linear)</li>
<li><strong>SIMD acceleration</strong> for parameter updates via Trueno</li>
<li>Convergence property tests for all optimizers</li>
</ul>
<h4 id="lora--qlora"><a class="header" href="#lora--qlora">LoRA &amp; QLoRA</a></h4>
<ul>
<li><strong>LoRA layers</strong> with configurable rank and alpha</li>
<li><strong>QLoRA</strong> with 4-bit quantized base weights</li>
<li><strong>Adapter management</strong> (save/load separately from base model)</li>
<li><strong>Memory benchmarks</strong> showing 4× reduction with QLoRA</li>
<li><strong>Gradient flow tests</strong> ensuring proper backpropagation</li>
</ul>
<h4 id="quantization"><a class="header" href="#quantization">Quantization</a></h4>
<ul>
<li><strong>QAT (Quantization-Aware Training)</strong> with fake quantize</li>
<li><strong>PTQ (Post-Training Quantization)</strong> with calibration</li>
<li><strong>4-bit and 8-bit</strong> quantization support</li>
<li><strong>Symmetric and asymmetric</strong> quantization modes</li>
<li><strong>Per-channel and per-tensor</strong> quantization</li>
<li>Compression ratio validation and accuracy degradation tests</li>
</ul>
<h4 id="model-merging-arcee-methods"><a class="header" href="#model-merging-arcee-methods">Model Merging (Arcee Methods)</a></h4>
<ul>
<li><strong>TIES</strong> (Task Inference via Elimination and Sign voting)</li>
<li><strong>DARE</strong> (Drop And REscale with Bernoulli masking)</li>
<li><strong>SLERP</strong> (Spherical Linear intERPolation)</li>
<li>Property tests for permutation invariance</li>
<li>Multi-model ensemble support</li>
</ul>
<h4 id="knowledge-distillation"><a class="header" href="#knowledge-distillation">Knowledge Distillation</a></h4>
<ul>
<li><strong>Temperature-scaled KL divergence</strong> loss</li>
<li><strong>Multi-teacher ensemble</strong> distillation</li>
<li><strong>Progressive layer-wise</strong> distillation</li>
<li><strong>44 distillation tests</strong> including 13 property tests</li>
<li>Temperature smoothing validation</li>
</ul>
<h4 id="declarative-configuration"><a class="header" href="#declarative-configuration">Declarative Configuration</a></h4>
<ul>
<li><strong>YAML-based training</strong> configuration (Ludwig-style)</li>
<li><strong>Schema validation</strong> with comprehensive error messages</li>
<li><strong>Auto-inference</strong> of feature types from data</li>
<li><strong>Single-command training</strong> via <code>train_from_yaml()</code></li>
<li>Builder pattern for optimizers and models from config</li>
</ul>
<h4 id="training-loop"><a class="header" href="#training-loop">Training Loop</a></h4>
<ul>
<li><strong>High-level Trainer</strong> abstraction</li>
<li><strong>Batch processing</strong> with configurable batch size</li>
<li><strong>Metrics tracking</strong> (loss history, learning rates, steps)</li>
<li><strong>Gradient clipping</strong> integration</li>
<li><strong>Learning rate scheduling</strong> during training</li>
<li><strong>train_step()</strong> and <strong>train_epoch()</strong> methods</li>
</ul>
<h4 id="model-io"><a class="header" href="#model-io">Model I/O</a></h4>
<ul>
<li><strong>Save/load models</strong> with multiple formats
<ul>
<li><strong>JSON</strong> (pretty-printed or compact)</li>
<li><strong>YAML</strong> for human-readable configs</li>
<li>Placeholder for <strong>GGUF</strong> (future Realizar integration)</li>
</ul>
</li>
<li><strong>ModelMetadata</strong> with custom fields</li>
<li><strong>Round-trip integrity</strong> validation</li>
<li>Automatic format detection from file extension</li>
</ul>
<h3 id="testing--quality"><a class="header" href="#testing--quality">Testing &amp; Quality</a></h3>
<ul>
<li><strong>258 tests</strong> passing (100% success rate)
<ul>
<li>Unit tests for all modules</li>
<li>Integration tests for end-to-end workflows</li>
<li>Property-based tests (200K+ iterations)</li>
<li>Gradient correctness validation</li>
<li>Round-trip serialization tests</li>
</ul>
</li>
<li><strong>0 clippy warnings</strong> (strict mode)</li>
<li><strong>0 TODOs</strong> remaining in codebase</li>
<li><strong>55 Rust source files</strong> with full documentation</li>
</ul>
<h3 id="examples"><a class="header" href="#examples">Examples</a></h3>
<ul>
<li><strong>training_loop.rs</strong> - Demonstrates Trainer API</li>
<li><strong>model_io.rs</strong> - Save/load workflow</li>
<li><strong>train_from_yaml_example.rs</strong> - Declarative training</li>
<li><strong>distillation.rs</strong> - Knowledge distillation</li>
<li><strong>merge_models.rs</strong> - Model merging methods</li>
<li><strong>train_from_yaml.rs</strong> - YAML configuration</li>
<li>Plus LLAMA2 examples (train, finetune-lora, finetune-qlora, memory-benchmarks)</li>
</ul>
<h3 id="documentation"><a class="header" href="#documentation">Documentation</a></h3>
<ul>
<li>Comprehensive API documentation for all public modules</li>
<li>README with quick start guide</li>
<li>Specification documents for all major components</li>
<li>Example configurations (config.yaml)</li>
</ul>
<h3 id="dependencies"><a class="header" href="#dependencies">Dependencies</a></h3>
<ul>
<li><strong>trueno 0.4.1</strong> - SIMD-accelerated compute engine</li>
<li><strong>ndarray 0.16</strong> - N-dimensional arrays</li>
<li><strong>serde 1.0</strong> - Serialization framework</li>
<li><strong>thiserror 2.0</strong> - Error handling</li>
<li><strong>proptest 1.4</strong> - Property-based testing (dev)</li>
<li><strong>tempfile 3.8</strong> - Testing utilities (dev)</li>
</ul>
<h3 id="notes"><a class="header" href="#notes">Notes</a></h3>
<ul>
<li>This is the initial release of Entrenar</li>
<li>GGUF loading requires future Realizar integration</li>
<li>Real data loading (Parquet/CSV) to be added</li>
<li>Performance benchmarks to be published</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="migration-guide"><a class="header" href="#migration-guide">Migration Guide</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="benchmarking-results"><a class="header" href="#benchmarking-results">Benchmarking Results</a></h1>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>

        <script src="ace.js"></script>
        <script src="editor.js"></script>
        <script src="mode-rust.js"></script>
        <script src="theme-dawn.js"></script>
        <script src="theme-tomorrow_night.js"></script>

        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>

    </div>
    </body>
</html>
