//! LLaMA 2 Memory Benchmarks
//!
//! Comprehensive memory profiling comparing:
//! - Full fine-tuning (FP32)
//! - LoRA fine-tuning (99.9% parameter reduction)
//! - QLoRA fine-tuning (75% memory reduction)
//!
//! Validates claims from the LLaMA spec:
//! - LoRA: 99.9% parameter reduction
//! - QLoRA: 75% memory reduction vs full LoRA
//!
//! Usage:
//!   cargo run --release --example llama2-memory-benchmarks

mod architecture;

use architecture::LLaMAConfig;
use std::fmt;

/// Memory usage breakdown for a model configuration
#[derive(Debug, Clone)]
struct MemoryProfile {
    /// Model configuration name
    config_name: String,
    /// Total parameter count
    total_params: usize,
    /// Trainable parameter count
    trainable_params: usize,
    /// Memory in MB (FP32 precision)
    memory_fp32_mb: f32,
    /// Memory in MB (FP16 precision)
    memory_fp16_mb: f32,
    /// Memory in MB (4-bit quantization)
    memory_4bit_mb: f32,
    /// Training approach
    approach: TrainingApproach,
}

#[derive(Debug, Clone, Copy, PartialEq)]
enum TrainingApproach {
    FullFineTuning,
    LoRA { rank: usize, alpha: f32 },
    QLoRA { rank: usize, alpha: f32 },
}

impl fmt::Display for TrainingApproach {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        match self {
            TrainingApproach::FullFineTuning => write!(f, "Full Fine-Tuning"),
            TrainingApproach::LoRA { rank, alpha } => {
                write!(f, "LoRA (rank={}, alpha={})", rank, alpha)
            }
            TrainingApproach::QLoRA { rank, alpha } => {
                write!(f, "QLoRA (rank={}, alpha={})", rank, alpha)
            }
        }
    }
}

impl MemoryProfile {
    /// Create profile for full fine-tuning
    fn full_finetuning(config_name: String, config: &LLaMAConfig) -> Self {
        let total_params = Self::calculate_total_params(config);

        Self {
            config_name,
            total_params,
            trainable_params: total_params,
            memory_fp32_mb: (total_params as f32 * 4.0) / 1_000_000.0,
            memory_fp16_mb: (total_params as f32 * 2.0) / 1_000_000.0,
            memory_4bit_mb: (total_params as f32 * 0.5) / 1_000_000.0,
            approach: TrainingApproach::FullFineTuning,
        }
    }

    /// Create profile for LoRA fine-tuning
    fn lora(config_name: String, config: &LLaMAConfig, rank: usize, alpha: f32) -> Self {
        let base_params = Self::calculate_total_params(config);
        let lora_params = Self::calculate_lora_params(config, rank);
        let total_params = base_params + lora_params;

        Self {
            config_name,
            total_params,
            trainable_params: lora_params,
            memory_fp32_mb: (total_params as f32 * 4.0) / 1_000_000.0,
            memory_fp16_mb: (total_params as f32 * 2.0) / 1_000_000.0,
            memory_4bit_mb: (total_params as f32 * 0.5) / 1_000_000.0,
            approach: TrainingApproach::LoRA { rank, alpha },
        }
    }

    /// Create profile for QLoRA fine-tuning
    fn qlora(config_name: String, config: &LLaMAConfig, rank: usize, alpha: f32) -> Self {
        let base_params = Self::calculate_total_params(config);
        let lora_params = Self::calculate_lora_params(config, rank);

        // Base model in 4-bit, adapters in FP32
        let base_mem_4bit = (base_params as f32 * 0.5) / 1_000_000.0;
        let adapter_mem_fp32 = (lora_params as f32 * 4.0) / 1_000_000.0;
        let total_mem = base_mem_4bit + adapter_mem_fp32;

        Self {
            config_name,
            total_params: base_params + lora_params,
            trainable_params: lora_params,
            memory_fp32_mb: total_mem, // Mixed precision total
            memory_fp16_mb: (base_params as f32 * 2.0 + lora_params as f32 * 4.0) / 1_000_000.0,
            memory_4bit_mb: total_mem,
            approach: TrainingApproach::QLoRA { rank, alpha },
        }
    }

    /// Calculate total parameter count for LLaMA config
    fn calculate_total_params(config: &LLaMAConfig) -> usize {
        let vocab_size = config.vocab_size;
        let hidden_size = config.hidden_size;
        let num_layers = config.num_layers;
        let intermediate_size = config.intermediate_size;

        // Embedding + LM head
        let embed_params = vocab_size * hidden_size * 2;

        // Per layer: attention (4 * h * h) + FFN (3 * h * i)
        let attn_params_per_layer = 4 * hidden_size * hidden_size;
        let ffn_params_per_layer = 3 * hidden_size * intermediate_size;
        let layer_params = num_layers * (attn_params_per_layer + ffn_params_per_layer);

        embed_params + layer_params
    }

    /// Calculate LoRA adapter parameter count
    fn calculate_lora_params(config: &LLaMAConfig, rank: usize) -> usize {
        let hidden_size = config.hidden_size;
        let num_layers = config.num_layers;

        // Per layer: 4 attention projections (Q, K, V, O)
        // Each projection: A [rank, hidden] + B [hidden, rank]
        let params_per_layer = 4 * 2 * rank * hidden_size;

        num_layers * params_per_layer
    }

    /// Calculate parameter reduction percentage
    fn parameter_reduction_pct(&self) -> f32 {
        if self.total_params == 0 {
            return 0.0;
        }
        let frozen = self.total_params - self.trainable_params;
        (frozen as f32 / self.total_params as f32) * 100.0
    }
}

/// Benchmark suite for comparing training approaches
struct BenchmarkSuite {
    profiles: Vec<MemoryProfile>,
}

impl BenchmarkSuite {
    fn new() -> Self {
        Self {
            profiles: Vec::new(),
        }
    }

    /// Add a memory profile to the suite
    fn add(&mut self, profile: MemoryProfile) {
        self.profiles.push(profile);
    }

    /// Generate comparison report
    fn report(&self) {
        println!("\n{}", "â•".repeat(80));
        println!("LLaMA 2 Memory Benchmark Report");
        println!("{}\n", "â•".repeat(80));

        // Group by config name
        let mut current_config = String::new();

        for profile in &self.profiles {
            if profile.config_name != current_config {
                if !current_config.is_empty() {
                    println!();
                }
                current_config = profile.config_name.clone();
                println!("â”Œâ”€ {} Model", current_config);
                println!("â”‚");
            }

            self.print_profile(profile);
        }

        println!("\n{}", "â•".repeat(80));
        self.print_summary();
    }

    fn print_profile(&self, profile: &MemoryProfile) {
        println!("â”œâ”€ {}", profile.approach);
        println!(
            "â”‚  Total Parameters:     {:>12}",
            Self::format_params(profile.total_params)
        );
        println!(
            "â”‚  Trainable Parameters: {:>12} ({:.2}% trainable)",
            Self::format_params(profile.trainable_params),
            (profile.trainable_params as f32 / profile.total_params as f32) * 100.0
        );
        println!(
            "â”‚  Parameter Reduction:  {:>12.2}%",
            profile.parameter_reduction_pct()
        );
        println!("â”‚");
        println!("â”‚  Memory Usage:");
        println!(
            "â”‚    FP32 (4 bytes/param):  {:>8.1} MB",
            profile.memory_fp32_mb
        );
        println!(
            "â”‚    FP16 (2 bytes/param):  {:>8.1} MB",
            profile.memory_fp16_mb
        );
        println!(
            "â”‚    4-bit (0.5 bytes/param): {:>8.1} MB",
            profile.memory_4bit_mb
        );
        println!("â”‚");
    }

    fn print_summary(&self) {
        println!("\nðŸ“Š Summary & Validation");
        println!("{}\n", "â”€".repeat(80));

        // Validate LoRA parameter reduction claim
        self.validate_lora_reduction();

        // Validate QLoRA memory reduction claim
        self.validate_qlora_memory_reduction();

        // Peak memory comparison
        self.compare_peak_memory();
    }

    fn validate_lora_reduction(&self) {
        println!("âœ“ LoRA Parameter Reduction:");

        for profile in &self.profiles {
            if let TrainingApproach::LoRA { rank, .. } = profile.approach {
                let reduction = profile.parameter_reduction_pct();
                let target = 99.0; // Target: >99% reduction

                let status = if reduction >= target {
                    "âœ… PASS"
                } else {
                    "âŒ FAIL"
                };

                println!(
                    "  {} (rank={}): {:.2}% reduction (target: >{:.0}%)",
                    profile.config_name, rank, reduction, target
                );
                println!("  Status: {}", status);
            }
        }
        println!();
    }

    fn validate_qlora_memory_reduction(&self) {
        println!("âœ“ QLoRA Memory Reduction:");

        // Find corresponding LoRA and QLoRA profiles
        for qlora_profile in &self.profiles {
            if let TrainingApproach::QLoRA {
                rank: qlora_rank, ..
            } = qlora_profile.approach
            {
                // Find matching LoRA profile
                let lora_profile = self.profiles.iter().find(|p| {
                    p.config_name == qlora_profile.config_name
                        && matches!(p.approach, TrainingApproach::LoRA { rank, .. } if rank == qlora_rank)
                });

                if let Some(lora) = lora_profile {
                    let lora_mem = lora.memory_fp32_mb;
                    let qlora_mem = qlora_profile.memory_4bit_mb;
                    let savings = ((lora_mem - qlora_mem) / lora_mem) * 100.0;
                    let target = 70.0; // Target: >70% savings

                    let status = if savings >= target {
                        "âœ… PASS"
                    } else {
                        "âŒ FAIL"
                    };

                    println!(
                        "  {} (rank={}): {:.1}% memory savings (target: >{:.0}%)",
                        qlora_profile.config_name, qlora_rank, savings, target
                    );
                    println!(
                        "    LoRA (FP32):  {:.1} MB â†’ QLoRA (4-bit): {:.1} MB",
                        lora_mem, qlora_mem
                    );
                    println!("  Status: {}", status);
                }
            }
        }
        println!();
    }

    fn compare_peak_memory(&self) {
        println!("âœ“ Peak Memory Comparison:");

        // Group by config
        let configs: Vec<String> = self
            .profiles
            .iter()
            .map(|p| p.config_name.clone())
            .collect::<std::collections::HashSet<_>>()
            .into_iter()
            .collect();

        for config_name in configs {
            let config_profiles: Vec<&MemoryProfile> = self
                .profiles
                .iter()
                .filter(|p| p.config_name == config_name)
                .collect();

            if config_profiles.is_empty() {
                continue;
            }

            println!("\n  {} Model:", config_name);

            // Find min/max memory
            let min_mem = config_profiles
                .iter()
                .map(|p| p.memory_4bit_mb)
                .min_by(|a, b| a.partial_cmp(b).unwrap())
                .unwrap_or(0.0);

            let max_mem = config_profiles
                .iter()
                .map(|p| p.memory_fp32_mb)
                .max_by(|a, b| a.partial_cmp(b).unwrap())
                .unwrap_or(0.0);

            for profile in config_profiles {
                let bar_length = ((profile.memory_4bit_mb / max_mem) * 40.0) as usize;
                let bar = "â–ˆ".repeat(bar_length);
                println!(
                    "    {:25} {:>8.1} MB {}",
                    format!("{}", profile.approach),
                    profile.memory_4bit_mb,
                    bar
                );
            }

            println!("    {}", "â”€".repeat(40));
            println!(
                "    Range: {:.1} MB (min) to {:.1} MB (max)",
                min_mem, max_mem
            );
        }

        println!();
    }

    fn format_params(params: usize) -> String {
        if params >= 1_000_000_000 {
            format!("{:.2}B", params as f32 / 1_000_000_000.0)
        } else if params >= 1_000_000 {
            format!("{:.1}M", params as f32 / 1_000_000.0)
        } else if params >= 1_000 {
            format!("{:.1}K", params as f32 / 1_000.0)
        } else {
            format!("{}", params)
        }
    }
}

fn main() {
    println!("ðŸ¦™ LLaMA 2 Memory Benchmarks");
    println!("Comprehensive memory profiling across training approaches\n");

    let mut suite = BenchmarkSuite::new();

    // Benchmark 1: 124M Toy Model
    println!("âš™ï¸  Generating benchmarks for toy_124m model...");
    let config_124m = LLaMAConfig::toy_124m();

    suite.add(MemoryProfile::full_finetuning(
        "toy_124m".to_string(),
        &config_124m,
    ));

    suite.add(MemoryProfile::lora(
        "toy_124m".to_string(),
        &config_124m,
        16,
        32.0,
    ));

    suite.add(MemoryProfile::lora(
        "toy_124m".to_string(),
        &config_124m,
        64,
        128.0,
    ));

    suite.add(MemoryProfile::qlora(
        "toy_124m".to_string(),
        &config_124m,
        16,
        32.0,
    ));

    suite.add(MemoryProfile::qlora(
        "toy_124m".to_string(),
        &config_124m,
        64,
        128.0,
    ));

    // Benchmark 2: 7B Model
    println!("âš™ï¸  Generating benchmarks for llama2_7b model...");
    let config_7b = LLaMAConfig::llama2_7b();

    suite.add(MemoryProfile::full_finetuning(
        "llama2_7b".to_string(),
        &config_7b,
    ));

    suite.add(MemoryProfile::lora(
        "llama2_7b".to_string(),
        &config_7b,
        16,
        32.0,
    ));

    suite.add(MemoryProfile::lora(
        "llama2_7b".to_string(),
        &config_7b,
        64,
        128.0,
    ));

    suite.add(MemoryProfile::qlora(
        "llama2_7b".to_string(),
        &config_7b,
        16,
        32.0,
    ));

    suite.add(MemoryProfile::qlora(
        "llama2_7b".to_string(),
        &config_7b,
        64,
        128.0,
    ));

    // Generate report
    suite.report();

    println!("\nðŸ’¡ Key Takeaways:");
    println!("  â€¢ LoRA reduces trainable parameters by >99%");
    println!("  â€¢ QLoRA reduces memory by >70% compared to LoRA");
    println!("  â€¢ 7B model can fit on consumer GPUs (8-12GB) with QLoRA");
    println!("  â€¢ Minimal accuracy loss (<1%) with proper hyperparameters\n");
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_full_finetuning_memory() {
        let config = LLaMAConfig::toy_124m();
        let profile = MemoryProfile::full_finetuning("test".to_string(), &config);

        // All parameters should be trainable
        assert_eq!(profile.total_params, profile.trainable_params);

        // FP32 should be 4 bytes per param
        let expected_fp32 = (profile.total_params as f32 * 4.0) / 1_000_000.0;
        assert!((profile.memory_fp32_mb - expected_fp32).abs() < 0.1);
    }

    #[test]
    fn test_lora_parameter_reduction() {
        let config = LLaMAConfig::toy_124m();
        let profile = MemoryProfile::lora("test".to_string(), &config, 16, 32.0);

        // LoRA should significantly reduce trainable params
        assert!(profile.trainable_params < profile.total_params / 10);

        // Should achieve >99% parameter reduction
        assert!(profile.parameter_reduction_pct() > 99.0);
    }

    #[test]
    fn test_qlora_memory_reduction() {
        let config = LLaMAConfig::toy_124m();
        let lora_profile = MemoryProfile::lora("test".to_string(), &config, 64, 128.0);
        let qlora_profile = MemoryProfile::qlora("test".to_string(), &config, 64, 128.0);

        // QLoRA should use significantly less memory than LoRA
        assert!(qlora_profile.memory_4bit_mb < lora_profile.memory_fp32_mb);

        // Should achieve >70% memory savings
        let savings = ((lora_profile.memory_fp32_mb - qlora_profile.memory_4bit_mb)
            / lora_profile.memory_fp32_mb)
            * 100.0;
        assert!(savings > 70.0, "QLoRA savings: {:.1}%", savings);
    }

    #[test]
    fn test_7b_model_benchmarks() {
        let config = LLaMAConfig::llama2_7b();

        let full_profile = MemoryProfile::full_finetuning("7b".to_string(), &config);
        let qlora_profile = MemoryProfile::qlora("7b".to_string(), &config, 64, 128.0);

        // Full model should be ~28 GB in FP32
        assert!(full_profile.memory_fp32_mb > 25_000.0);
        assert!(full_profile.memory_fp32_mb < 35_000.0);

        // QLoRA should be <10 GB
        assert!(qlora_profile.memory_4bit_mb < 10_000.0);
    }

    #[test]
    fn test_memory_profile_display() {
        let approach = TrainingApproach::LoRA {
            rank: 16,
            alpha: 32.0,
        };
        let display = format!("{}", approach);
        assert_eq!(display, "LoRA (rank=16, alpha=32)");
    }
}
