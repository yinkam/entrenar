# entrenar

**Rust Training & Optimization Library**

Entrenar provides a tape-based autograd engine with optimizers, designed for training neural networks with support for LoRA, quantization, model merging, and knowledge distillation.

## Status

**Phase 1 (Autograd Engine): âœ… COMPLETED**

- âœ… Core Tensor type with gradient tracking
- âœ… Tape-based automatic differentiation
- âœ… Backward operations: Add, Mul, Scale, ReLU, Softmax, Sum
- âœ… Finite difference gradient validation
- âœ… Property-based tests (1000+ test cases per operation)
- âœ… SGD and Adam optimizers
- âœ… All tests passing (18 tests)
- âœ… Clippy clean (zero warnings)
- âœ… 1,130 lines of code

## Features

### Implemented

#### Autograd Engine
- **Tensor**: Core type with automatic gradient tracking
- **Operations**:
  - Arithmetic: `add`, `mul`, `scale`, `sum`
  - Activations: `relu`, `softmax`
- **Backward Pass**: Automatic differentiation via computational graph
- **Gradient Checking**: Finite difference validation for all operations

#### Optimizers
- **SGD**: With optional momentum
- **Adam**: Adaptive moment estimation with bias correction

### Testing

All implementations follow **EXTREME TDD** methodology:

```bash
# Run all tests
cargo test

# Run specific test module
cargo test autograd

# Run with output
cargo test -- --nocapture

# Check code quality
cargo clippy -- -D warnings
cargo fmt --check
```

**Test Coverage:**
- Unit tests for all operations
- Property-based tests (1000+ cases per operation using proptest)
- Gradient checking via finite difference (epsilon=1e-3, tolerance=0.1)
- Optimizer convergence tests

## Usage

### Basic Autograd

```rust
use entrenar::autograd::*;

// Create tensors
let a = Tensor::from_vec(vec![1.0, 2.0, 3.0], true);  // requires_grad=true
let b = Tensor::from_vec(vec![4.0, 5.0, 6.0], true);

// Forward pass
let c = add(&a, &b);
let d = relu(&c);
let mut loss = sum(&d);

// Backward pass
backward(&mut loss, None);

// Access gradients
let grad_a = a.grad().unwrap();
let grad_b = b.grad().unwrap();
```

### Using Optimizers

```rust
use entrenar::autograd::*;
use entrenar::optim::*;

// Create parameters
let mut params = vec![
    Tensor::from_vec(vec![0.5, -0.3], true),
];

// Create optimizer
let mut optimizer = Adam::default_params(0.01);

for epoch in 0..100 {
    // Compute gradients (your forward pass here)
    // ...

    // Update parameters
    optimizer.step(&mut params);
    optimizer.zero_grad(&mut params);
}
```

## Architecture

```
src/
â”œâ”€â”€ autograd/         âœ… Tape-based automatic differentiation
â”‚   â”œâ”€â”€ tensor.rs     âœ… Tensor with gradient tracking
â”‚   â”œâ”€â”€ ops.rs        âœ… Forward/backward operations
â”‚   â”œâ”€â”€ backward.rs   âœ… BackwardOp trait
â”‚   â”œâ”€â”€ context.rs    âœ… Execution context
â”‚   â””â”€â”€ tests.rs      âœ… Comprehensive test suite
â”œâ”€â”€ optim/            âœ… Optimizers
â”‚   â”œâ”€â”€ optimizer.rs  âœ… Optimizer trait
â”‚   â”œâ”€â”€ sgd.rs        âœ… SGD with momentum
â”‚   â””â”€â”€ adam.rs       âœ… Adam optimizer
â”œâ”€â”€ lora/             ğŸš§ Placeholder (Phase 3)
â”œâ”€â”€ quant/            ğŸš§ Placeholder (Phase 4)
â”œâ”€â”€ merge/            ğŸš§ Placeholder (Phase 5)
â”œâ”€â”€ distill/          ğŸš§ Placeholder (Phase 7)
â”œâ”€â”€ config/           ğŸš§ Placeholder (Phase 6)
â””â”€â”€ train/            ğŸš§ Placeholder
```

## Roadmap

See `docs/specifications/entrenar-spec.md` for complete specification.

### âœ… Phase 1: Autograd Engine (COMPLETED)
- Tape-based context with gradient tracking
- Core backward operations with gradient validation
- Property-based tests (1000+ iterations)

### ğŸš§ Phase 2: Optimizers (IN PROGRESS)
- âœ… SGD with momentum
- âœ… Adam
- â³ AdamW (decoupled weight decay)
- â³ Learning rate schedulers
- â³ Gradient clipping

### â³ Phase 3: LoRA (144h estimated)
- Low-rank adaptation layers
- QLoRA (4-bit base weights)
- Adapter save/load

### â³ Phase 4: Quantization (136h estimated)
- QAT (Quantization-Aware Training)
- PTQ (Post-Training Quantization)
- GGUF export (Q4_0/Q8_0)

### â³ Phase 5: Model Merging (96h estimated)
- TIES (Task Inference via Elimination and Sign)
- DARE (Drop And REscale)
- SLERP (Spherical Linear Interpolation)

### â³ Phase 6: Declarative Config (64h estimated)
- YAML configuration schema
- Auto-feature type inference
- Single-command training

### â³ Phase 7: Distillation (64h estimated)
- Knowledge distillation loss
- Multi-teacher ensemble
- Progressive distillation

## Development

### Quality Gates (Tiered Workflow)

```bash
# Tier 1 (Fast <5s) - Before every commit
make tier1

# Tier 2 (Integration <30s) - Before push
make tier2

# Tier 3 (Full <5m) - Before PR
make tier3

# Full CI Pipeline
make ci    # Tier 3 + coverage + mutants + PMAT + security
```

### Standard Commands

```bash
# Build
make build              # Debug
make release            # Release

# Testing
make test               # Fast tests
make coverage           # Coverage report (>90% required)
make mutants            # Mutation testing (>80% kill rate)

# Code Quality
make lint               # Clippy
make format             # Format code
make deny-check         # Dependency security

# Clean
make clean

# View all commands
make help
```

### Ticket-Based Development

All work is tracked via tickets (ENT-001 through ENT-040) in `roadmap.yaml`. See CLAUDE.md for workflow details.

```bash
make roadmap-status     # View progress
```

## Quality Metrics

Current status:
- âœ… All tests passing (18 tests)
- âœ… Clippy clean (0 warnings)
- âœ… Property-based testing (1000+ cases per operation)
- âœ… Gradient validation (finite difference checking)

## License

MIT
