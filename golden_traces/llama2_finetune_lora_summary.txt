=================================

ðŸ“‹ Configuration:
   - LoRA rank: 16
   - LoRA alpha: 32
   - Learning rate: 1.00e-4
   - Weight decay: 0.01
   - Batch size: 8

ðŸ”§ Loading base model from checkpoints/llama-124m.bin
   - Base parameters: 162.4M

ðŸ”— Applying LoRA adapters...
   - Trainable parameters: 1.2M
   - Total parameters: 163.6M
   - Parameter reduction: 99.3%

ðŸš€ Starting LoRA fine-tuning...

Epoch 1/3
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Step 0: loss=2.5000, lr=1.00e-4
  Step 10: loss=2.5000, lr=9.97e-5
  Step 20: loss=2.5000, lr=9.89e-5
  Step 30: loss=2.5000, lr=9.76e-5
  Step 40: loss=2.5000, lr=9.59e-5
  Step 50: loss=2.5000, lr=9.37e-5
  Step 60: loss=2.5000, lr=9.11e-5
  Step 70: loss=2.5000, lr=8.81e-5
  Step 80: loss=2.5000, lr=8.48e-5
  Step 90: loss=2.5000, lr=8.11e-5

ðŸ“Š Epoch 1 Summary:
   - Train loss: 2.5000

Epoch 2/3
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Step 0: loss=2.5000, lr=7.71e-5
  Step 10: loss=2.5000, lr=7.29e-5
  Step 20: loss=2.5000, lr=6.85e-5
  Step 30: loss=2.5000, lr=6.39e-5
  Step 40: loss=2.5000, lr=5.92e-5
  Step 50: loss=2.5000, lr=5.45e-5
  Step 60: loss=2.5000, lr=4.98e-5
  Step 70: loss=2.5000, lr=4.52e-5
  Step 80: loss=2.5000, lr=4.06e-5
  Step 90: loss=2.5000, lr=3.63e-5

ðŸ“Š Epoch 2 Summary:
   - Train loss: 2.5000

Epoch 3/3
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Step 0: loss=2.5000, lr=3.21e-5
  Step 10: loss=2.5000, lr=2.82e-5
  Step 20: loss=2.5000, lr=2.45e-5
  Step 30: loss=2.5000, lr=2.12e-5
  Step 40: loss=2.5000, lr=1.83e-5
  Step 50: loss=2.5000, lr=1.58e-5
  Step 60: loss=2.5000, lr=1.37e-5
  Step 70: loss=2.5000, lr=1.21e-5
  Step 80: loss=2.5000, lr=1.09e-5
  Step 90: loss=2.5000, lr=1.02e-5

ðŸ“Š Epoch 3 Summary:
   - Train loss: 2.5000

âœ… Fine-tuning complete!
  ðŸ’¾ Saving LoRA adapters to checkpoints/lora_adapters.bin

ðŸ”€ Merging LoRA adapters into base model...
   âœ“ Adapters merged - model ready for inference

% time     seconds  usecs/call     calls    errors syscall
------ ----------- ----------- --------- --------- ----------------
 53.45    0.077364           7      9914           getrandom
  6.89    0.009966          19       511           mremap
  1.16    0.001678          11       152           mmap
 37.48    0.054250         395       137           munmap
  0.39    0.000564           8        64           write
  0.23    0.000337           9        36           brk
  0.07    0.000095          13         7           read
  0.08    0.000112          16         7           mprotect
  0.06    0.000084          14         6         1 openat
  0.02    0.000036           7         5           rt_sigaction
  0.04    0.000054          10         5           newfstatat
  0.03    0.000045           9         5           close
  0.03    0.000038           9         4           unknown
  0.02    0.000025           6         4           pread64
  0.01    0.000021           7         3           sigaltstack
  0.01    0.000020          10         2         1 arch_prctl
  0.01    0.000013          13         1           poll
  0.00    0.000007           7         1           set_tid_address
  0.01    0.000013          13         1         1 access
  0.00    0.000006           6         1           set_robust_list
------ ----------- ----------- --------- --------- ----------------
100.00    0.144728          13     10866         3 total
